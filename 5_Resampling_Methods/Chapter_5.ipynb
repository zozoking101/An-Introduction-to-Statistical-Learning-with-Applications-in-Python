{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428d309b",
   "metadata": {},
   "source": [
    "### Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea98acc",
   "metadata": {},
   "source": [
    "Resampling methods are an indispensable tool in modern statistics. They\n",
    "involve repeatedly drawing samples from a training set and refitting a model\n",
    "of interest on each sample in order to obtain additional information about\n",
    "the fitted model. For example, in order to estimate the variability of a linear\n",
    "regression fit, we can repeatedly draw different samples from the training\n",
    "data, fit a linear regression to each new sample, and then examine the\n",
    "extent to which the resulting fits differ. Such an approach may allow us to\n",
    "obtain information that would not be available from fitting the model only\n",
    "once using the original training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab74e1",
   "metadata": {},
   "source": [
    "Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate\n",
    "its performance, or to select the appropriate level of flexibility. \n",
    "\n",
    "The process of evaluating a model’s performance is known as model assessment, whereas model\n",
    "the process of selecting the proper level of flexibility for a model is known as\n",
    "model selection. \n",
    "\n",
    "The bootstrap is used in several contexts, most commonly model\n",
    "to provide a measure of accuracy of a parameter estimate or of a given statistical learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f7ef6",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4390d",
   "metadata": {},
   "source": [
    "The test error is the average error that results from using\n",
    "a statistical learning method to predict the response on a new observation—\n",
    "that is, a measurement that was not used in training the method. Given\n",
    "a data set, the use of a particular statistical learning method is warranted\n",
    "if it results in a low test error. The test error can be easily calculated if a\n",
    "designated test set is available. Unfortunately, this is usually not the case.\n",
    "In contrast, the training error can be easily calculated by applying the\n",
    "statistical learning method to the observations used in its training. But as\n",
    "we saw in Chapter 2, the training error rate often is quite different from the\n",
    "test error rate, and in particular the former can dramatically underestimate\n",
    "the latter.\n",
    "\n",
    "In the absence of a very large designated test set that can be used to\n",
    "directly estimate the test error rate, a number of techniques can be used\n",
    "to estimate this quantity using the available training data. Some methods\n",
    "make a mathematical adjustment to the training error rate in order to\n",
    "estimate the test error rate. Such approaches are discussed.\n",
    "In this section, we instead consider a class of methods that estimate the\n",
    "test error rate by holding out a subset of the training observations from the\n",
    "fitting process, and then applying the statistical learning method to those\n",
    "held out observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40baa390",
   "metadata": {},
   "source": [
    "#####  The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaeafb5",
   "metadata": {},
   "source": [
    "Suppose that we would like to estimate the test error associated with fit\n",
    "ting a particular statistical learning method on a set of observations. The\n",
    "validation set approach, displayed in Figure 5.1, is a very simple strategy validation\n",
    "for this task. It involves randomly dividing the available set of observa\n",
    "tions into two parts, a training set and a validation set or hold-out set. The validation\n",
    "model is fit on the training set, and the fitted model is used to predict the\n",
    "responses for the observations in the validation set. The resulting validation\n",
    "set error rate—typically assessed using MSE in the case of a quantitative\n",
    "response—provides an estimate of the test error rate.\n",
    "Weillustrate the validation set approach on the Auto data set. Recall from\n",
    "Chapter 3 that there appears to be a non-linear relationship between mpg\n",
    "and horsepower, and that a model that predicts mpg using horsepower and\n",
    "horsepower2 gives better results than a model that uses only a linear term.\n",
    "It is natural to wonder whether a cubic or higher-order fit might provide\n",
    "even better results. We answer this question in Chapter 3 by looking at\n",
    "the p-values associated with a cubic term and higher-order polynomial\n",
    "terms in a linear regression. But we could also answer this question using\n",
    "the validation method. We randomly split the 392 observations into two sets, a training set containing 196 of the data points, and a validation set\n",
    "containing the remaining 196 observations. The validation set error rates\n",
    "that result from fitting various regression models on the training sample\n",
    "and evaluating their performance on the validation sample, using MSE\n",
    "as a measure of validation set error, are shown in the left-hand panel of\n",
    "Figure 5.2. The validation set MSE for the quadratic fit is considerably\n",
    "smaller than for the linear fit. However, the validation set MSE for the cubic\n",
    "f\n",
    "it is actually slightly larger than for the quadratic fit. This implies that\n",
    "including a cubic term in the regression does not lead to better prediction\n",
    "than simply using a quadratic term.\n",
    "Recall that in order to create the left-hand panel of Figure 5.2, we ran\n",
    "domly divided the data set into two parts, a training set and a validation\n",
    "set. If we repeat the process of randomly splitting the sample set into two\n",
    "parts, we will get a somewhat different estimate for the test MSE. As an\n",
    "illustration, the right-hand panel of Figure 5.2 displays ten different vali\n",
    "dation set MSE curves from the Auto data set, produced using ten different\n",
    "random splits of the observations into training and validation sets. All ten\n",
    "curves indicate that the model with a quadratic term has a dramatically\n",
    "smaller validation set MSE than the model with only a linear term. Fur\n",
    "thermore, all ten curves indicate that there is not much benefit in including\n",
    "cubic or higher-order polynomial terms in the model. But it is worth noting\n",
    "that each of the ten curves results in a different test MSE estimate for each\n",
    "of the ten regression models considered. And there is no consensus among\n",
    "the curves as to which model results in the smallest validation set MSE.\n",
    "Based on the variability among these curves, all that we can conclude with\n",
    "any confidence is that the linear fit is not adequate for this data.\n",
    "The validation set approach is conceptually simple and is easy to imple\n",
    "ment. But it has two potential drawbacks:\n",
    "1. As is shown in the right-hand panel of Figure 5.2, the validation esti\n",
    "mate of the test error rate can be highly variable, depending on pre\n",
    "cisely which observations are included in the training set and which\n",
    "observations are included in the validation set.\n",
    "2. In the validation approach, only a subset of the observations—those\n",
    "that are included in the training set rather than in the validation\n",
    "set—are used to fit the model. Since statistical methods tend to per\n",
    "form worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate\n",
    "for the model fit on the entire data set.\n",
    "In the coming subsections, we will present cross-validation, a refinement of\n",
    "the validation set approach that addresses these two issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af9682",
   "metadata": {},
   "source": [
    "##### Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d07f2",
   "metadata": {},
   "source": [
    "5.1.2 Leave-One-Out Cross-Validation\n",
    "Leave-one-out cross-validation (LOOCV) is closely related to the validation leave-one\n",
    "set approach of Section 5.1.1, but it attempts to address that method’s\n",
    "drawbacks.\n",
    "Like the validation set approach, LOOCV involves splitting the set of\n",
    "observations into two parts. However, instead of creating two subsets of\n",
    "comparable size, a single observation (x1,y1) is used for the validation\n",
    "set, and the remaining observations {(x2,y2),...,(xn,yn)} make up the\n",
    "training set. The statistical learning method is fit on the n 1 training\n",
    "observations, and a prediction ˆy1 is made for the excluded observation,\n",
    "using its value x1. Since (x1,y1) was not used in the fitting process, MSE1 =\n",
    "(y1 \n",
    "ˆ\n",
    "y1)2 provides an approximately unbiased estimate for the test error.\n",
    "But even though MSE1 is unbiased for the test error, it is a poor estimate\n",
    "because it is highly variable, since it is based upon a single observation\n",
    "(x1,y1).\n",
    "We can repeat the procedure by selecting (x2,y2) for the validation\n",
    "data, training the statistical learning procedure on the n 1 observations\n",
    "{(x1,y1),(x3,y3),...,(xn,yn)}, and computing MSE2 =(y2 ˆy2)2. Repeat\n",
    "ing this approach n times produces n squared errors, MSE1,..., MSEn.\n",
    "The LOOC proach n times produces n squared errors, MSE1,..., MSEn.\n",
    "The LOOCV estimate for the test MSE is the average of these n test error\n",
    "estimates:\n",
    "CV(n) = 1\n",
    "n \n",
    "n\n",
    "i=1 \n",
    "MSEi.\n",
    "(5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9516b37",
   "metadata": {},
   "source": [
    "LOOCV has a couple of major advantages over the validation set ap\n",
    "proach. First, it has far less bias. In LOOCV, we repeatedly fit the sta\n",
    "tistical learning method using training sets that contain n 1 observa\n",
    "tions, almost as many as are in the entire data set. This is in contrast to\n",
    "the validation set approach, in which the training set is typically around\n",
    "half the size of the original data set. Consequently, the LOOCV approach\n",
    "tends not to overestimate the test error rate as much as the validation\n",
    "set approach does. Second, in contrast to the validation approach which\n",
    "will yield different results when applied repeatedly due to randomness in\n",
    "the training/validation set splits, performing LOOCV multiple times will\n",
    "always yield the same results: there is no randomness in the training/vali\n",
    "dation set splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943ddfb",
   "metadata": {},
   "source": [
    "LOOCVhas the potential to be expensive to implement, since the model\n",
    "has to be fit n times. This can be very time consuming if n is large, and if\n",
    "each individual model is slow to fit. With least squares linear or polynomial\n",
    "regression, an amazing shortcut makes the cost of LOOCV the same as that\n",
    "of a single model fit! The following formula holds:\n",
    "CV(n) = 1\n",
    "n \n",
    "n\n",
    "i=1 \n",
    "yi ˆyi\n",
    "2\n",
    ",\n",
    "(5.2)\n",
    "\n",
    "where ˆyi is the ith fitted value from the original least squares fit, and hi is\n",
    "the leverage defined in (3.37) on page 105.1 This is like the ordinary MSE,\n",
    "except the ith residual is divided by 1 hi. The leverage lies between 1/n\n",
    "and 1, and reflects the amount that an observation influences its own fit.\n",
    "Hence the residuals for high-leverage points are inflated in this formula by\n",
    "exactly the right amount for this equality to hold.\n",
    "LOOCV is a very general method, and can be used with any kind of\n",
    "predictive modeling. For example we could use it with logistic regression\n",
    "or linear discriminant analysis, or any of the methods discussed in later\n",
    "chapters. The magic formula (5.2) does not hold in general, in which case\n",
    "the model has to be refit n times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2e9b4",
   "metadata": {},
   "source": [
    "#####  k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48748614",
   "metadata": {},
   "source": [
    "An alternative to LOOCV is k-fold CV. This approach involves randomly k-fold CV\n",
    "dividing the set of observations into k groups, or folds, of approximately\n",
    "equal size. The first fold is treated as a validation set, and the method\n",
    "is fit on the remaining k 1 folds. The mean squared error, MSE1, is\n",
    "then computed on the observations in the held-out fold. This procedure is\n",
    "repeated k times; each time, a different group of observations is treated\n",
    "as a validation set. This process results in k estimates of the test error,\n",
    "MSE1,MSE2,...,MSEk. The k-fold CV estimate is computed by averaging\n",
    "these values,\n",
    "CV(k) = 1\n",
    "k \n",
    "k\n",
    "i=1 \n",
    "MSEi.\n",
    "Figure 5.5 illustrates the k-fold CV approach.\n",
    "(5.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77df72",
   "metadata": {},
   "source": [
    "It is not hard to see that LOOCV is a special case of k-fold CV in which k\n",
    "is set to equal n. In practice, one typically performs k-fold CV using k =5\n",
    "or k = 10. What is the advantage of using k =5or k = 10 rather than\n",
    "k = n? The most obvious advantage is computational. LOOCV requires\n",
    "f\n",
    "itting the statistical learning method n times. This has the potential to be\n",
    "computationally expensive (except for linear models fit by least squares,\n",
    "in which case formula (5.2) can be used). But cross-validation is a very\n",
    "general approach that can be applied to almost any statistical learning\n",
    "method. Some statistical learning methods have computationally intensive\n",
    "f\n",
    "itting procedures, and so performing LOOCV may pose computational\n",
    "problems, especially if n is extremely large. In contrast, performing 10-fold\n",
    "CV requires fitting the learning procedure only ten times, which may be\n",
    "much more feasible. As we see in Section 5.1.4, there also can be other\n",
    "non-computational advantages to performing 5-fold or 10-fold CV, which\n",
    "involve the bias-variance trade-off.\n",
    "The right-hand panel of Figure 5.4 displays nine different 10-fold CV\n",
    "estimates for the Auto data set, each resulting from a different random split\n",
    "of the observations into ten folds. As we can see from the figure, there is\n",
    "some variability in the CV estimates as a result of the variability in how\n",
    "the observations are divided into ten folds. But this variability is typically\n",
    "much lower than the variability in the test error estimates that results from\n",
    "the validation set approach (right-hand panel of Figure 5.2).\n",
    "When we examine real data, we do not know the true test MSE, and\n",
    "so it is difficult to determine the accuracy of the cross-validation estimate.\n",
    "However, if we examine simulated data, then we can compute the true\n",
    "test MSE, and can thereby evaluate the accuracy of our cross-validation\n",
    "results. In Figure 5.6, we plot the cross-validation estimates and true test\n",
    "error rates that result from applying smoothing splines to the simulated\n",
    "data sets illustrated in Figures 2.9–2.11 of Chapter 2. The true test MSE\n",
    "is displayed in blue. The black dashed and orange solid lines respectively\n",
    "show the estimated LOOCV and 10-fold CV estimates. In all three plots,\n",
    "the two cross-validation estimates are very similar.\n",
    "\n",
    "rue test MSE.\n",
    "When we perform cross-validation, our goal might be to determine how\n",
    "well a given statistical learning procedure can be expected to perform on\n",
    "independent data; in this case, the actual estimate of the test MSE is\n",
    "of interest. But at other times we are interested only in the location of\n",
    "the minimum point in the estimated test MSE curve. This is because we\n",
    "might be performing cross-validation on a number of statistical learning\n",
    "methods, or on a single method using different levels of flexibility, in order\n",
    "to identify the method that results in the lowest test error. For this purpose,\n",
    "the location of the minimum point in the estimated test MSE curve is\n",
    "important, but the actual value of the estimated test MSE is not. We find\n",
    "in Figure 5.6 that despite the fact that they sometimes underestimate the\n",
    "true test MSE, all of the CV curves come close to identifying the correct\n",
    "level of flexibility—that is, the flexibility level corresponding to the smallest\n",
    "test MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f0357",
   "metadata": {},
   "source": [
    "##### Bias-Variance Trade-Off for k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790de511",
   "metadata": {},
   "source": [
    " k-fold CV with k<nhas a compu\n",
    "tational advantage to LOOCV. But putting computational issues aside,\n",
    "a less obvious but potentially more important advantage of k-fold CV is\n",
    "that it often gives more accurate estimates of the test error rate than does\n",
    "LOOCV. This has to do with a bias-variance trade-off.\n",
    "It was mentioned in Section 5.1.1 that the validation set approach can\n",
    "lead to overestimates of the test error rate, since in this approach the\n",
    "training set used to fit the statistical learning method contains only half\n",
    "the observations of the entire data set"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
