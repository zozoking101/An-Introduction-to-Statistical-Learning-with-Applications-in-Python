{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54525e21",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5e174",
   "metadata": {},
   "source": [
    " Recall the Advertising data from Chapter 2. Figure 2.1 displays sales (in thousands of units) for a particular product as a function of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest,\n",
    " on the basis of this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation? Here are a few important questions that we might seek to address:\n",
    "\n",
    "  1. *Is there a relationship between advertising budget and sales?* Our first goal should be to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, then one might argue that no money should be spent on advertising!\n",
    "\n",
    "  2. *How strong is the relationship between advertising budget and sales?* Assuming that there is a relationship between advertising and sales, we would like to know the strength of this relationship. Does knowledge of the advertising budget provide a lot of information aboutproduct sales?\n",
    "\n",
    "  3. *Which media are associated with sales?*\n",
    " Are all three media—TV, radio, and newspaper—associated with sales, or are just one or two of the media associated? To answer this question, we must find a way to separate out the individual contribution of each medium to sales when we have spent money on all three media.\n",
    "\n",
    " 4. *How large is the association between each medium and sales?*\n",
    " For every dollar spent on advertising in a particular medium, by\n",
    " what amount will sales increase? How accurately can we predict this\n",
    " amount of increase?\n",
    "\n",
    " 5. *How accurately can we predict future sales?*\n",
    " For any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?\n",
    "\n",
    " 6. *Is the relationship linear?*\n",
    " If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. If not, then it may still be possible to transform the predictor or the response so that linear regression can be used.\n",
    "\n",
    " 7. *Is there synergy among the advertising media?*\n",
    " Perhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect, while in statistics it is called an interaction effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0c97e",
   "metadata": {},
   "source": [
    "#### Simple Linear Regression\n",
    "\n",
    "*Simple linear regression* lives up to its name: it is a very straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$. Mathematically, we can write this linear relationship as\n",
    "\n",
    "\\begin{equation}\n",
    "Y \\approx \\beta_0 + \\beta_1 X.\n",
    "\\tag{3.1}\n",
    "\\end{equation}\n",
    "\n",
    "You might read “$\\approx$” as “is approximately modeled as”. We will sometimes describe (3.1) by saying that we are regressing $Y$ on $X$ (or $Y$ onto $X$).\n",
    "\n",
    " For example, X may represent TV advertising and Y may represent sales. Then we can regress sales onto TV by fitting the model\n",
    "\n",
    " \\begin{equation}\n",
    "sales \\approx \\beta_0 + \\beta_1 \\times TV.\n",
    "\\tag{3.2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In Equation 3.1, $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the *intercept* and *slope* terms in the linear model. Together, $\\beta_0$ and $\\beta_1$ are known as the model *coefficients* or *parameters*. Once we have used our training data to produce estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising by computing\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x,\n",
    "\\tag{3.2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}$ indicates a prediction of $Y$ on the basis of $X = x$. Here we use a hat symbol, $\\hat{~}$, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2ef83",
   "metadata": {},
   "source": [
    "#### Estimating the Coefficients\n",
    "\n",
    "In practice, $\\beta_0$ and $\\beta_1$ are unknown. So before we can use (3.1) to make predictions, we must use data to estimate the coefficients. Let\n",
    "\n",
    "$$(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$$\n",
    "\n",
    "represent $n$ observation pairs, each of which consists of a measurement of $X$ and a measurement of $Y$. In the *Advertising* example, this data set consists of the TV advertising budget and product sales in $n = 200$ different markets. (Recall that this data are displayed in Figure 2.1.) Our goal is to obtain coefficient estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that the linear model (3.1) fits the available data well—that is, so that $y_i \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ for $i = 1, \\ldots, n$. In other words, we want to find an intercept $\\hat{\\beta}_0$ and a slope $\\hat{\\beta}_1$ such that the resulting line is as close as possible to the $n = 200$ data points. There are a number of ways of measuring *closeness*. However, by far the most common approach involves minimizing the *least squares criterion*, and we take that approach in this chapter. \n",
    "\n",
    "Let $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ be the prediction for $Y$ based on the $i$ th value of $X$. Then $e_i = y_i - \\hat{y}_i$ represents the $i$th *residual*—this is the difference between the $i$ th observed response value and the $i$th response value that is predicted by our linear model. We define the *residual sum of squares* (RSS) as\n",
    "\n",
    "$$RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,$$\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS} = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n",
    "\\tag{3.3}\n",
    "\\end{equation}\n",
    "\n",
    "The least squares approach chooses $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ to minimize the RSS. Using some calculus, one can show that the minimizers are\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 \\;=\\; \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}\n",
    "{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 \\;=\\; \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ and $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ are the sample means. In other words, (3.4) defines the *least squares coefficient estimates* for simple linear regression.\n",
    "\n",
    "Figure 3.1 displays the simple linear regression fit to the *Advertising* data, where $\\hat{\\beta}_0 = 7.03$ and $\\hat{\\beta}_1 = 0.047$. In other words, according to this approximation, an additional \\$1000 spent on TV advertising is associated with selling approximately 47 additional units of the product. In Figure 3.2, we have computed RSS for a number of values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, using the advertising and sales as the response and $X$ as the predictor. In each plot, the red dot represents the pair of least squares estimates $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ given by (3.4). These values clearly minimize the RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16489a25",
   "metadata": {},
   "source": [
    "#### Assessing the Accuracy of the Coefficient Estimates\n",
    "\n",
    "Recall from $(2.1)$ that we assume that the *true* relationship between $X$ and $Y$ takes the form $Y = f(X) + \\epsilon$ for some unknown function $f$, where $\\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as\n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\beta_0 + \\beta_1 X + \\epsilon.\n",
    "\\tag{3.5}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\beta_0$ is the intercept term—that is, the expected value of $Y$ when $X = 0$, and $\\beta_1$ quantifies the association between $X$ and $Y$. As we saw in the previous section, we use data to produce estimates of $\\beta_0$ and $\\beta_1$ via the *least squares* approach. If the true relationship is probably not linear, there may be relationships not captured by this model, and there may be measurement error. We typically assume that the error term is independent of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026a7f8",
   "metadata": {},
   "source": [
    "The analogy between linear regression and estimation of the mean of a\n",
    "random variable is an apt one based on the concept of bias. \n",
    "\n",
    "If we use the bias sample mean $\\hat{\\mu}$ to estimate $\\mu$, this estimate is **unbiased**, in the sense that on average we expect $\\hat{\\mu}$ to equal $\\mu$.\n",
    "\n",
    "What exactly does this mean? It means that on the basis of one particular set of observations $y_1, \\dots, y_n$, $\\hat{\\mu}$ might overestimate $\\mu$, and on the basis of another set of observations, $\\hat{\\mu}$ might underestimate $\\mu$. \n",
    "\n",
    "But if we could average a huge number of estimates of $\\mu$ obtained from a huge number of sets of observations, then this average would exactly equal $\\mu$. Hence, an **unbiased estimator** does not systematically over- or under-estimate the true parameter. \n",
    "\n",
    "The property of unbiasedness holds for the least squares coefficient estimates given by Equation (3.4) as well: if we estimate $\\beta_0$ and $\\beta_1$ on the basis of a particular data set, then our estimates won’t be exactly equal to $\\beta_0$ and $\\beta_1$. \n",
    "\n",
    "But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on! In fact, we can see from the right-hand panel of Figure 3.3 that the average of many least squares lines, each estimated from a separate data set, is pretty close to the true population regression line.\n",
    "\n",
    "We continue the analogy with the estimation of the population mean $\\mu$ of a random variable $Y$. A natural question is as follows: how accurate is the sample mean $\\bar{y}$ as an estimate of $\\mu$? We have established that the average of $\\bar{y}$'s over many data sets will be very close to $\\mu$, but that a single estimate $\\bar{y}$ may be a substantial underestimate or overestimate of $\\mu$. How far off will that single estimate of $\\mu$ be? In general, we answer this question by computing the *standard error* of $\\bar{y}$, written as $\\mathrm{SE}(\\bar{y})$. We have the well-known formula\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Var}(\\bar{y}) = \\mathrm{SE}(\\bar{y})^2 = \\frac{\\sigma^2}{n},\n",
    "\\tag{3.7}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$. Roughly speaking, the standard error tells us the average amount that this estimate $\\bar{y}$ differs from the actual value of $\\mu$. Equation 3.7 also tells us how this deviation shrinks with $n$—the more observations we have, the smaller the standard error of $\\bar{y}$. In a similar vein, we can wonder how close $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are to the true values $\\beta_0$ and $\\beta_1$. To compute the standard errors associated with $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, we use the following formulas:\n",
    "\n",
    "$$\n",
    "\\mathrm{SE}(\\hat{\\beta}_0)^2 \n",
    "= \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right]\n",
    "\\quad\n",
    "\\mathrm{SE}(\\hat{\\beta}_1)^2 \n",
    "= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "\\tag{3.8}\n",
    "$$\n",
    "\n",
    "where $\\sigma^2 = \\mathrm{Var}(\\epsilon)$. For these formulas to be strictly valid, we need to assume that the errors $\\epsilon_i$ for each observation have common variance $\\sigma^2$ and are uncorrelated. This is clearly not true in Figure 3.1, but the formula still turns out to be a good approximation. Notice in the formula that $\\mathrm{SE}(\\hat{\\beta}_1)$ is smaller when the $x_i$ are more spread out; intuitively we have more *leverage* to estimate a slope when this is the case. We also see that $\\mathrm{SE}(\\hat{\\beta}_0)$ would be the same as $\\mathrm{SE}(\\bar{y})$ if we even zero (in which case $\\bar{x}$ would be equal to zero). In general, $\\sigma^2$ is not known, but can be estimated from the data. This estimate of $\\sigma$ is the *residual standard error*, and is given by the formula\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n e_i^2} = \\sqrt{\\frac{\\text{RSS}}{n-2}}.\n",
    "$$\n",
    "\n",
    "Strictly speaking, when $\\sigma^2$ is estimated from the data we should also use $\\mathrm{SE}_b(\\hat{\\beta}_1)$ to indicate that an estimate has been made. But for simplicity we will not use this extra “hat”.\n",
    "\n",
    "Standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value being estimated. The standard error comes into play in the formula for upper and lower confidence interval endpoints. A 95% confidence interval has the following property: if we compare a large number of sets, 95% of confidence intervals constructed in this way will contain the true unknown value. For example, a 95% confidence interval for the true value of $\\beta_1$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 \\pm 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1),\n",
    "$$\n",
    "\n",
    "meaning that there is approximately a 95% chance that the true value of $\\beta_1$ falls within the interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95875529",
   "metadata": {},
   "source": [
    "That is, there is approximately a 95% chance that the interval\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[ \\hat{\\beta}_1 - 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1),\\ \\hat{\\beta}_1 + 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1)\\right]\n",
    "\\tag{3.10}\n",
    "\\end{equation}\n",
    "\n",
    "will contain the true value of $\\beta_1$. Similarly, a confidence interval for $\\beta_0$ approximately takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 \\pm 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_0).\n",
    "\\tag{3.11}\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the advertising data, the 95% confidence interval for $\\beta_0$ is $[6.130, 7.935]$ and the 95% confidence interval for $\\beta_1$ is $[0.042, 0.053]$. Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between 6,130 and 7,935 units. Furthermore, for each \\$1,000 increase in television advertising, there will be an average increase in sales of between 42 and 53 units.\n",
    "\n",
    "Standard errors can also be used to perform *hypothesis tests* on the coefficients. The most common hypothesis test involves testing the *null hypothesis* of\n",
    "\n",
    "\\begin{equation}\n",
    "H_0: \\text{There is no relationship between $X$ and $Y$}\n",
    "\\tag{3.12}\n",
    "\\end{equation}\n",
    "\n",
    "versus the *alternative hypothesis*\n",
    "\n",
    "\\begin{equation}\n",
    "H_a: \\text{There is some relationship between $X$ and $Y$}\n",
    "\\tag{3.13}\n",
    "\\end{equation}\n",
    "\n",
    "Mathematically, this corresponds to testing\n",
    "\n",
    "\\[\n",
    "H_0 : \\beta_1 = 0\n",
    "\\]\n",
    "\\[\n",
    "H_a : \\beta_1 \\neq 0.\n",
    "\\]\n",
    "\n",
    "since if $\\beta_1 = 0$ then the model (3.5) reduces to $Y = \\beta_0 + \\epsilon$, and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine whether $\\hat{\\beta}_1$, our estimate for $\\beta_1$, is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero. How far is far enough? This of course depends on the accuracy of $\\hat{\\beta}_1$—that is, it depends on $\\mathrm{SE}(\\hat{\\beta}_1)$. If $\\mathrm{SE}(\\hat{\\beta}_1)$ is small, then even relatively small values of $\\hat{\\beta}_1$ may provide strong evidence that $\\beta_1 \\neq 0$, and hence that there is a relationship between $X$ and $Y$. In contrast, if $\\mathrm{SE}(\\hat{\\beta}_1)$ is large, then $\\hat{\\beta}_1$ must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a *t*-statistic, given by\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\hat{\\beta}_1}{\\mathrm{SE}(\\hat{\\beta}_1)}\n",
    "\\tag{3.14}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "That is, there is approximately a 95% chance that the interval\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[ \\hat{\\beta}_1 - 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1),\\ \\hat{\\beta}_1 + 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1)\\right]\n",
    "\\tag{3.10}\n",
    "\\end{equation}\n",
    "\n",
    "will contain the true value of $\\beta_1$. Similarly, a confidence interval for $\\beta_0$ approximately takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 \\pm 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_0).\n",
    "\\tag{3.11}\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the advertising data, the 95% confidence interval for $\\beta_0$ is $[6.130, 7.935]$ and the 95% confidence interval for $\\beta_1$ is $[0.042, 0.053]$. Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between 6,130 and 7,935 units. Furthermore, for each \\$1,000 increase in television advertising, there will be an average increase in sales of between 42 and 53 units.\n",
    "\n",
    "Standard errors can also be used to perform *hypothesis tests* on the coefficients. The most common hypothesis test involves testing the *null hypothesis* of\n",
    "\n",
    "\\begin{equation}\n",
    "H_0: \\text{There is no relationship between $X$ and $Y$}\n",
    "\\tag{3.12}\n",
    "\\end{equation}\n",
    "\n",
    "versus the *alternative hypothesis*\n",
    "\n",
    "\\begin{equation}\n",
    "H_a: \\text{There is some relationship between $X$ and $Y$}\n",
    "\\tag{3.13}\n",
    "\\end{equation}\n",
    "\n",
    "Mathematically, this corresponds to testing\n",
    "\n",
    "$$\n",
    "H_0 : \\beta_1 = 0\n",
    "$$\n",
    "versus\n",
    "$$\n",
    "H_a : \\beta_1 \\neq 0.\n",
    "$$\n",
    "\n",
    "since if $\\beta_1 = 0$ then the model (3.5) reduces to $Y = \\beta_0 + \\epsilon$, and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine whether $\\hat{\\beta}_1$, our estimate for $\\beta_1$, is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero. How far is far enough? This of course depends on the accuracy of $\\hat{\\beta}_1$—that is, it depends on $\\mathrm{SE}(\\hat{\\beta}_1)$. If $\\mathrm{SE}(\\hat{\\beta}_1)$ is small, then even relatively small values of $\\hat{\\beta}_1$ may provide strong evidence that $\\beta_1 \\neq 0$, and hence that there is a relationship between $X$ and $Y$. In contrast, if $\\mathrm{SE}(\\hat{\\beta}_1)$ is large, then $\\hat{\\beta}_1$ must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a *t*-statistic, given by\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\hat{\\beta}_1}{\\mathrm{SE}(\\hat{\\beta}_1)}\n",
    "\\tag{3.14}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "|               | Coefficient | Std. error | t-statistic | p-value |\n",
    "|---------------|-------------|------------|-------------|---------|\n",
    "| Intercept     | 7.0325      | 0.4578     | 15.36       | < 0.0001|\n",
    "| TV            | 0.0475      | 0.0027     | 17.67       | < 0.0001|\n",
    "\n",
    " For the Advertising data, coefficients of the least squares model \n",
    "for the regression of number of units sold on TV advertising budget. An increase \n",
    "of \\$1000 in the TV advertising budget is associated with an increase in sales by \n",
    "approximately 47.5 units. (Recall that in this dataset, sales are measured in \n",
    "thousands of units, and the TV variable is in thousands of dollars.)\n",
    "\n",
    "\n",
    "The larger the number of standard deviations that $\\hat{\\beta}_j$ is away from 0, \n",
    "i.e. there really is no relationship between $X$ and $Y$, then we expect that (3.14) \n",
    "will have a t-distribution with $n - 2$ degrees of freedom. The t-distribution \n",
    "has a bell shape and for values of t greater than approximately 2 it is quite \n",
    "similar to the standard normal distribution. Consequently, it is a simple matter \n",
    "to compute the probability of observing any number equal to $|t|$ or larger in \n",
    "absolute value, assuming $\\beta_j = 0$. We call this probability the *p-value*. \n",
    "\n",
    "Roughly speaking, we interpret the p-value as follows: a small p-value indicates \n",
    "that it is unlikely to observe such a substantial association between the predictor \n",
    "and the response due to chance, in the absence of any real association between the \n",
    "predictor and the response. Hence, if we see a small p-value, then we can infer that \n",
    "there is an association between the predictor and the response. We reject the \n",
    "*null hypothesis*—that is, we declare a relationship to exist between predictor \n",
    "and response—if the p-value is small enough. Typical p-value cutoffs for rejecting \n",
    "the null hypothesis are 5% or 1%, although this topic will be explored later in much \n",
    "greater detail. When $n = 30$, these correspond to t-statistics (3.14) \n",
    "of around 2 and 2.75, respectively. \n",
    "\n",
    "The table provides details of the least squares model for the regression of number \n",
    "of units sold on TV advertising budget for the Advertising data. Note that the \n",
    "p-values for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are very large relative to their \n",
    "standard errors, so the t-statistics are also large; the probabilities of seeing \n",
    "such values of t if $H_0$ is true are virtually zero. Hence we can conclude that \n",
    "$\\beta_0 \\neq 0$ and $\\beta_1 \\neq 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c6285",
   "metadata": {},
   "source": [
    "#### Assessing the Accuracy of the Model\n",
    "\n",
    "If we reject the null hypothesis (3.12) in favor of the alternative hypothesis (3.13), \n",
    "it is natural to want to quantify the *extent* to which the regression model fits \n",
    "the data. The quality of a linear regression fit is typically assessed using two \n",
    "related quantities: the *residual standard error* (RSE) and the $R^2$ statistic.\n",
    "\n",
    "| Quantity                  | Value |\n",
    "|---------------------------|-------|\n",
    "| Residual standard error   | 3.26  |\n",
    "| $R^2$                     | 0.612 |\n",
    "| F-statistic               | 312.1 |\n",
    "\n",
    "For the Advertising data, more information about the least \n",
    "squares model for the regression of number of units sold on TV advertising budget.\n",
    "\n",
    "\n",
    "The table displays the RSE, the $R^2$ statistic, and the F-statistic (to be described \n",
    "in Section 3.2.2) for the linear regression of number of units sold on TV advertising budget.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e917bc",
   "metadata": {},
   "source": [
    "##### Residual Standard Error\n",
    "\n",
    "Recall from the model (3.5) that associated with each observation is an error term $\\epsilon$. \n",
    "Due to the presence of these error terms, even if we knew the true regression line \n",
    "(i.e. even if $\\beta_0$ and $\\beta_1$ were known), we would not be able to perfectly \n",
    "predict $Y$ from $X$. The RSE is an estimate of the standard deviation of $\\epsilon$. \n",
    "\n",
    "Roughly speaking, it is the average amount that the response will deviate from the \n",
    "true regression line. It is computed using the formula\n",
    "\n",
    "\\[\n",
    "RSE = \\sqrt{\\frac{1}{n-2}RSS} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}.\n",
    "\\tag{3.15}\n",
    "\\]\n",
    "\n",
    "Note that RSS was defined in Section 3.1.1, and is given by the formula\n",
    "\n",
    "\\[\n",
    "RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2.\n",
    "\\tag{3.16}\n",
    "\\]\n",
    "\n",
    "\n",
    "In the case of the advertising data, we see from the linear regression output in \n",
    "Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate \n",
    "from the true regression line by approximately 3.26 units, on average. Another way \n",
    "to think about this is that even if the model were correct and the true values of \n",
    "the unknown coefficients $\\beta_0$ and $\\beta_1$ were known exactly, any prediction \n",
    "of sales on the basis of TV advertising would still be off by about 3.26 units on \n",
    "average. Of course, whether or not 3.26 units is an acceptable prediction error \n",
    "depends on the application. In this case, the average sales are around 14,000 units, \n",
    "so 3.26 units correspond to a percentage error of $3.26/14,000 \\approx 0.02\\%$. \n",
    "\n",
    "\n",
    "The RSE is considered a measure of the *lack of fit* of the model (3.5) to the data. \n",
    "If the predictions obtained using the model are very close to the true outcomes \n",
    "— that is, if $\\hat{y}_i$ is very close to $y_i$ for $i = 1, \\dots, n$ — then \n",
    "(3.15) will be small, and we can conclude that the model fits the data very well. \n",
    "On the other hand, if $\\hat{y}_i$ is far from $y_i$ for one or more observations, \n",
    "then the RSE may be quite large, indicating that the model doesn’t fit the data well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367dbec",
   "metadata": {},
   "source": [
    "##### $R^2$ Statistic\n",
    " The RSE provides an absolute measure of lack of fit of the model $(3.5)$ to the data. But since it is measured in the units of Y, it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit. \n",
    "\n",
    "It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of $Y$.\n",
    "\n",
    "To calculate $R^2$, we use the formula\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}}\n",
    "= 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n",
    "\\tag{3.17}\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\text{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "is the **total sum of squares**, and RSS is defined in Equation (3.16).  \n",
    "\n",
    "TSS measures the total variance in the response $Y$, and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. \n",
    "\n",
    "Hence, $\\text{TSS} - \\text{RSS}$ measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.  \n",
    "\n",
    "An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response is explained by the regression. A number near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, the error variance $\\sigma^2$ is high, or both.\n",
    "\n",
    "The $R^2$ statistic is a measure of the linear relationship between $X$ and $Y$. \n",
    "\n",
    "Recall that correlation, defined as\n",
    "\n",
    "$$\n",
    "\\text{Cor}(X, Y) \\;=\\; \n",
    "\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}\n",
    "{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\, \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}\n",
    "\\tag{3.18}\n",
    "$$\n",
    "\n",
    "is also a measure of the linear relationship between $X$ and $Y$.  \n",
    "\n",
    "This suggests that we might be able to use $r = \\text{Cor}(X, Y)$ instead of $R^2$ in order to assess the fit of the linear model.\n",
    "\n",
    "\n",
    "Simple regression of sales on radio\n",
    "\n",
    "| Coefficient | Std. Error | t-statistic | p-value  |\n",
    "|-------------|------------|-------------|----------|\n",
    "| Intercept   | 9.312      | 0.563       | 16.54    | <0.0001 |\n",
    "| radio       | 0.203      | 0.020       | 9.92     | <0.0001 |\n",
    "\n",
    "\n",
    "Simple regression of sales on newspaper\n",
    "\n",
    "| Coefficient | Std. Error | t-statistic | p-value  |\n",
    "|-------------|------------|-------------|----------|\n",
    "| Intercept   | 12.351     | 0.621       | 19.88    | <0.0001 |\n",
    "| newspaper   | 0.055      | 0.017       | 3.30     | 0.00115 |\n",
    "\n",
    "\n",
    "More simple linear regression models for the Advertising data.  \n",
    "\n",
    "Coefficients of the simple linear regression model for number of units sold on Top: radio advertising budget. Bottom: newspaper advertising budget.\n",
    "\n",
    "A \\$1,000 increase in spending on radio advertising is associated with an average increase in sales by around **203 units**, while the same increase in spending on newspaper advertising is associated with an average increase in sales by around **55 units**.  \n",
    "\n",
    "(Note that the sales variable is in thousands of units, and the radio and newspaper variables are in thousands of dollars.)  \n",
    "\n",
    "\n",
    "In the linear regression setting, we have\n",
    "\n",
    "$$\n",
    "R^2 = r^2\n",
    "$$\n",
    "\n",
    "In other words, the squared correlation and the $R^2$ statistic are identical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8290d3",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression\n",
    "\n",
    "Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. \n",
    "\n",
    "For example, in the **Advertising** data, we have examined the relationship between sales and TV advertising. We also have data for the amount of money spent advertising on the radio and in newspapers, and we may want to know whether either of these two media is associated with sales. How can we extend our analysis of the advertising data in order to accommodate these two additional predictors?\n",
    "\n",
    "One option is to run three separate simple linear regressions, each of which uses a different advertising medium as a predictor. For instance, we can fit a simple linear regression to predict sales on the basis of the amount spent on radio advertisements. Results are shown in **Table 3.3 (top table)**. We find that a \\$1,000 increase in spending on radio advertising is associated with an increase in sales of around **203 units**. **Table 3.3 (bottom table)** contains the least squares coefficients for a simple linear regression of sales onto newspaper advertising budget. A \\$1,000 increase in newspaper advertising budget is associated with an increase in sales of approximately **55 units**.\n",
    "\n",
    "\n",
    "However, the approach of fitting a separate simple linear regression model for each predictor is not entirely satisfactory.  \n",
    "\n",
    "- First of all, it is unclear how to make a single prediction of sales given the three advertising media budgets, since each of the budgets is associated with a separate regression equation.  \n",
    "- Second, each of the three regression equations ignores the other two media in forming estimates for the regression coefficients.  \n",
    "\n",
    "We will see shortly that if the media budgets are correlated with each other in the 200 markets in our data set, then this can lead to very misleading estimates of the association between each media budget and sales.\n",
    "\n",
    "Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model (3.5) so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model.\n",
    "\n",
    "In general, suppose that we have $p$ distinct predictors. Then the multiple linear regression model takes the form\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n",
    "\\tag{3.19}\n",
    "$$\n",
    "\n",
    "where $X_j$ represents the $j$-th predictor and $\\beta_j$ quantifies the association between that variable and the response. We interpret $\\beta_j$ as the average effect on $Y$ of a one-unit increase in $X_j$, holding all other predictors fixed.\n",
    "\n",
    "In the advertising example, (3.19) becomes:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\, \\text{TV} + \\beta_2 \\, \\text{radio} + \\beta_3 \\, \\text{newspaper} + \\varepsilon\n",
    "\\tag{3.20}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff212b",
   "metadata": {},
   "source": [
    "##### Estimating the Regression Coefficients\n",
    "\n",
    "As was the case in the simple linear regression setting, the regression coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ in (3.19) are unknown, and must be estimated. \n",
    "\n",
    "Given estimates $\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$, we can make predictions using the formula\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_p x_p\n",
    "\\tag{3.21}\n",
    "$$\n",
    "\n",
    "The parameters are estimated using the same least squares approach that we saw in the context of simple linear regression. We choose $\\beta_0, \\beta_1, \\dots, \\beta_p$ to minimize the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\text{RSS} = \\sum_{i=1}^n \\Big( y_i - \\hat{y}_i \\Big)^2 \n",
    "= \\sum_{i=1}^n \\Big( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\cdots - \\hat{\\beta}_p x_{ip} \\Big)^2\n",
    "\\tag{3.22}\n",
    "$$\n",
    "\n",
    "The values $\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$ that minimize (3.22) are the **multiple least squares regression coefficient estimates**. Unlike the simple linear regression coefficient estimates given in (3.4), the multiple regression coefficient estimates have somewhat complicated forms that are most easily represented using matrix algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbcaff",
   "metadata": {},
   "source": [
    "The Multiple Regression Coefficient Estimates table displays the multiple regression coefficient estimates when TV,\n",
    "radio, and newspaper advertising budgets are used to predict product sales\n",
    "using the Advertising data. We interpret these results as follows: for a given\n",
    "amount of TV and newspaper advertising, spending an additional $1,000 on\n",
    "radio advertising is associated with approximately 189 units of additional\n",
    "sales. Comparing these coefficient estimates to those displayed in Tables 3.1\n",
    "and 3.3, we notice that the multiple regression coefficient estimates for\n",
    "TV and radio are pretty similar to the simple linear regression coefficient\n",
    "estimates. However, while the newspaper regression coefficient estimate in\n",
    "Table 3.3 was significantly non-zero, the coefficient estimate for newspaper\n",
    "in the multiple regression model is close to zero, and the corresponding \n",
    "*p*-value is no longer significant, with a value around **0.86**.  \n",
    "\n",
    "This illustrates that the simple and multiple regression coefficients can be\n",
    "quite different. This difference stems from the fact that in the simple regression\n",
    "case, the slope term represents the average increase in product sales associated\n",
    "with a $1,000 increase in newspaper advertising, **ignoring other predictors such as\n",
    "TV and radio**.  \n",
    "\n",
    "By contrast, in the multiple regression setting, the coefficient for newspaper\n",
    "represents the average increase in product sales associated with increasing \n",
    "newspaper spending by $1,000 **while holding TV and radio fixed**.\n",
    "\n",
    "\n",
    "Multiple Regression Coefficient Estimates\n",
    "\n",
    "| Coefficient | Std. error | t-statistic | p-value   |\n",
    "|-------------|------------|-------------|-----------|\n",
    "| Intercept   | 2.939      | 0.3119      | 9.42      | <0.0001 |\n",
    "| TV          | 0.046      | 0.0014      | 32.81     | <0.0001 |\n",
    "| radio       | 0.189      | 0.0086      | 21.89     | <0.0001 |\n",
    "| newspaper   | 0.001      | 0.0059      | 0.18      | 0.8599  |\n",
    "\n",
    "*For the Advertising data, least squares coefficient estimates of the multiple\n",
    "linear regression of number of units sold on TV, radio, and newspaper\n",
    "advertising budgets.*\n",
    "\n",
    "\n",
    "Does it make sense for the multiple regression to suggest no relationship\n",
    "between sales and newspaper while the simple linear regression implies the\n",
    "opposite?  \n",
    "\n",
    "In fact it does. Consider the **correlation matrix** for the three predictor\n",
    "variables and response variable, displayed in Table 3.5. Notice that the\n",
    "correlation between radio and newspaper is **0.35**. This indicates\n",
    "that markets with high newspaper advertising tend to also have high radio\n",
    "advertising.\n",
    "\n",
    "\n",
    "Correlation Matrix (TV, radio, newspaper, sales)\n",
    "\n",
    "|             | TV     | radio  | newspaper | sales  |\n",
    "|-------------|--------|--------|-----------|--------|\n",
    "| TV          | 1.0000 | 0.0548 | 0.0567    | 0.7822 |\n",
    "| radio       | 0.0548 | 1.0000 | 0.3541    | 0.5762 |\n",
    "| newspaper   | 0.0567 | 0.3541 | 1.0000    | 0.2283 |\n",
    "| sales       | 0.7822 | 0.5762 | 0.2283    | 1.0000 |\n",
    "\n",
    "\n",
    "Now suppose that the multiple regression is correct and\n",
    "**newspaper advertising is not associated with sales, but radio advertising\n",
    "is associated with sales.** Then in markets where we spend more on radio\n",
    "our sales will tend to be higher, and as our correlation matrix shows, we\n",
    "also tend to spend more on newspaper advertising in those same markets.  \n",
    "\n",
    "Hence, in a simple linear regression which only examines sales versus\n",
    "newspaper, we will observe that higher values of newspaper tend to be\n",
    "associated with higher values of sales, even though newspaper advertising is\n",
    "not directly associated with sales.  \n",
    "\n",
    "So newspaper advertising is a **surrogate** for radio advertising; newspaper\n",
    "gets “credit” for the association between radio on sales.\n",
    "\n",
    "\n",
    "This slightly counterintuitive result is very common in many real-life\n",
    "situations. Consider an absurd example to illustrate the point:  \n",
    "\n",
    "- Running a regression of **shark attacks versus ice cream sales** for data collected at\n",
    "a given beach community over a period of time would show a positive\n",
    "relationship, similar to that seen between sales and newspaper.  \n",
    "- Of course no one has (yet) suggested that ice creams should be banned at beaches\n",
    "to reduce shark attacks.  \n",
    "\n",
    "In reality:  \n",
    "- Higher temperatures cause more people to visit the beach, which in turn \n",
    "results in more ice cream sales and more shark attacks.  \n",
    "- A **multiple regression** of shark attacks onto ice cream sales **and temperature**\n",
    "reveals that, as intuition implies, ice cream sales is no longer a significant\n",
    "predictor after adjusting for temperature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fc5d5",
   "metadata": {},
   "source": [
    "##### Some Important Questions\n",
    "When we perform multiple linear regression, we usually are interested in\n",
    "answering a few important questions.\n",
    " 1. Is at least one of the predictors X1,X2,...,Xp useful in predicting\n",
    " the response?\n",
    " 2. Do all the predictors help to explain Y , or is only a subset of the\n",
    " predictors useful?\n",
    " 3. How well does the model fit the data?\n",
    " 4. Given a set of predictor values, what response value should we predict,\n",
    " and how accurate is our prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb882fd",
   "metadata": {},
   "source": [
    "##### One: Is There a Relationship Between the Response and Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e8bda",
   "metadata": {},
   "source": [
    "Recall that in the simple linear regression setting, in order to determine  \n",
    "whether there is a relationship between the response and the predictor, we  \n",
    "can simply check whether $\\beta_1 = 0$.  \n",
    "\n",
    "In the multiple regression setting with $p$ predictors, we need to ask whether  \n",
    "all of the regression coefficients are zero, i.e.:  \n",
    "\n",
    "$$ \n",
    "H_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\n",
    "$$  \n",
    "\n",
    "versus the alternative  \n",
    "\n",
    "$$ \n",
    "H_a : \\text{at least one } \\beta_j \\neq 0\n",
    "$$  \n",
    "\n",
    "This hypothesis test is performed by computing the **F-statistic**:  \n",
    "\n",
    "$$\n",
    "F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} \\tag{3.23}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "\n",
    "- $TSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$ (Total Sum of Squares)  \n",
    "- $RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ (Residual Sum of Squares)  \n",
    "\n",
    "If the linear model assumptions are correct, then:  \n",
    "\n",
    "$$\n",
    "E\\left\\{\\frac{RSS}{n - p - 1}\\right\\} = \\sigma^2\n",
    "$$  \n",
    "\n",
    "and, provided $H_0$ is true,  \n",
    "\n",
    "$$\n",
    "E\\left\\{\\frac{TSS - RSS}{p}\\right\\} = \\sigma^2\n",
    "$$  \n",
    "\n",
    "- When there is **no relationship** between the response and predictors,  \n",
    "  the F-statistic should be close to 1.  \n",
    "\n",
    "- When $H_a$ is true, then  \n",
    "\n",
    "$$\n",
    "E\\left\\{\\frac{TSS - RSS}{p}\\right\\} > \\sigma^2\n",
    "$$  \n",
    "\n",
    "so we expect $F > 1$.  \n",
    "\n",
    "For example, the F-statistic for the multiple linear regression model obtained  \n",
    "by regressing **sales** onto **radio, TV, and newspaper** is:  \n",
    "\n",
    "$$\n",
    "F = 570\n",
    "$$  \n",
    "\n",
    "Since this is far larger than 1, it provides compelling evidence against the  \n",
    "null hypothesis $H_0$.  \n",
    "The large F-statistic suggests that at least one of the advertising media  \n",
    "must be related to sales.  However, if the F-statistic had been closer to 1, we would not reject $H_0$. How large does the F-statistic need to be before we can reject $H_0$?\n",
    "\n",
    "And how large must the F-statistic be before we can conclude that there is a relationship?  \n",
    "It turns out that the answer depends on the values of $n$ and $p$.  \n",
    "\n",
    "- When $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$.  \n",
    "- In contrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small.  \n",
    "\n",
    "| **Quantity**               | **Value** |\n",
    "|-----------------------------|-----------|\n",
    "| Residual standard error     | 1.69      |\n",
    "| \\(R^2\\)                     | 0.897     |\n",
    "| F-statistic                 | 570       |\n",
    "\n",
    "More information about the least squares model for the regression  \n",
    "of number of units sold on TV, newspaper, and radio advertising budgets in the  \n",
    "Advertising data. Other information about this model was displayed in the Multiple Regression Coefficient Estimates table.\n",
    "\n",
    "When $H_0$ is true and the errors $\\epsilon_i$ have a normal distribution, the F-statistic follows an **F-distribution**.  \n",
    "For any given value of $n$ and $p$, statistical software can be used to compute the **p-value** associated with the F-statistic.  \n",
    "Based on this p-value, we can determine whether or not to reject $H_0$.  \n",
    "\n",
    "For the advertising data, the p-value associated with the F-statistic in Table 3.6 is essentially zero,  \n",
    "so we have extremely strong evidence that at least one of the media is associated with increased sales.  \n",
    "\n",
    "\n",
    "In (3.23) we are testing $H_0$ that **all the coefficients are zero**.  \n",
    "Sometimes, we want to test that a particular subset of $q$ of the coefficients are zero.  \n",
    "This corresponds to the null hypothesis:  \n",
    "\n",
    "$$\n",
    "H_0 : \\beta_{p-q+1} = \\beta_{p-q+2} = \\cdots = \\beta_p = 0\n",
    "$$  \n",
    "\n",
    "where (for convenience) the variables chosen for omission are listed at the end.  \n",
    "\n",
    "In this case we fit a second model that uses all the variables except those last $q$.  \n",
    "Suppose the residual sum of squares for that reduced model is $RSS_0$.  \n",
    "Then the appropriate F-statistic is:  \n",
    "\n",
    "$$\n",
    "F = \\frac{(RSS_0 - RSS)/q}{RSS/(n - p - 1)} \\tag{3.24}\n",
    "$$  \n",
    "\n",
    "\n",
    "Notice that in Table 3.4, for each individual predictor a **t-statistic** and a **p-value** were reported.  \n",
    "These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors.  \n",
    "\n",
    "It turns out that each of these is exactly equivalent to the F-test that omits that single variable from the model (i.e. $q=1$ in (3.24)).  \n",
    "So it reports the **partial effect** of adding that variable to the model.  \n",
    "\n",
    "For example, as discussed earlier:  \n",
    "- The p-values indicate that **TV** and **radio** are related to sales.  \n",
    "- There is no evidence that **newspaper** is associated with sales, when TV and radio are held fixed.   \n",
    "\n",
    "Given the individual p-values, it may seem that if any one of them is small, then at least one predictor is related to the response.  \n",
    "However, this reasoning is flawed, especially when the number of predictors $p$ is large.  \n",
    "\n",
    "- Suppose $p=100$ and $H_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$ is true.  \n",
    "- In this case, no variable is truly associated with the response.  \n",
    "- However, by chance, about 5% of the p-values will fall below 0.05.  \n",
    "- So we expect around five small p-values even in the absence of any real associations.  \n",
    "- In fact, it is very likely that we will observe at least one p-value below 0.05 by chance!  \n",
    "\n",
    "Hence, if we rely only on individual t-tests, we are very likely to **incorrectly conclude** that there is a relationship.  \n",
    "\n",
    "The **F-statistic** avoids this problem, because it adjusts for the number of predictors.  \n",
    "If $H_0$ is true, there is only a 5% chance that the F-statistic will produce a p-value below 0.05,  \n",
    "regardless of the number of predictors or the number of observations.  \n",
    "\n",
    "Limitations of the F-statistic  \n",
    "\n",
    "The F-statistic is useful when $p$ is relatively small compared to $n$.  \n",
    "However:  \n",
    "\n",
    "- If $p > n$, there are more coefficients $\\beta_j$ to estimate than there are observations.  \n",
    "- In this case, we cannot fit the multiple linear regression model using least squares.  \n",
    "- As a result, the F-statistic (and many related concepts) cannot be used.  \n",
    "\n",
    "When $p$ is large, alternative approaches—such as **forward selection** and other model selection techniques—are required.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb3fba",
   "metadata": {},
   "source": [
    "## Two: Deciding on Important Variables\n",
    "\n",
    "As discussed in the previous section, the first step in a multiple regression\n",
    "analysis is to compute the F-statistic and to examine the associated *p*-value.  \n",
    "If we conclude on the basis of that *p*-value that at least one of the\n",
    "predictors is related to the response, then it is natural to wonder **which are\n",
    "the guilty ones!** We could look at the individual *p*-values as in Table 3.4,\n",
    "but as discussed (and as further explored in Chapter 13), if \\(p\\) is large we\n",
    "are likely to make some false discoveries.\n",
    "\n",
    "It is possible that all of the predictors are associated with the response,\n",
    "but it is more often the case that the response is only associated with\n",
    "a subset of the predictors. The task of determining which predictors are\n",
    "associated with the response, in order to fit a single model involving only\n",
    "those predictors, is referred to as **variable selection**. The variable selection\n",
    "problem is studied extensively in Chapter 6, and so here we will provide\n",
    "only a brief outline of some classical approaches.\n",
    "\n",
    "---\n",
    "\n",
    "### Considering Subsets of Models\n",
    "\n",
    "Ideally, we would like to perform variable selection by trying out a lot of\n",
    "different models, each containing a different subset of the predictors.  \n",
    "\n",
    "For instance, if \\(p = 2\\), then we can consider four models:  \n",
    "1. A model containing no variables.  \n",
    "2. A model containing \\(X_1\\) only.  \n",
    "3. A model containing \\(X_2\\) only.  \n",
    "4. A model containing both \\(X_1\\) and \\(X_2\\).  \n",
    "\n",
    "We can then select the best model out of all of the models that we have\n",
    "considered.  \n",
    "\n",
    "How do we determine which model is best? Various statistics can be used to\n",
    "judge the quality of a model. These include:  \n",
    "- **Mallow’s \\(C_p\\)**  \n",
    "- **Akaike Information Criterion (AIC)**  \n",
    "- **Bayesian Information Criterion (BIC)**  \n",
    "- **Adjusted \\(R^2\\)**  \n",
    "\n",
    "These are discussed in more detail in Chapter 6. We can also determine which\n",
    "model is best by plotting various model outputs, such as the residuals, in order\n",
    "to search for patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem of Too Many Models\n",
    "\n",
    "Unfortunately, there are a total of \\(2^p\\) models that contain subsets of \\(p\\)\n",
    "variables. This means that even for moderate \\(p\\), trying out every possible\n",
    "subset of the predictors is infeasible.  \n",
    "\n",
    "- If \\(p = 2\\), then there are \\(2^2 = 4\\) models to consider.  \n",
    "- If \\(p = 30\\), then we must consider \\(2^{30} = 1,073,741,824\\) models!  \n",
    "\n",
    "This is not practical. Therefore, unless \\(p\\) is very small, we cannot consider\n",
    "all \\(2^p\\) models, and instead we need an automated and efficient approach to\n",
    "choose a smaller set of models to consider.\n",
    "\n",
    "---\n",
    "\n",
    "### Classical Approaches to Variable Selection\n",
    "\n",
    "There are three classical approaches for this task:\n",
    "\n",
    "#### • Forward Selection\n",
    "We begin with the **null model**—a model that contains an intercept but no\n",
    "predictors.  \n",
    "- Fit \\(p\\) simple linear regressions.  \n",
    "- Add to the null model the variable that results in the lowest RSS.  \n",
    "- Add to that model the variable that results in the lowest RSS for the new\n",
    "two-variable model.  \n",
    "- Continue this process until some stopping rule is satisfied.\n",
    "\n",
    "---\n",
    "\n",
    "#### • Backward Selection\n",
    "We start with **all variables** in the model.  \n",
    "- Remove the variable with the largest *p*-value (least statistically significant).  \n",
    "- Fit the new \\((p-1)\\)-variable model.  \n",
    "- Again remove the variable with the largest *p*-value.  \n",
    "- Continue this procedure until a stopping rule is reached (e.g., all remaining\n",
    "variables have a *p*-value below some threshold).  \n",
    "\n",
    "---\n",
    "\n",
    "#### • Mixed Selection\n",
    "A combination of forward and backward selection.  \n",
    "- Start with no variables in the model.  \n",
    "- Add the variable that provides the best fit (forward step).  \n",
    "- At each step, check if any variable in the model has a *p*-value above a\n",
    "threshold. If so, remove it (backward step).  \n",
    "- Continue until:  \n",
    "  - All variables in the model have sufficiently low *p*-values, and  \n",
    "  - All variables outside the model would have a large *p*-value if added.  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- **Backward selection** cannot be used if \\(p > n\\).  \n",
    "- **Forward selection** can always be used, but it is greedy and might include\n",
    "variables that later become redundant.  \n",
    "- **Mixed selection** can remedy this by combining the strengths of both\n",
    "approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f69232",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91278026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ISLP import load_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
