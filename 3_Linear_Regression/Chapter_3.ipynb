{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54525e21",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5e174",
   "metadata": {},
   "source": [
    " Recall the Advertising data from Chapter 2. Figure 2.1 displays sales (in thousands of units) for a particular product as a function of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest,\n",
    " on the basis of this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation? Here are a few important questions that we might seek to address:\n",
    "\n",
    "  1. *Is there a relationship between advertising budget and sales?* Our first goal should be to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, then one might argue that no money should be spent on advertising!\n",
    "\n",
    "  2. *How strong is the relationship between advertising budget and sales?* Assuming that there is a relationship between advertising and sales, we would like to know the strength of this relationship. Does knowledge of the advertising budget provide a lot of information aboutproduct sales?\n",
    "\n",
    "  3. *Which media are associated with sales?*\n",
    " Are all three media—TV, radio, and newspaper—associated with sales, or are just one or two of the media associated? To answer this question, we must find a way to separate out the individual contribution of each medium to sales when we have spent money on all three media.\n",
    "\n",
    " 4. *How large is the association between each medium and sales?*\n",
    " For every dollar spent on advertising in a particular medium, by\n",
    " what amount will sales increase? How accurately can we predict this\n",
    " amount of increase?\n",
    "\n",
    " 5. *How accurately can we predict future sales?*\n",
    " For any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?\n",
    "\n",
    " 6. *Is the relationship linear?*\n",
    " If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. If not, then it may still be possible to transform the predictor or the response so that linear regression can be used.\n",
    "\n",
    " 7. *Is there synergy among the advertising media?*\n",
    " Perhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect, while in statistics it is called an interaction effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0c97e",
   "metadata": {},
   "source": [
    "#### Simple Linear Regression\n",
    "\n",
    "*Simple linear regression* lives up to its name: it is a very straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$. Mathematically, we can write this linear relationship as\n",
    "\n",
    "\\begin{equation}\n",
    "Y \\approx \\beta_0 + \\beta_1 X.\n",
    "\\tag{3.1}\n",
    "\\end{equation}\n",
    "\n",
    "You might read “$\\approx$” as “is approximately modeled as”. We will sometimes describe (3.1) by saying that we are regressing $Y$ on $X$ (or $Y$ onto $X$).\n",
    "\n",
    " For example, X may represent TV advertising and Y may represent sales. Then we can regress sales onto TV by fitting the model\n",
    "\n",
    " \\begin{equation}\n",
    "sales \\approx \\beta_0 + \\beta_1 \\times TV.\n",
    "\\tag{3.2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In Equation 3.1, $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the *intercept* and *slope* terms in the linear model. Together, $\\beta_0$ and $\\beta_1$ are known as the model *coefficients* or *parameters*. Once we have used our training data to produce estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising by computing\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x,\n",
    "\\tag{3.2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}$ indicates a prediction of $Y$ on the basis of $X = x$. Here we use a hat symbol, $\\hat{~}$, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2ef83",
   "metadata": {},
   "source": [
    "#### Estimating the Coefficients\n",
    "\n",
    "In practice, $\\beta_0$ and $\\beta_1$ are unknown. So before we can use (3.1) to make predictions, we must use data to estimate the coefficients. Let\n",
    "\n",
    "$$(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$$\n",
    "\n",
    "represent $n$ observation pairs, each of which consists of a measurement of $X$ and a measurement of $Y$. In the *Advertising* example, this data set consists of the TV advertising budget and product sales in $n = 200$ different markets. (Recall that this data are displayed in Figure 2.1.) Our goal is to obtain coefficient estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that the linear model (3.1) fits the available data well—that is, so that $y_i \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ for $i = 1, \\ldots, n$. In other words, we want to find an intercept $\\hat{\\beta}_0$ and a slope $\\hat{\\beta}_1$ such that the resulting line is as close as possible to the $n = 200$ data points. There are a number of ways of measuring *closeness*. However, by far the most common approach involves minimizing the *least squares criterion*, and we take that approach in this chapter. \n",
    "\n",
    "Let $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ be the prediction for $Y$ based on the $i$ th value of $X$. Then $e_i = y_i - \\hat{y}_i$ represents the $i$th *residual*—this is the difference between the $i$ th observed response value and the $i$th response value that is predicted by our linear model. We define the *residual sum of squares* (RSS) as\n",
    "\n",
    "$$RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,$$\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS} = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n",
    "\\tag{3.3}\n",
    "\\end{equation}\n",
    "\n",
    "The least squares approach chooses $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ to minimize the RSS. Using some calculus, one can show that the minimizers are\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 \\;=\\; \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}\n",
    "{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 \\;=\\; \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ and $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ are the sample means. In other words, (3.4) defines the *least squares coefficient estimates* for simple linear regression.\n",
    "\n",
    "Figure 3.1 displays the simple linear regression fit to the *Advertising* data, where $\\hat{\\beta}_0 = 7.03$ and $\\hat{\\beta}_1 = 0.047$. In other words, according to this approximation, an additional \\$1000 spent on TV advertising is associated with selling approximately 47 additional units of the product. In Figure 3.2, we have computed RSS for a number of values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, using the advertising and sales as the response and $X$ as the predictor. In each plot, the red dot represents the pair of least squares estimates $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ given by (3.4). These values clearly minimize the RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16489a25",
   "metadata": {},
   "source": [
    "#### Assessing the Accuracy of the Coefficient Estimates\n",
    "\n",
    "Recall from $(2.1)$ that we assume that the *true* relationship between $X$ and $Y$ takes the form $Y = f(X) + \\epsilon$ for some unknown function $f$, where $\\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as\n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\beta_0 + \\beta_1 X + \\epsilon.\n",
    "\\tag{3.5}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\beta_0$ is the intercept term—that is, the expected value of $Y$ when $X = 0$, and $\\beta_1$ quantifies the association between $X$ and $Y$. As we saw in the previous section, we use data to produce estimates of $\\beta_0$ and $\\beta_1$ via the *least squares* approach. If the true relationship is probably not linear, there may be relationships not captured by this model, and there may be measurement error. We typically assume that the error term is independent of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026a7f8",
   "metadata": {},
   "source": [
    "The analogy between linear regression and estimation of the mean of a\n",
    "random variable is an apt one based on the concept of bias. \n",
    "\n",
    "If we use the bias sample mean $\\hat{\\mu}$ to estimate $\\mu$, this estimate is **unbiased**, in the sense that on average we expect $\\hat{\\mu}$ to equal $\\mu$.\n",
    "\n",
    "What exactly does this mean? It means that on the basis of one particular set of observations $y_1, \\dots, y_n$, $\\hat{\\mu}$ might overestimate $\\mu$, and on the basis of another set of observations, $\\hat{\\mu}$ might underestimate $\\mu$. \n",
    "\n",
    "But if we could average a huge number of estimates of $\\mu$ obtained from a huge number of sets of observations, then this average would exactly equal $\\mu$. Hence, an **unbiased estimator** does not systematically over- or under-estimate the true parameter. \n",
    "\n",
    "The property of unbiasedness holds for the least squares coefficient estimates given by Equation (3.4) as well: if we estimate $\\beta_0$ and $\\beta_1$ on the basis of a particular data set, then our estimates won’t be exactly equal to $\\beta_0$ and $\\beta_1$. \n",
    "\n",
    "But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on! In fact, we can see from the right-hand panel of Figure 3.3 that the average of many least squares lines, each estimated from a separate data set, is pretty close to the true population regression line.\n",
    "\n",
    "We continue the analogy with the estimation of the population mean $\\mu$ of a random variable $Y$. A natural question is as follows: how accurate is the sample mean $\\bar{y}$ as an estimate of $\\mu$? We have established that the average of $\\bar{y}$'s over many data sets will be very close to $\\mu$, but that a single estimate $\\bar{y}$ may be a substantial underestimate or overestimate of $\\mu$. How far off will that single estimate of $\\mu$ be? In general, we answer this question by computing the *standard error* of $\\bar{y}$, written as $\\mathrm{SE}(\\bar{y})$. We have the well-known formula\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Var}(\\bar{y}) = \\mathrm{SE}(\\bar{y})^2 = \\frac{\\sigma^2}{n},\n",
    "\\tag{3.7}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$. Roughly speaking, the standard error tells us the average amount that this estimate $\\bar{y}$ differs from the actual value of $\\mu$. Equation 3.7 also tells us how this deviation shrinks with $n$—the more observations we have, the smaller the standard error of $\\bar{y}$. In a similar vein, we can wonder how close $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are to the true values $\\beta_0$ and $\\beta_1$. To compute the standard errors associated with $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, we use the following formulas:\n",
    "\n",
    "$$\n",
    "\\mathrm{SE}(\\hat{\\beta}_0)^2 \n",
    "= \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right]\n",
    "\\quad\n",
    "\\mathrm{SE}(\\hat{\\beta}_1)^2 \n",
    "= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "\\tag{3.8}\n",
    "$$\n",
    "\n",
    "where $\\sigma^2 = \\mathrm{Var}(\\epsilon)$. For these formulas to be strictly valid, we need to assume that the errors $\\epsilon_i$ for each observation have common variance $\\sigma^2$ and are uncorrelated. This is clearly not true in Figure 3.1, but the formula still turns out to be a good approximation. Notice in the formula that $\\mathrm{SE}(\\hat{\\beta}_1)$ is smaller when the $x_i$ are more spread out; intuitively we have more *leverage* to estimate a slope when this is the case. We also see that $\\mathrm{SE}(\\hat{\\beta}_0)$ would be the same as $\\mathrm{SE}(\\bar{y})$ if we even zero (in which case $\\bar{x}$ would be equal to zero). In general, $\\sigma^2$ is not known, but can be estimated from the data. This estimate of $\\sigma$ is the *residual standard error*, and is given by the formula\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n e_i^2} = \\sqrt{\\frac{\\text{RSS}}{n-2}}.\n",
    "$$\n",
    "\n",
    "Strictly speaking, when $\\sigma^2$ is estimated from the data we should also use $\\mathrm{SE}_b(\\hat{\\beta}_1)$ to indicate that an estimate has been made. But for simplicity we will not use this extra “hat”.\n",
    "\n",
    "Standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value being estimated. The standard error comes into play in the formula for upper and lower confidence interval endpoints. A 95% confidence interval has the following property: if we compare a large number of sets, 95% of confidence intervals constructed in this way will contain the true unknown value. For example, a 95% confidence interval for the true value of $\\beta_1$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 \\pm 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1),\n",
    "$$\n",
    "\n",
    "meaning that there is approximately a 95% chance that the true value of $\\beta_1$ falls within the interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95875529",
   "metadata": {},
   "source": [
    "That is, there is approximately a 95% chance that the interval\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[ \\hat{\\beta}_1 - 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1),\\ \\hat{\\beta}_1 + 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1)\\right]\n",
    "\\tag{3.10}\n",
    "\\end{equation}\n",
    "\n",
    "will contain the true value of $\\beta_1$. Similarly, a confidence interval for $\\beta_0$ approximately takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 \\pm 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_0).\n",
    "\\tag{3.11}\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the advertising data, the 95% confidence interval for $\\beta_0$ is $[6.130, 7.935]$ and the 95% confidence interval for $\\beta_1$ is $[0.042, 0.053]$. Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between 6,130 and 7,935 units. Furthermore, for each \\$1,000 increase in television advertising, there will be an average increase in sales of between 42 and 53 units.\n",
    "\n",
    "Standard errors can also be used to perform *hypothesis tests* on the coefficients. The most common hypothesis test involves testing the *null hypothesis* of\n",
    "\n",
    "\\begin{equation}\n",
    "H_0: \\text{There is no relationship between $X$ and $Y$}\n",
    "\\tag{3.12}\n",
    "\\end{equation}\n",
    "\n",
    "versus the *alternative hypothesis*\n",
    "\n",
    "\\begin{equation}\n",
    "H_a: \\text{There is some relationship between $X$ and $Y$}\n",
    "\\tag{3.13}\n",
    "\\end{equation}\n",
    "\n",
    "Mathematically, this corresponds to testing\n",
    "\n",
    "\\[\n",
    "H_0 : \\beta_1 = 0\n",
    "\\]\n",
    "\\[\n",
    "H_a : \\beta_1 \\neq 0.\n",
    "\\]\n",
    "\n",
    "since if $\\beta_1 = 0$ then the model (3.5) reduces to $Y = \\beta_0 + \\epsilon$, and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine whether $\\hat{\\beta}_1$, our estimate for $\\beta_1$, is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero. How far is far enough? This of course depends on the accuracy of $\\hat{\\beta}_1$—that is, it depends on $\\mathrm{SE}(\\hat{\\beta}_1)$. If $\\mathrm{SE}(\\hat{\\beta}_1)$ is small, then even relatively small values of $\\hat{\\beta}_1$ may provide strong evidence that $\\beta_1 \\neq 0$, and hence that there is a relationship between $X$ and $Y$. In contrast, if $\\mathrm{SE}(\\hat{\\beta}_1)$ is large, then $\\hat{\\beta}_1$ must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a *t*-statistic, given by\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\hat{\\beta}_1}{\\mathrm{SE}(\\hat{\\beta}_1)}\n",
    "\\tag{3.14}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "That is, there is approximately a 95% chance that the interval\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[ \\hat{\\beta}_1 - 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1),\\ \\hat{\\beta}_1 + 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_1)\\right]\n",
    "\\tag{3.10}\n",
    "\\end{equation}\n",
    "\n",
    "will contain the true value of $\\beta_1$. Similarly, a confidence interval for $\\beta_0$ approximately takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_0 \\pm 2 \\cdot \\mathrm{SE}(\\hat{\\beta}_0).\n",
    "\\tag{3.11}\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the advertising data, the 95% confidence interval for $\\beta_0$ is $[6.130, 7.935]$ and the 95% confidence interval for $\\beta_1$ is $[0.042, 0.053]$. Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between 6,130 and 7,935 units. Furthermore, for each \\$1,000 increase in television advertising, there will be an average increase in sales of between 42 and 53 units.\n",
    "\n",
    "Standard errors can also be used to perform *hypothesis tests* on the coefficients. The most common hypothesis test involves testing the *null hypothesis* of\n",
    "\n",
    "\\begin{equation}\n",
    "H_0: \\text{There is no relationship between $X$ and $Y$}\n",
    "\\tag{3.12}\n",
    "\\end{equation}\n",
    "\n",
    "versus the *alternative hypothesis*\n",
    "\n",
    "\\begin{equation}\n",
    "H_a: \\text{There is some relationship between $X$ and $Y$}\n",
    "\\tag{3.13}\n",
    "\\end{equation}\n",
    "\n",
    "Mathematically, this corresponds to testing\n",
    "\n",
    "$$\n",
    "H_0 : \\beta_1 = 0\n",
    "$$\n",
    "versus\n",
    "$$\n",
    "H_a : \\beta_1 \\neq 0.\n",
    "$$\n",
    "\n",
    "since if $\\beta_1 = 0$ then the model (3.5) reduces to $Y = \\beta_0 + \\epsilon$, and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine whether $\\hat{\\beta}_1$, our estimate for $\\beta_1$, is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero. How far is far enough? This of course depends on the accuracy of $\\hat{\\beta}_1$—that is, it depends on $\\mathrm{SE}(\\hat{\\beta}_1)$. If $\\mathrm{SE}(\\hat{\\beta}_1)$ is small, then even relatively small values of $\\hat{\\beta}_1$ may provide strong evidence that $\\beta_1 \\neq 0$, and hence that there is a relationship between $X$ and $Y$. In contrast, if $\\mathrm{SE}(\\hat{\\beta}_1)$ is large, then $\\hat{\\beta}_1$ must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a *t*-statistic, given by\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\hat{\\beta}_1}{\\mathrm{SE}(\\hat{\\beta}_1)}\n",
    "\\tag{3.14}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "|               | Coefficient | Std. error | t-statistic | p-value |\n",
    "|---------------|-------------|------------|-------------|---------|\n",
    "| Intercept     | 7.0325      | 0.4578     | 15.36       | < 0.0001|\n",
    "| TV            | 0.0475      | 0.0027     | 17.67       | < 0.0001|\n",
    "\n",
    " For the Advertising data, coefficients of the least squares model \n",
    "for the regression of number of units sold on TV advertising budget. An increase \n",
    "of \\$1000 in the TV advertising budget is associated with an increase in sales by \n",
    "approximately 47.5 units. (Recall that in this dataset, sales are measured in \n",
    "thousands of units, and the TV variable is in thousands of dollars.)\n",
    "\n",
    "\n",
    "The larger the number of standard deviations that $\\hat{\\beta}_j$ is away from 0, \n",
    "i.e. there really is no relationship between $X$ and $Y$, then we expect that (3.14) \n",
    "will have a t-distribution with $n - 2$ degrees of freedom. The t-distribution \n",
    "has a bell shape and for values of t greater than approximately 2 it is quite \n",
    "similar to the standard normal distribution. Consequently, it is a simple matter \n",
    "to compute the probability of observing any number equal to $|t|$ or larger in \n",
    "absolute value, assuming $\\beta_j = 0$. We call this probability the *p-value*. \n",
    "\n",
    "Roughly speaking, we interpret the p-value as follows: a small p-value indicates \n",
    "that it is unlikely to observe such a substantial association between the predictor \n",
    "and the response due to chance, in the absence of any real association between the \n",
    "predictor and the response. Hence, if we see a small p-value, then we can infer that \n",
    "there is an association between the predictor and the response. We reject the \n",
    "*null hypothesis*—that is, we declare a relationship to exist between predictor \n",
    "and response—if the p-value is small enough. Typical p-value cutoffs for rejecting \n",
    "the null hypothesis are 5% or 1%, although this topic will be explored later in much \n",
    "greater detail. When $n = 30$, these correspond to t-statistics (3.14) \n",
    "of around 2 and 2.75, respectively. \n",
    "\n",
    "The table provides details of the least squares model for the regression of number \n",
    "of units sold on TV advertising budget for the Advertising data. Note that the \n",
    "p-values for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are very large relative to their \n",
    "standard errors, so the t-statistics are also large; the probabilities of seeing \n",
    "such values of t if $H_0$ is true are virtually zero. Hence we can conclude that \n",
    "$\\beta_0 \\neq 0$ and $\\beta_1 \\neq 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c6285",
   "metadata": {},
   "source": [
    "#### Assessing the Accuracy of the Model\n",
    "\n",
    "If we reject the null hypothesis (3.12) in favor of the alternative hypothesis (3.13), \n",
    "it is natural to want to quantify the *extent* to which the regression model fits \n",
    "the data. The quality of a linear regression fit is typically assessed using two \n",
    "related quantities: the *residual standard error* (RSE) and the $R^2$ statistic.\n",
    "\n",
    "| Quantity                  | Value |\n",
    "|---------------------------|-------|\n",
    "| Residual standard error   | 3.26  |\n",
    "| $R^2$                     | 0.612 |\n",
    "| F-statistic               | 312.1 |\n",
    "\n",
    "For the Advertising data, more information about the least \n",
    "squares model for the regression of number of units sold on TV advertising budget.\n",
    "\n",
    "\n",
    "The table displays the RSE, the $R^2$ statistic, and the F-statistic (to be described \n",
    "in Section 3.2.2) for the linear regression of number of units sold on TV advertising budget.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e917bc",
   "metadata": {},
   "source": [
    "##### Residual Standard Error\n",
    "\n",
    "Recall from the model (3.5) that associated with each observation is an error term $\\epsilon$. \n",
    "Due to the presence of these error terms, even if we knew the true regression line \n",
    "(i.e. even if $\\beta_0$ and $\\beta_1$ were known), we would not be able to perfectly \n",
    "predict $Y$ from $X$. The RSE is an estimate of the standard deviation of $\\epsilon$. \n",
    "\n",
    "Roughly speaking, it is the average amount that the response will deviate from the \n",
    "true regression line. It is computed using the formula\n",
    "\n",
    "\\[\n",
    "RSE = \\sqrt{\\frac{1}{n-2}RSS} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}.\n",
    "\\tag{3.15}\n",
    "\\]\n",
    "\n",
    "Note that RSS was defined in Section 3.1.1, and is given by the formula\n",
    "\n",
    "\\[\n",
    "RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2.\n",
    "\\tag{3.16}\n",
    "\\]\n",
    "\n",
    "\n",
    "In the case of the advertising data, we see from the linear regression output in \n",
    "Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate \n",
    "from the true regression line by approximately 3.26 units, on average. Another way \n",
    "to think about this is that even if the model were correct and the true values of \n",
    "the unknown coefficients $\\beta_0$ and $\\beta_1$ were known exactly, any prediction \n",
    "of sales on the basis of TV advertising would still be off by about 3.26 units on \n",
    "average. Of course, whether or not 3.26 units is an acceptable prediction error \n",
    "depends on the application. In this case, the average sales are around 14,000 units, \n",
    "so 3.26 units correspond to a percentage error of $3.26/14,000 \\approx 0.02\\%$. \n",
    "\n",
    "\n",
    "The RSE is considered a measure of the *lack of fit* of the model (3.5) to the data. \n",
    "If the predictions obtained using the model are very close to the true outcomes \n",
    "— that is, if $\\hat{y}_i$ is very close to $y_i$ for $i = 1, \\dots, n$ — then \n",
    "(3.15) will be small, and we can conclude that the model fits the data very well. \n",
    "On the other hand, if $\\hat{y}_i$ is far from $y_i$ for one or more observations, \n",
    "then the RSE may be quite large, indicating that the model doesn’t fit the data well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367dbec",
   "metadata": {},
   "source": [
    "##### $R^2$ Statistic\n",
    " The RSE provides an absolute measure of lack of fit of the model $(3.5)$ to the data. But since it is measured in the units of Y, it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit. \n",
    "\n",
    "It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of $Y$.\n",
    "\n",
    "To calculate $R^2$, we use the formula\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}}\n",
    "= 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n",
    "\\tag{3.17}\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\text{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "is the **total sum of squares**, and RSS is defined in Equation (3.16).  \n",
    "\n",
    "TSS measures the total variance in the response $Y$, and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. \n",
    "\n",
    "Hence, $\\text{TSS} - \\text{RSS}$ measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.  \n",
    "\n",
    "An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response is explained by the regression. A number near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, the error variance $\\sigma^2$ is high, or both.\n",
    "\n",
    "The $R^2$ statistic is a measure of the linear relationship between $X$ and $Y$. \n",
    "\n",
    "Recall that correlation, defined as\n",
    "\n",
    "$$\n",
    "\\text{Cor}(X, Y) \\;=\\; \n",
    "\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}\n",
    "{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\, \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}\n",
    "\\tag{3.18}\n",
    "$$\n",
    "\n",
    "is also a measure of the linear relationship between $X$ and $Y$.  \n",
    "\n",
    "This suggests that we might be able to use $r = \\text{Cor}(X, Y)$ instead of $R^2$ in order to assess the fit of the linear model.\n",
    "\n",
    "\n",
    "Simple regression of sales on radio\n",
    "\n",
    "| Coefficient | Std. Error | t-statistic | p-value  |\n",
    "|-------------|------------|-------------|----------|\n",
    "| Intercept   | 9.312      | 0.563       | 16.54    | <0.0001 |\n",
    "| radio       | 0.203      | 0.020       | 9.92     | <0.0001 |\n",
    "\n",
    "\n",
    "Simple regression of sales on newspaper\n",
    "\n",
    "| Coefficient | Std. Error | t-statistic | p-value  |\n",
    "|-------------|------------|-------------|----------|\n",
    "| Intercept   | 12.351     | 0.621       | 19.88    | <0.0001 |\n",
    "| newspaper   | 0.055      | 0.017       | 3.30     | 0.00115 |\n",
    "\n",
    "\n",
    "More simple linear regression models for the Advertising data.  \n",
    "\n",
    "Coefficients of the simple linear regression model for number of units sold on Top: radio advertising budget. Bottom: newspaper advertising budget.\n",
    "\n",
    "A \\$1,000 increase in spending on radio advertising is associated with an average increase in sales by around **203 units**, while the same increase in spending on newspaper advertising is associated with an average increase in sales by around **55 units**.  \n",
    "\n",
    "(Note that the sales variable is in thousands of units, and the radio and newspaper variables are in thousands of dollars.)  \n",
    "\n",
    "\n",
    "In the linear regression setting, we have\n",
    "\n",
    "$$\n",
    "R^2 = r^2\n",
    "$$\n",
    "\n",
    "In other words, the squared correlation and the $R^2$ statistic are identical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8290d3",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression\n",
    "\n",
    "Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. \n",
    "\n",
    "For example, in the **Advertising** data, we have examined the relationship between sales and TV advertising. We also have data for the amount of money spent advertising on the radio and in newspapers, and we may want to know whether either of these two media is associated with sales. How can we extend our analysis of the advertising data in order to accommodate these two additional predictors?\n",
    "\n",
    "One option is to run three separate simple linear regressions, each of which uses a different advertising medium as a predictor. For instance, we can fit a simple linear regression to predict sales on the basis of the amount spent on radio advertisements. Results are shown in **Table 3.3 (top table)**. We find that a \\$1,000 increase in spending on radio advertising is associated with an increase in sales of around **203 units**. **Table 3.3 (bottom table)** contains the least squares coefficients for a simple linear regression of sales onto newspaper advertising budget. A \\$1,000 increase in newspaper advertising budget is associated with an increase in sales of approximately **55 units**.\n",
    "\n",
    "\n",
    "However, the approach of fitting a separate simple linear regression model for each predictor is not entirely satisfactory.  \n",
    "\n",
    "- First of all, it is unclear how to make a single prediction of sales given the three advertising media budgets, since each of the budgets is associated with a separate regression equation.  \n",
    "- Second, each of the three regression equations ignores the other two media in forming estimates for the regression coefficients.  \n",
    "\n",
    "We will see shortly that if the media budgets are correlated with each other in the 200 markets in our data set, then this can lead to very misleading estimates of the association between each media budget and sales.\n",
    "\n",
    "Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model (3.5) so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model.\n",
    "\n",
    "In general, suppose that we have $p$ distinct predictors. Then the multiple linear regression model takes the form\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n",
    "\\tag{3.19}\n",
    "$$\n",
    "\n",
    "where $X_j$ represents the $j$-th predictor and $\\beta_j$ quantifies the association between that variable and the response. We interpret $\\beta_j$ as the average effect on $Y$ of a one-unit increase in $X_j$, holding all other predictors fixed.\n",
    "\n",
    "In the advertising example, (3.19) becomes:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\, \\text{TV} + \\beta_2 \\, \\text{radio} + \\beta_3 \\, \\text{newspaper} + \\varepsilon\n",
    "\\tag{3.20}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff212b",
   "metadata": {},
   "source": [
    "##### Estimating the Regression Coefficients\n",
    "\n",
    "As was the case in the simple linear regression setting, the regression coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ in (3.19) are unknown, and must be estimated. \n",
    "\n",
    "Given estimates $\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$, we can make predictions using the formula\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_p x_p\n",
    "\\tag{3.21}\n",
    "$$\n",
    "\n",
    "The parameters are estimated using the same least squares approach that we saw in the context of simple linear regression. We choose $\\beta_0, \\beta_1, \\dots, \\beta_p$ to minimize the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\text{RSS} = \\sum_{i=1}^n \\Big( y_i - \\hat{y}_i \\Big)^2 \n",
    "= \\sum_{i=1}^n \\Big( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\cdots - \\hat{\\beta}_p x_{ip} \\Big)^2\n",
    "\\tag{3.22}\n",
    "$$\n",
    "\n",
    "The values $\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$ that minimize (3.22) are the **multiple least squares regression coefficient estimates**. Unlike the simple linear regression coefficient estimates given in (3.4), the multiple regression coefficient estimates have somewhat complicated forms that are most easily represented using matrix algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbcaff",
   "metadata": {},
   "source": [
    "The Multiple Regression Coefficient Estimates table displays the multiple regression coefficient estimates when TV,\n",
    "radio, and newspaper advertising budgets are used to predict product sales\n",
    "using the Advertising data. We interpret these results as follows: for a given\n",
    "amount of TV and newspaper advertising, spending an additional $1,000 on\n",
    "radio advertising is associated with approximately 189 units of additional\n",
    "sales. Comparing these coefficient estimates to those displayed in Tables 3.1\n",
    "and 3.3, we notice that the multiple regression coefficient estimates for\n",
    "TV and radio are pretty similar to the simple linear regression coefficient\n",
    "estimates. However, while the newspaper regression coefficient estimate in\n",
    "Table 3.3 was significantly non-zero, the coefficient estimate for newspaper\n",
    "in the multiple regression model is close to zero, and the corresponding \n",
    "*p*-value is no longer significant, with a value around **0.86**.  \n",
    "\n",
    "This illustrates that the simple and multiple regression coefficients can be\n",
    "quite different. This difference stems from the fact that in the simple regression\n",
    "case, the slope term represents the average increase in product sales associated\n",
    "with a $1,000 increase in newspaper advertising, **ignoring other predictors such as\n",
    "TV and radio**.  \n",
    "\n",
    "By contrast, in the multiple regression setting, the coefficient for newspaper\n",
    "represents the average increase in product sales associated with increasing \n",
    "newspaper spending by $1,000 **while holding TV and radio fixed**.\n",
    "\n",
    "\n",
    "Multiple Regression Coefficient Estimates\n",
    "\n",
    "| Coefficient | Std. error | t-statistic | p-value   |\n",
    "|-------------|------------|-------------|-----------|\n",
    "| Intercept   | 2.939      | 0.3119      | 9.42      | <0.0001 |\n",
    "| TV          | 0.046      | 0.0014      | 32.81     | <0.0001 |\n",
    "| radio       | 0.189      | 0.0086      | 21.89     | <0.0001 |\n",
    "| newspaper   | 0.001      | 0.0059      | 0.18      | 0.8599  |\n",
    "\n",
    "*For the Advertising data, least squares coefficient estimates of the multiple\n",
    "linear regression of number of units sold on TV, radio, and newspaper\n",
    "advertising budgets.*\n",
    "\n",
    "\n",
    "Does it make sense for the multiple regression to suggest no relationship\n",
    "between sales and newspaper while the simple linear regression implies the\n",
    "opposite?  \n",
    "\n",
    "In fact it does. Consider the **correlation matrix** for the three predictor\n",
    "variables and response variable, displayed in Table 3.5. Notice that the\n",
    "correlation between radio and newspaper is **0.35**. This indicates\n",
    "that markets with high newspaper advertising tend to also have high radio\n",
    "advertising.\n",
    "\n",
    "\n",
    "Correlation Matrix (TV, radio, newspaper, sales)\n",
    "\n",
    "|             | TV     | radio  | newspaper | sales  |\n",
    "|-------------|--------|--------|-----------|--------|\n",
    "| TV          | 1.0000 | 0.0548 | 0.0567    | 0.7822 |\n",
    "| radio       | 0.0548 | 1.0000 | 0.3541    | 0.5762 |\n",
    "| newspaper   | 0.0567 | 0.3541 | 1.0000    | 0.2283 |\n",
    "| sales       | 0.7822 | 0.5762 | 0.2283    | 1.0000 |\n",
    "\n",
    "\n",
    "Now suppose that the multiple regression is correct and\n",
    "**newspaper advertising is not associated with sales, but radio advertising\n",
    "is associated with sales.** Then in markets where we spend more on radio\n",
    "our sales will tend to be higher, and as our correlation matrix shows, we\n",
    "also tend to spend more on newspaper advertising in those same markets.  \n",
    "\n",
    "Hence, in a simple linear regression which only examines sales versus\n",
    "newspaper, we will observe that higher values of newspaper tend to be\n",
    "associated with higher values of sales, even though newspaper advertising is\n",
    "not directly associated with sales.  \n",
    "\n",
    "So newspaper advertising is a **surrogate** for radio advertising; newspaper\n",
    "gets “credit” for the association between radio on sales.\n",
    "\n",
    "\n",
    "This slightly counterintuitive result is very common in many real-life\n",
    "situations. Consider an absurd example to illustrate the point:  \n",
    "\n",
    "- Running a regression of **shark attacks versus ice cream sales** for data collected at\n",
    "a given beach community over a period of time would show a positive\n",
    "relationship, similar to that seen between sales and newspaper.  \n",
    "- Of course no one has (yet) suggested that ice creams should be banned at beaches\n",
    "to reduce shark attacks.  \n",
    "\n",
    "In reality:  \n",
    "- Higher temperatures cause more people to visit the beach, which in turn \n",
    "results in more ice cream sales and more shark attacks.  \n",
    "- A **multiple regression** of shark attacks onto ice cream sales **and temperature**\n",
    "reveals that, as intuition implies, ice cream sales is no longer a significant\n",
    "predictor after adjusting for temperature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fc5d5",
   "metadata": {},
   "source": [
    "##### Some Important Questions\n",
    "When we perform multiple linear regression, we usually are interested in\n",
    "answering a few important questions.\n",
    " 1. Is at least one of the predictors X1,X2,...,Xp useful in predicting\n",
    " the response?\n",
    " 2. Do all the predictors help to explain Y , or is only a subset of the\n",
    " predictors useful?\n",
    " 3. How well does the model fit the data?\n",
    " 4. Given a set of predictor values, what response value should we predict,\n",
    " and how accurate is our prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb882fd",
   "metadata": {},
   "source": [
    "##### One: Is There a Relationship Between the Response and Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e8bda",
   "metadata": {},
   "source": [
    "Recall that in the simple linear regression setting, in order to determine  \n",
    "whether there is a relationship between the response and the predictor, we  \n",
    "can simply check whether $\\beta_1 = 0$.  \n",
    "\n",
    "In the multiple regression setting with $p$ predictors, we need to ask whether  \n",
    "all of the regression coefficients are zero, i.e.:  \n",
    "\n",
    "$$ \n",
    "H_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\n",
    "$$  \n",
    "\n",
    "versus the alternative  \n",
    "\n",
    "$$ \n",
    "H_a : \\text{at least one } \\beta_j \\neq 0\n",
    "$$  \n",
    "\n",
    "This hypothesis test is performed by computing the **F-statistic**:  \n",
    "\n",
    "$$\n",
    "F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} \\tag{3.23}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "\n",
    "- $TSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$ (Total Sum of Squares)  \n",
    "- $RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ (Residual Sum of Squares)  \n",
    "\n",
    "If the linear model assumptions are correct, then:  \n",
    "\n",
    "$$\n",
    "E\\left\\{\\frac{RSS}{n - p - 1}\\right\\} = \\sigma^2\n",
    "$$  \n",
    "\n",
    "and, provided $H_0$ is true,  \n",
    "\n",
    "$$\n",
    "E\\left\\{\\frac{TSS - RSS}{p}\\right\\} = \\sigma^2\n",
    "$$  \n",
    "\n",
    "- When there is **no relationship** between the response and predictors,  \n",
    "  the F-statistic should be close to 1.  \n",
    "\n",
    "- When $H_a$ is true, then  \n",
    "\n",
    "$$\n",
    "E\\left\\{\\frac{TSS - RSS}{p}\\right\\} > \\sigma^2\n",
    "$$  \n",
    "\n",
    "so we expect $F > 1$.  \n",
    "\n",
    "For example, the F-statistic for the multiple linear regression model obtained  \n",
    "by regressing **sales** onto **radio, TV, and newspaper** is:  \n",
    "\n",
    "$$\n",
    "F = 570\n",
    "$$  \n",
    "\n",
    "Since this is far larger than 1, it provides compelling evidence against the  \n",
    "null hypothesis $H_0$.  \n",
    "The large F-statistic suggests that at least one of the advertising media  \n",
    "must be related to sales.  However, if the F-statistic had been closer to 1, we would not reject $H_0$. How large does the F-statistic need to be before we can reject $H_0$?\n",
    "\n",
    "And how large must the F-statistic be before we can conclude that there is a relationship?  \n",
    "It turns out that the answer depends on the values of $n$ and $p$.  \n",
    "\n",
    "- When $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$.  \n",
    "- In contrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small.  \n",
    "\n",
    "| **Quantity**               | **Value** |\n",
    "|-----------------------------|-----------|\n",
    "| Residual standard error     | 1.69      |\n",
    "| \\(R^2\\)                     | 0.897     |\n",
    "| F-statistic                 | 570       |\n",
    "\n",
    "More information about the least squares model for the regression  \n",
    "of number of units sold on TV, newspaper, and radio advertising budgets in the  \n",
    "Advertising data. Other information about this model was displayed in the Multiple Regression Coefficient Estimates table.\n",
    "\n",
    "When $H_0$ is true and the errors $\\epsilon_i$ have a normal distribution, the F-statistic follows an **F-distribution**.  \n",
    "For any given value of $n$ and $p$, statistical software can be used to compute the **p-value** associated with the F-statistic.  \n",
    "Based on this p-value, we can determine whether or not to reject $H_0$.  \n",
    "\n",
    "For the advertising data, the p-value associated with the F-statistic in Table 3.6 is essentially zero,  \n",
    "so we have extremely strong evidence that at least one of the media is associated with increased sales.  \n",
    "\n",
    "\n",
    "In (3.23) we are testing $H_0$ that **all the coefficients are zero**.  \n",
    "Sometimes, we want to test that a particular subset of $q$ of the coefficients are zero.  \n",
    "This corresponds to the null hypothesis:  \n",
    "\n",
    "$$\n",
    "H_0 : \\beta_{p-q+1} = \\beta_{p-q+2} = \\cdots = \\beta_p = 0\n",
    "$$  \n",
    "\n",
    "where (for convenience) the variables chosen for omission are listed at the end.  \n",
    "\n",
    "In this case we fit a second model that uses all the variables except those last $q$.  \n",
    "Suppose the residual sum of squares for that reduced model is $RSS_0$.  \n",
    "Then the appropriate F-statistic is:  \n",
    "\n",
    "$$\n",
    "F = \\frac{(RSS_0 - RSS)/q}{RSS/(n - p - 1)} \\tag{3.24}\n",
    "$$  \n",
    "\n",
    "\n",
    "Notice that in Table 3.4, for each individual predictor a **t-statistic** and a **p-value** were reported.  \n",
    "These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors.  \n",
    "\n",
    "It turns out that each of these is exactly equivalent to the F-test that omits that single variable from the model (i.e. $q=1$ in (3.24)).  \n",
    "So it reports the **partial effect** of adding that variable to the model.  \n",
    "\n",
    "For example, as discussed earlier:  \n",
    "- The p-values indicate that **TV** and **radio** are related to sales.  \n",
    "- There is no evidence that **newspaper** is associated with sales, when TV and radio are held fixed.   \n",
    "\n",
    "Given the individual p-values, it may seem that if any one of them is small, then at least one predictor is related to the response.  \n",
    "However, this reasoning is flawed, especially when the number of predictors $p$ is large.  \n",
    "\n",
    "- Suppose $p=100$ and $H_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$ is true.  \n",
    "- In this case, no variable is truly associated with the response.  \n",
    "- However, by chance, about 5% of the p-values will fall below 0.05.  \n",
    "- So we expect around five small p-values even in the absence of any real associations.  \n",
    "- In fact, it is very likely that we will observe at least one p-value below 0.05 by chance!  \n",
    "\n",
    "Hence, if we rely only on individual t-tests, we are very likely to **incorrectly conclude** that there is a relationship.  \n",
    "\n",
    "The **F-statistic** avoids this problem, because it adjusts for the number of predictors.  \n",
    "If $H_0$ is true, there is only a 5% chance that the F-statistic will produce a p-value below 0.05,  \n",
    "regardless of the number of predictors or the number of observations.  \n",
    "\n",
    "Limitations of the F-statistic  \n",
    "\n",
    "The F-statistic is useful when $p$ is relatively small compared to $n$.  \n",
    "However:  \n",
    "\n",
    "- If $p > n$, there are more coefficients $\\beta_j$ to estimate than there are observations.  \n",
    "- In this case, we cannot fit the multiple linear regression model using least squares.  \n",
    "- As a result, the F-statistic (and many related concepts) cannot be used.  \n",
    "\n",
    "When $p$ is large, alternative approaches—such as **forward selection** and other model selection techniques—are required.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb55be",
   "metadata": {},
   "source": [
    "##### Two: Deciding on Important Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb3fba",
   "metadata": {},
   "source": [
    "As earlier discussed, the first step in a multiple regression\n",
    "analysis is to compute the F-statistic and to examine the associated *p*-value.  \n",
    "If we conclude on the basis of that *p*-value that at least one of the\n",
    "predictors is related to the response, then it is natural to wonder **which are\n",
    "the guilty ones!** We could look at the individual *p*-values as in the Multiple Regression Coefficient Estimates table,\n",
    "but as discussed, if \\(p\\) is large we\n",
    "are likely to make some false discoveries.\n",
    "\n",
    "It is possible that all of the predictors are associated with the response,\n",
    "but it is more often the case that the response is only associated with\n",
    "a subset of the predictors. The task of determining which predictors are\n",
    "associated with the response, in order to fit a single model involving only\n",
    "those predictors, is referred to as **variable selection**. The variable selection\n",
    "problem is studied extensively later, and so here we will provide\n",
    "only a brief outline of some classical approaches.\n",
    "\n",
    "Considering Subsets of Models\n",
    "\n",
    "Ideally, we would like to perform variable selection by trying out a lot of\n",
    "different models, each containing a different subset of the predictors.  \n",
    "\n",
    "For instance, if $p = 2$, then we can consider four models:  \n",
    "1. A model containing no variables.  \n",
    "2. A model containing $X_1$ only.  \n",
    "3. A model containing $X_2$ only.  \n",
    "4. A model containing both $X_1$ and $X_2$.  \n",
    "\n",
    "We can then select the best model out of all of the models that we have\n",
    "considered.  \n",
    "\n",
    "How do we determine which model is best? Various statistics can be used to\n",
    "judge the quality of a model. These include:  \n",
    "- **Mallow’s $C_p$**  \n",
    "- **Akaike Information Criterion (AIC)**  \n",
    "- **Bayesian Information Criterion (BIC)**  \n",
    "- **Adjusted $R^2$**  \n",
    "\n",
    "These are discussed in more detail in Chapter 6. We can also determine which\n",
    "model is best by plotting various model outputs, such as the residuals, in order\n",
    "to search for patterns.\n",
    "\n",
    "The Problem of Too Many Models\n",
    "\n",
    "Unfortunately, there are a total of $2^p$ models that contain subsets of \\(p\\)\n",
    "variables. This means that even for moderate \\(p\\), trying out every possible\n",
    "subset of the predictors is infeasible.  \n",
    "\n",
    "- If \\(p = 2\\), then there are $2^2 = 4$ models to consider.  \n",
    "- If \\(p = 30\\), then we must consider $2^{30} = 1,073,741,824$ models!  \n",
    "\n",
    "This is not practical. Therefore, unless \\(p\\) is very small, we cannot consider\n",
    "all $2^p$ models, and instead we need an automated and efficient approach to\n",
    "choose a smaller set of models to consider.\n",
    "\n",
    "Classical Approaches to Variable Selection\n",
    "\n",
    "There are three classical approaches for this task:\n",
    "\n",
    "• Forward Selection\n",
    "We begin with the **null model**—a model that contains an intercept but no\n",
    "predictors.  \n",
    "- Fit \\(p\\) simple linear regressions.  \n",
    "- Add to the null model the variable that results in the lowest RSS.  \n",
    "- Add to that model the variable that results in the lowest RSS for the new\n",
    "two-variable model.  \n",
    "- Continue this process until some stopping rule is satisfied.\n",
    "\n",
    "\n",
    "• Backward Selection\n",
    "We start with **all variables** in the model.  \n",
    "- Remove the variable with the largest *p*-value (least statistically significant).  \n",
    "- Fit the new \\((p-1)\\)-variable model.  \n",
    "- Again remove the variable with the largest *p*-value.  \n",
    "- Continue this procedure until a stopping rule is reached (e.g., all remaining\n",
    "variables have a *p*-value below some threshold).  \n",
    "\n",
    "\n",
    "• Mixed Selection\n",
    "A combination of forward and backward selection.  \n",
    "- Start with no variables in the model.  \n",
    "- Add the variable that provides the best fit (forward step).  \n",
    "- At each step, check if any variable in the model has a *p*-value above a\n",
    "threshold. If so, remove it (backward step).  \n",
    "- Continue until:  \n",
    "  - All variables in the model have sufficiently low *p*-values, and  \n",
    "  - All variables outside the model would have a large *p*-value if added.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10ccc0",
   "metadata": {},
   "source": [
    "##### Three: Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f69232",
   "metadata": {},
   "source": [
    "Two of the most common numerical measures of model fit are the **RSE** and\n",
    "$R^2$, the fraction of variance explained. These quantities are computed and\n",
    "interpreted in the same fashion as for simple linear regression.\n",
    "\n",
    "Recall that in simple regression, $R^2$ is the square of the correlation of the\n",
    "response and the variable. In multiple linear regression, it turns out that it\n",
    "equals $\\text{Cor}(Y, \\hat{Y})^2$, the square of the correlation between the response and\n",
    "the fitted linear model; in fact one property of the fitted linear model is\n",
    "that it maximizes this correlation among all possible linear models.\n",
    "\n",
    "An $R^2$ value close to 1 indicates that the model explains a large portion\n",
    "of the variance in the response variable. As an example, we saw in\n",
    "Table 3.6 that for the Advertising data, the model that uses all three\n",
    "advertising media to predict sales has an $R^2$ of 0.8972. On the other hand,\n",
    "the model that uses only TV and radio to predict sales has an $R^2$ value\n",
    "of 0.89719. In other words, there is a small increase in $R^2$ if we include\n",
    "newspaper advertising in the model that already contains TV and radio\n",
    "advertising, even though we saw earlier that the *p*-value for newspaper\n",
    "advertising in Table 3.4 is not significant. \n",
    "\n",
    "It turns out that $R^2$ will always increase when more variables are added to\n",
    "the model, even if those variables are only weakly associated with the\n",
    "response. This is due to the fact that adding another variable always results\n",
    "in a decrease in the residual sum of squares (RSS) on the training data\n",
    "(though not necessarily the testing data). Thus, the $R^2$ statistic, which is\n",
    "also computed on the training data, must increase. \n",
    "\n",
    "The fact that adding newspaper advertising to the model containing only TV\n",
    "and radio advertising leads to just a tiny increase in $R^2$ provides additional\n",
    "evidence that newspaper can be dropped from the model. Essentially,\n",
    "newspaper provides no real improvement in the model fit to the training\n",
    "samples, and its inclusion will likely lead to poor results on independent test\n",
    "samples due to overfitting.\n",
    "\n",
    "By contrast, the model containing only TV as a predictor had an $R^2$ of\n",
    "0.61 (Table 3.2). Adding radio to the model leads to a substantial improvement\n",
    "in $R^2$. This implies that a model that uses TV and radio expenditures to\n",
    "predict sales is substantially better than one that uses only TV advertising. \n",
    "We could further quantify this improvement by looking at the *p*-value\n",
    "for the radio coefficient in a model that contains only TV and radio as\n",
    "predictors.\n",
    "\n",
    "The model that contains only TV and radio as predictors has an **RSE**\n",
    "of 1.681, and the model that also contains newspaper as a predictor has\n",
    "an **RSE** of 1.686 (Table 3.6). In contrast, the model that contains only TV\n",
    "has an **RSE** of 3.26 (Table 3.2). This corroborates our previous conclusion\n",
    "that a model that uses TV and radio expenditures to predict sales is much\n",
    "more accurate (on the training data) than one that only uses TV spending.\n",
    "\n",
    "Furthermore, given that TV and radio expenditures are used as predictors,\n",
    "there is no point in also using newspaper spending as a predictor in the\n",
    "model. The observant reader may wonder how RSE can increase when\n",
    "newspaper is added to the model given that RSS must decrease. \n",
    "\n",
    "In general, RSE is defined as\n",
    "\n",
    "$$\n",
    "\\text{RSE} = \\sqrt{\\frac{1}{n - p - 1} \\cdot RSS} \\tag{3.25}\n",
    "$$\n",
    "\n",
    "Which simplifies to **(3.15)** for a simple linear regression. Thus, models with\n",
    "more variables can have higher RSE if the decrease in RSS is small relative\n",
    "to the increase in $p$.\n",
    "\n",
    "In addition to looking at the RSE and $R^2$ statistics just discussed, it\n",
    "can be useful to plot the data. Graphical summaries can reveal problems\n",
    "with a model that are not visible from numerical statistics. For example,\n",
    "**Figure 3.5** displays a three-dimensional plot of TV and radio versus sales.\n",
    "We see that some observations lie above and some observations lie below\n",
    "the least squares regression plane. In particular, the linear model seems to\n",
    "overestimate sales for instances in which most of the advertising money\n",
    "was spent exclusively on either TV or radio. It underestimates sales for\n",
    "instances where the budget was split between the two media. \n",
    "\n",
    "This pronounced non-linear pattern suggests a **synergy** or **interaction effect**\n",
    "between the advertising media, whereby combining the media together results in a\n",
    "bigger boost to sales than using any single medium. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0cf73",
   "metadata": {},
   "source": [
    "##### Four: Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6568fe",
   "metadata": {},
   "source": [
    "Once we have fit the multiple regression model, it is straightforward to\n",
    "apply **(3.21)** in order to predict the response $Y$ on the basis of a set of\n",
    "values for the predictors $X_1, X_2, \\ldots, X_p$. However, there are three sorts of\n",
    "uncertainty associated with this prediction:\n",
    "\n",
    "1. **Uncertainty in coefficient estimates**  \n",
    "   The coefficient estimates $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are estimates for  \n",
    "   $\\beta_0, \\beta_1, \\ldots, \\beta_p$.  \n",
    "\n",
    "   That is, the least squares plane  \n",
    "\n",
    "   $$\n",
    "   \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p\n",
    "   $$\n",
    "\n",
    "   is only an estimate for the true population regression plane  \n",
    "\n",
    "   $$\n",
    "   f(X) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n",
    "   $$\n",
    "\n",
    "   The inaccuracy in the coefficient estimates is related to the **reducible\n",
    "   error** from Chapter 2. We can compute a **confidence interval** in order\n",
    "   to determine how close $\\hat{Y}$ will be to $f(X)$.\n",
    "\n",
    "2. **Model bias**  \n",
    "   In practice, assuming a linear model for $f(X)$ is almost\n",
    "   always an approximation of reality, so there is an additional source of\n",
    "   potentially reducible error which we call **model bias**.  \n",
    "   When we use a linear model, we are estimating the best linear approximation\n",
    "   to the true surface. However, here we will ignore this discrepancy,\n",
    "   and operate as if the linear model were correct.\n",
    "\n",
    "3. **Irreducible error**  \n",
    "   Even if we knew $f(X)$—that is, even if we knew the true values\n",
    "   for $\\beta_0, \\beta_1, \\ldots, \\beta_p$—the response value cannot be predicted perfectly\n",
    "   because of the random error in the model **(3.20)**. In Chapter 2, we\n",
    "   referred to this as the **irreducible error**.  \n",
    "   How much will $Y$ vary from $\\hat{Y}$?  \n",
    "   We use **prediction intervals** to answer this question. Prediction\n",
    "   intervals are always wider than confidence intervals, because they\n",
    "   incorporate both:\n",
    "   - the error in the estimate for $f(X)$ (the reducible error), and  \n",
    "   - the uncertainty as to how much an individual point will\n",
    "     differ from the population regression plane (the irreducible error).  \n",
    "\n",
    "\n",
    "We use a **confidence interval** to quantify the uncertainty surrounding\n",
    "the average sales over a large number of cities.  \n",
    "\n",
    "For example, given that \\$100,000 is spent on TV advertising and \\$20,000 is spent on radio advertising\n",
    "in each city, the 95% confidence interval is:  \n",
    "\n",
    "$$\n",
    "[10,985, \\; 11,528]\n",
    "$$\n",
    "\n",
    "We interpret this to mean that **95% of intervals of this form will contain the true value of $f(X)$.**\n",
    "\n",
    "On the other hand, a **prediction interval** can be used to quantify the\n",
    "uncertainty surrounding sales for a **particular city**.  \n",
    "\n",
    "Given that \\$100,000 is spent on TV advertising and \\$20,000 is spent on radio advertising in that city,\n",
    "the 95% prediction interval is:  \n",
    "\n",
    "$$\n",
    "[7,930, \\; 14,580]\n",
    "$$\n",
    "\n",
    "We interpret this to mean that **95% of intervals of this form will contain the true value of $Y$ for this\n",
    "city**.  \n",
    "\n",
    "Note that both intervals are centered at **11,256**, but the prediction\n",
    "interval is substantially **wider** than the confidence interval, reflecting the\n",
    "increased uncertainty about sales for a given city in comparison to the\n",
    "average sales over many locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed748e7",
   "metadata": {},
   "source": [
    "#### Other Considerations in the Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a554d25",
   "metadata": {},
   "source": [
    "##### Qualitative Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b75e5",
   "metadata": {},
   "source": [
    "So far, we have assumed that all variables in our linear regression model are **quantitative**.  \n",
    "But in practice, this is not necessarily the case; often some predictors are **qualitative**.\n",
    "\n",
    "For example, the **Credit** data set (Figure 3.6) records variables for a number of credit card holders.  \n",
    "The **response** is `balance` (average credit card debt for each individual) and there are several quantitative predictors:\n",
    "\n",
    "- `age` (years)  \n",
    "- `cards` (number of credit cards)  \n",
    "- `education` (years of education)  \n",
    "- `income` (in thousands of dollars)  \n",
    "- `limit` (credit limit)  \n",
    "- `rating` (credit rating)  \n",
    "\n",
    "In addition to these quantitative variables, we also have **four qualitative variables**:  \n",
    "\n",
    "- `own` (house ownership)  \n",
    "- `student` (student status)  \n",
    "- `status` (marital status)  \n",
    "- `region` (East, West, or South)  \n",
    "\n",
    "\n",
    "Predictors with Only Two Levels\n",
    "\n",
    "Suppose we wish to investigate differences in credit card balance between those who own a house and those who don’t, ignoring the other variables.  \n",
    "If a qualitative predictor (also called a **factor**) has only **two levels**, then incorporating it into a regression model is straightforward.  \n",
    "\n",
    "We create an **indicator (dummy) variable**:\n",
    "\n",
    "$$\n",
    "x_i =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $i$th person owns a house} \\\\\n",
    "0 & \\text{if $i$th person does not own a house}\n",
    "\\end{cases}\n",
    "\\tag{3.26}\n",
    "$$\n",
    "\n",
    "This results in the regression model:\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if $i$th person owns a house} \\\\\n",
    "\\beta_0 + \\epsilon_i & \\text{if $i$th person does not own a house}\n",
    "\\end{cases}\n",
    "\\tag{3.27}\n",
    "$$\n",
    "\n",
    "- $\\beta_0$: average credit card balance among **non-owners**  \n",
    "- $\\beta_0 + \\beta_1$: average balance among **owners**  \n",
    "- $\\beta_1$: the difference in average balance between owners and non-owners  \n",
    "\n",
    "\n",
    "\n",
    "Table showing the coefficient estimates for this model.\n",
    "\n",
    "| Coefficient  | Estimate | Std. Error | t-statistic | p-value  |\n",
    "|--------------|----------|------------|-------------|----------|\n",
    "| Intercept    | 509.80   | 33.13      | 15.389      | < 0.0001 |\n",
    "| own\\[Yes\\]   | 19.73    | 46.05      | 0.429       | 0.6690   |\n",
    "\n",
    "Least squares coefficient estimates for the regression of balance on `own` in the Credit data set.  \n",
    "\n",
    "- Average balance for non-owners: **\\$509.80**  \n",
    "- Average balance for owners: **\\$509.80 + \\$19.73 = \\$529.53**  \n",
    "- The p-value is very high, indicating no significant evidence of a difference in average balance.  \n",
    "\n",
    "Notes on Coding\n",
    "\n",
    "The choice of coding (owners = 1, non-owners = 0) is arbitrary.  \n",
    "Reversing the coding changes the coefficients but **not the predictions**.  \n",
    "\n",
    "Alternative coding scheme:\n",
    "\n",
    "$$\n",
    "x_i =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $i$th person owns a house} \\\\\n",
    "-1 & \\text{if $i$th person does not own a house}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This leads to the model:\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if owner} \\\\\n",
    "\\beta_0 - \\beta_1 + \\epsilon_i & \\text{if non-owner}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this case:  \n",
    "- $\\beta_0$: overall average balance (ignoring ownership effect)  \n",
    "- $\\beta_1$: half the difference between owners and non-owners  \n",
    "\n",
    "\n",
    "\n",
    "Qualitative Predictors with More than Two Levels\n",
    "\n",
    "When a predictor has more than two levels, a single dummy variable is not enough.  \n",
    "We need to create **multiple dummy variables**.\n",
    "\n",
    "Example: `region` (East, West, South).  \n",
    "\n",
    "We define:\n",
    "\n",
    "$$\n",
    "x_{i1} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $i$th person is from the South} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\tag{3.28}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{i2} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $i$th person is from the West} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\tag{3.29}\n",
    "$$\n",
    "\n",
    "The model becomes:\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if South} \\\\\n",
    "\\beta_0 + \\beta_2 + \\epsilon_i & \\text{if West} \\\\\n",
    "\\beta_0 + \\epsilon_i & \\text{if East}\n",
    "\\end{cases}\n",
    "\\tag{3.30}\n",
    "$$\n",
    "\n",
    "- $\\beta_0$: average balance for **East** (the baseline group)  \n",
    "- $\\beta_1$: difference in average balance between **South** and **East**  \n",
    "- $\\beta_2$: difference between **West** and **East**  \n",
    "\n",
    "There will always be **one fewer dummy variable** than the number of levels.  \n",
    "The group with no dummy variable (here, East) is called the **baseline**.\n",
    "\n",
    "Regression of Balance on Region Table\n",
    "\n",
    "| Coefficient     | Estimate | Std. Error | t-statistic | p-value  |\n",
    "|-----------------|----------|------------|-------------|----------|\n",
    "| Intercept       | 531.00   | 46.32      | 11.464      | < 0.0001 |\n",
    "| region[South]   | -18.69   | 56.68      | -0.221      | 0.8260   |\n",
    "| region[West]    | -12.50   | 65.02      | -0.287      | 0.7740   |\n",
    "\n",
    "Least squares coefficient estimates for the regression of balance onto `region` in the Credit data set, using dummy variables (3.28) and (3.29).\n",
    "\n",
    "From the table:  \n",
    "- Estimated balance for **East** (baseline) = \\$531.00  \n",
    "- Those in the **South**: \\$18.69 less than East  \n",
    "- Those in the **West**: \\$12.50 less than East  \n",
    "\n",
    "However, the large p-values suggest **no statistical evidence** of real differences.  \n",
    "The choice of baseline (East) is arbitrary, and predictions remain the same regardless of coding.  \n",
    "\n",
    "To test all regional effects simultaneously, we can perform an **F-test**:\n",
    "\n",
    "$$\n",
    "H_0 : \\beta_1 = \\beta_2 = 0\n",
    "$$\n",
    "\n",
    "This F-test has a p-value of 0.96, meaning we cannot reject the null hypothesis that region has no relationship with balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c5ef5",
   "metadata": {},
   "source": [
    "##### Extensions of the Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6d575",
   "metadata": {},
   "source": [
    "The standard linear regression model (3.19) assumes that relationships are **additive** and **linear**:  \n",
    "\n",
    "- **Additivity**: the effect of a predictor $X_j$ on $Y$ does not depend on the other predictors.  \n",
    "- **Linearity**: a one-unit increase in $X_j$ always changes $Y$ by a constant $\\beta_j$, regardless of the value of $X_j$.  \n",
    "\n",
    "These assumptions can be restrictive. A common extension is to include **interaction terms**.\n",
    "\n",
    "\n",
    "Example: Interaction Effect\n",
    "\n",
    "Standard two-variable linear regression model:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "A one-unit increase in $X_1$ always increases $Y$ by $\\beta_1$, independent of $X_2$.  \n",
    "To allow $X_1$’s effect to depend on $X_2$, we add an **interaction term**:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon\n",
    "\\tag{3.31}\n",
    "$$\n",
    "\n",
    "Rewriting:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + (\\beta_1 + \\beta_3 X_2) X_1 + \\beta_2 X_2 + \\epsilon\n",
    "= \\beta_0 + \\tilde{\\beta}_1 X_1 + \\beta_2 X_2 + \\epsilon\n",
    "\\tag{3.32}\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\beta}_1 = \\beta_1 + \\beta_3 X_2$.  \n",
    "Thus, the effect of $X_1$ on $Y$ depends on $X_2$, and vice versa.  \n",
    "\n",
    "\n",
    "Advertising Data with Interaction Term Table\n",
    "\n",
    "| Coefficient   | Estimate | Std. Error | t-statistic | p-value  |\n",
    "|---------------|----------|------------|-------------|----------|\n",
    "| Intercept     | 6.7502   | 0.248      | 27.23       | < 0.0001 |\n",
    "| TV            | 0.0191   | 0.002      | 12.70       | < 0.0001 |\n",
    "| radio         | 0.0289   | 0.009      | 3.24        | 0.0014   |\n",
    "| TV × radio    | 0.0011   | 0.000      | 20.73       | < 0.0001 |\n",
    "\n",
    "Coefficient estimates for regression of sales onto TV, radio, and their interaction.\n",
    "\n",
    "\n",
    "\n",
    "Suppose we fit a model for factory productivity:\n",
    "\n",
    "$$\n",
    "\\text{units} = 1.2 + 3.4 \\cdot \\text{lines} + 0.22 \\cdot \\text{workers} + 1.4 \\cdot (\\text{lines} \\times \\text{workers})\n",
    "$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\text{units} = 1.2 + (3.4 + 1.4 \\cdot \\text{workers}) \\cdot \\text{lines} + 0.22 \\cdot \\text{workers}\n",
    "$$\n",
    "\n",
    "So, the effect of an additional line increases with the number of workers.\n",
    "\n",
    "\n",
    "Advertising Example with Interaction\n",
    "\n",
    "For the Advertising data, the model is:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\cdot TV + \\beta_2 \\cdot radio + \\beta_3 \\cdot (TV \\times radio) + \\epsilon\n",
    "$$\n",
    "\n",
    "which can be written as:\n",
    "\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + (\\beta_1 + \\beta_3 \\cdot radio) \\cdot TV + \\beta_2 \\cdot radio + \\epsilon\n",
    "\\tag{3.33}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038752a",
   "metadata": {},
   "source": [
    "Associated with a one-unit increase in radio advertising (or vice-versa).  \n",
    "The coefficients that result from fitting the model (3.33) are given in the table above  \n",
    "\n",
    "The results strongly suggest that the model that includes the interaction term is superior to the model that contains only main effects.  \n",
    "The p-value for the interaction term, **TV × radio**, is extremely low, indicating that there is strong evidence for  \n",
    "\n",
    "$$\n",
    "H_a : \\beta_3 \\neq 0\n",
    "$$  \n",
    "\n",
    "In other words, it is clear that the true relationship is not additive.  \n",
    "The \\( R^2 \\) for the model (3.33) is **96.8%**, compared to only **89.7%** for the model that predicts sales using TV and radio without an interaction term.  \n",
    "\n",
    "This means that  \n",
    "\n",
    "$$\n",
    "\\frac{96.8 - 89.7}{100 - 89.7} = 69\\%\n",
    "$$  \n",
    "\n",
    "of the variability in sales that remains after fitting the additive model has been explained by the interaction term.  \n",
    "\n",
    "The coefficient estimates in Table 3.9 suggest that an increase in TV advertising of \\$1,000 is associated with increased sales of \n",
    "$(\\hat{\\beta}_1 + \\hat{\\beta}_3 \\cdot \\text{radio}) \\times 1000 = (19 + 1.1 \\cdot \\text{radio})$ units.  \n",
    "\n",
    "And an increase in radio advertising of \\$1,000 will be associated with an increase in sales of      \n",
    "$(\\hat{\\beta}_2 + \\hat{\\beta}_3 \\cdot \\text{TV}) \\times 1000 = (29 + 1.1 \\cdot \\text{TV})$ units.  \n",
    "\n",
    "\n",
    "Hierarchical Principle  \n",
    "\n",
    "In this example, the p-values associated with TV, radio, and the interaction term are all statistically significant (Table 3.9), and so it is obvious that all three variables should be included in the model.  \n",
    "\n",
    "However, it is sometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.  \n",
    "\n",
    "The **hierarchical principle** states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.  \n",
    "\n",
    "In other words, if the interaction between $X_1$ and $X_2$ seems important, then we should include both $X_1$ and $X_2$ in the model even if their coefficient estimates have large p-values.  \n",
    "\n",
    "The rationale for this principle is that if $X_1 \\cdot X_2$ is related to the response, then whether or not the coefficients of $X_1$ or $X_2$ are exactly zero is of little interest.  \n",
    "Also, $X_1 \\cdot X_2$ is typically correlated with $X_1$ and $X_2$, and so leaving them out tends to alter the meaning of the interaction.  \n",
    "\n",
    "\n",
    "Interactions with Qualitative Variables  \n",
    "\n",
    "In the previous example, we considered an interaction between TV and radio, both quantitative variables. However, the concept of interactions applies just as well to qualitative variables, or to a combination of quantitative and qualitative variables.  \n",
    "\n",
    "In fact, an interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.  \n",
    "\n",
    "Consider the **Credit** data set from Section 3.3.1, and suppose that we wish to predict balance using the **income (quantitative)** and **student (qualitative)** variables.  \n",
    "\n",
    "In the absence of an interaction term, the model takes the form  \n",
    "\n",
    "$$\n",
    "\\text{balance}_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2, & \\text{if $i$th person is a student} \\\\\n",
    "\\beta_0 + \\beta_1 \\cdot \\text{income}_i, & \\text{if $i$th person is not a student}\n",
    "\\end{cases}\n",
    "\\tag{3.34}\n",
    "$$  \n",
    "\n",
    "This amounts to fitting two **parallel lines** to the data, one for students and one for non-students.  \n",
    "The lines have different intercepts, \\(\\beta_0 + \\beta_2\\) versus \\(\\beta_0\\), but the same slope \\(\\beta_1\\).  \n",
    "\n",
    "\n",
    "Adding an Interaction Term  \n",
    "\n",
    "This limitation can be addressed by adding an interaction variable, created by multiplying income with the dummy variable for student.  \n",
    "\n",
    "The model now becomes  \n",
    "\n",
    "$$\n",
    "\\text{balance}_i =\n",
    "\\begin{cases}\n",
    "(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\cdot \\text{income}_i, & \\text{if student} \\\\\n",
    "\\beta_0 + \\beta_1 \\cdot \\text{income}_i, & \\text{if not student}\n",
    "\\end{cases}\n",
    "\\tag{3.35}\n",
    "$$  \n",
    "\n",
    "Now the regression lines for students and non-students differ in both **intercepts** and **slopes**.  \n",
    "This allows for the possibility that changes in income may affect the credit card balances of students and non-students differently.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Non-linear Relationships  \n",
    "\n",
    "The linear regression model (3.19) assumes a linear relationship between the response and predictors. But in some cases, the true relationship may be **non-linear**.  \n",
    "\n",
    "Consider **Figure 3.8**, where the mpg (gas mileage) versus horsepower is shown for a number of cars in the Auto dataset.  \n",
    "\n",
    "A simple extension is **polynomial regression**. For example:  \n",
    "\n",
    "$$\n",
    "\\text{mpg} = \\beta_0 + \\beta_1 \\cdot \\text{horsepower} + \\beta_2 \\cdot \\text{horsepower}^2 + \\epsilon\n",
    "\\tag{3.36}\n",
    "$$  \n",
    "\n",
    "This is still a **linear model** in terms of coefficients, even though horsepower enters non-linearly.  \n",
    "\n",
    "\n",
    "Auto Dataset Table\n",
    "\n",
    "| Coefficient  | Std. Error | t-statistic | p-value  |\n",
    "|--------------|------------|-------------|----------|\n",
    "| Intercept    | 56.9001    | 1.8004      | 31.6     | <0.0001 |\n",
    "| horsepower   | -0.4662    | 0.0311      | -15.0    | <0.0001 |\n",
    "| horsepower²  | 0.0012     | 0.0001      | 10.1     | <0.0001 |\n",
    "\n",
    "For the Auto dataset, least squares coefficient estimates associated with the regression of mpg onto horsepower and horsepower².  \n",
    "\n",
    "The quadratic fit $R^2 = 0.688$ is substantially better than the linear fit $R^2 = 0.606$.  \n",
    "However, higher-order polynomials (up to degree 5) may lead to unnecessarily wiggly fits that do not improve interpretability.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166a881",
   "metadata": {},
   "source": [
    "##### 3.3.3 Potential Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c37e6",
   "metadata": {},
   "source": [
    "When we fit a linear regression model to a particular dataset, many problems may occur.  \n",
    "Most common among these are the following:\n",
    "\n",
    "1. Non-linearity of the response-predictor relationships  \n",
    "2. Correlation of error terms  \n",
    "3. Non-constant variance of error terms  \n",
    "4. Outliers  \n",
    "5. High-leverage points  \n",
    "6. Collinearity  \n",
    "\n",
    "In practice, identifying and overcoming these problems is as much an art as a science.  \n",
    "We provide only a brief summary of some key points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30edbb",
   "metadata": {},
   "source": [
    "1. Non-linearity of the Data\n",
    "\n",
    "The linear regression model assumes a straight-line relationship between predictors and the response.  \n",
    "If the true relationship is far from linear, conclusions from the fit are suspect and prediction accuracy can be significantly reduced.  \n",
    "\n",
    "**Residual plots** are a useful graphical tool for identifying non-linearity.  \n",
    "- For simple regression: plot residuals $e_i = y_i - \\hat{y}_i$ vs. predictor $x_i$.  \n",
    "- For multiple regression: plot residuals vs. fitted values $\\hat{y}_i$.\n",
    "\n",
    "Ideally, no pattern should appear. A U-shaped or curved pattern indicates non-linearity.  \n",
    "A simple remedy: apply non-linear transformations of predictors (e.g., $\\log X$, $\\sqrt{X}$, $X^2$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40f5f0",
   "metadata": {},
   "source": [
    "2. Correlation of Error Terms\n",
    "\n",
    "An important assumption is that error terms $\\epsilon_1, \\epsilon_2, ..., \\epsilon_n$ are **uncorrelated**.  \n",
    "\n",
    "- If errors are correlated, standard errors are underestimated → confidence/prediction intervals too narrow.  \n",
    "- $p$-values may be misleadingly small, leading to erroneous significance claims.  \n",
    "\n",
    "**Example:** If we duplicated data (creating pairs of identical observations), SE calculations would act as though we had $2n$.\n",
    " independent samples, creating a false sense of confidence.  \n",
    "\n",
    "Correlated errors often occur in **time series data**: residuals at adjacent time points may be correlated (tracking).  \n",
    "\n",
    "**Remedy:** Check residual plots against time. If patterns exist, specialized time series methods are required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649eecb7",
   "metadata": {},
   "source": [
    "3. Non-constant Variance of Error Terms\n",
    "\n",
    "The assumption:  \n",
    "\n",
    "$$\n",
    "\\text{Var}(\\epsilon_i) = \\sigma^2\n",
    "$$\n",
    "\n",
    "is often violated by **heteroscedasticity**.  \n",
    "- Residual plots showing a \"funnel shape\" (variance increasing with fitted values) are a common indicator.  \n",
    "\n",
    "**Solutions:**  \n",
    "- Transform the response (e.g., $ \\log Y $, $ \\sqrt{Y} $)  \n",
    "- Weighted least squares (if variances known or proportional to sample size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20296da",
   "metadata": {},
   "source": [
    "4. Outliers\n",
    "\n",
    "An **outlier** is a point where $ y_i $ is far from its predicted value.  \n",
    "\n",
    "- Outliers may arise from recording errors or natural variability.  \n",
    "- Outliers may not change slope much, but can inflate **Residual Standard Error (RSE)** or distort $ R^2 $.  \n",
    "\n",
    "**Detection:**  \n",
    "- Plot residuals (large deviations visible)  \n",
    "- Use **studentized residuals**:  \n",
    "\n",
    "$$\n",
    "r_i = \\frac{e_i}{\\hat{\\sigma}(e_i)}\n",
    "$$\n",
    "\n",
    "Values $ |r_i| > 3 $ suggest outliers.  \n",
    "\n",
    "**Action:**  \n",
    "- If caused by data errors → remove  \n",
    "- If genuine → may indicate model deficiency (missing predictor, wrong form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdd430",
   "metadata": {},
   "source": [
    "5. High Leverage Points\n",
    "\n",
    "- Outliers: unusual $y_i$ given predictors  \n",
    "- **High leverage**: unusual predictor value(s) $x_i$  \n",
    "\n",
    "In multiple regression, leverage is measured by:  \n",
    "\n",
    "$$\n",
    "h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}\n",
    "\\tag{3.37}\n",
    "$$\n",
    "\n",
    "- $h_i$ ranges from $\\tfrac{1}{n}$ to $1$  \n",
    "- Average leverage = $\\frac{p+1}{n}$  \n",
    "\n",
    "If an observation’s $h_i$ greatly exceeds this average, it is a **high leverage point**.  \n",
    "\n",
    "Danger: High leverage + high residual → **particularly influential**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d356537",
   "metadata": {},
   "source": [
    "6. Collinearity\n",
    "\n",
    "**Collinearity** occurs when two or more predictors are closely related.  \n",
    "\n",
    "- Makes it difficult to separate effects of individual predictors  \n",
    "- Leads to unstable estimates and inflated standard errors  \n",
    "\n",
    "**Effect on variance:**  \n",
    "- With collinearity, $SE(\\hat{\\beta}_j)$ grows → smaller $t$-statistics → predictors may appear insignificant.  \n",
    "\n",
    "**Detection methods:**  \n",
    "1. Inspect correlation matrix  \n",
    "2. Compute **Variance Inflation Factor (VIF):**\n",
    "\n",
    "$$\n",
    "\\text{VIF}(\\hat{\\beta}_j) = \\frac{1}{1 - R^2_{X_j|X_{-j}}}\n",
    "$$\n",
    "\n",
    "- $R^2_{X_j|X_{-j}}$: regression of $X_j$ on the other predictors $X_{-j}$\n",
    " on all other predictors  \n",
    "- $\\text{VIF}$ > 5 or 10 → problematic collinearity  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac899a1",
   "metadata": {},
   "source": [
    "**Example (Credit data):**  \n",
    "- VIF values: Age = 1.01, Rating = 160.67, Limit = 160.59 → strong collinearity.  \n",
    "\n",
    "**Solutions:**  \n",
    "- Drop one predictor  \n",
    "- Combine collinear variables into a new composite predictor  \n",
    "\n",
    "\n",
    "Example of Collinearity Effects\n",
    "\n",
    "| Coefficient | Std. Error | t-statistic | p-value   |\n",
    "|-------------|------------|-------------|-----------|\n",
    "| **Model 1 (balance ~ age + limit)** ||||\n",
    "| Intercept   | -173.411    | 43.828      | -3.957     | <0.0001 |\n",
    "| age         | 2.292      | 0.672       | -3.407     | 0.0007  |\n",
    "| limit       | 0.173      | 0.005       | 34.496    | <0.0001 |\n",
    "| **Model 2 (balance ~ rating + limit)** ||||\n",
    "| Intercept   | -377.537    | 45.254      | -8.343     | <0.0001 |\n",
    "| rating      | 2.202      | 0.952       | 2.312     | 0.0213  |\n",
    "| limit       | 0.025      | 0.064       | 0.384     | 0.7012  |\n",
    "\n",
    "- Note: Standard error of **limit** increases 12-fold in Model 2 due to collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d408f402",
   "metadata": {},
   "source": [
    "#### The Marketing Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a50cd",
   "metadata": {},
   "source": [
    "The Seven Questions About the Advertising Data\n",
    "\n",
    "1. **Is there a relationship between sales and advertising budget?**  \n",
    "   This question can be answered by fitting a multiple regression model of sales onto TV, radio, and newspaper, as in (3.20), and testing the hypothesis:  \n",
    "\n",
    "   $$\n",
    "   H_0 : \\beta_{\\text{TV}} = \\beta_{\\text{radio}} = \\beta_{\\text{newspaper}} = 0\n",
    "   $$\n",
    "\n",
    "   Previously, we showed that the F-statistic can be used to determine whether or not we should reject this null hypothesis. In this case the p-value corresponding to the F-statistic in Table 3.6 is very low, indicating clear evidence of a relationship between advertising and sales. \n",
    "\n",
    "2. **How strong is the relationship?**  \n",
    "   We discussed two measures of model accuracy in Section 3.1.3:  \n",
    "\n",
    "   - The **RSE** estimates the standard deviation of the response from the population regression line.  \n",
    "     For the Advertising data, the RSE is 1.69 units while the mean value for the response is 14.022, indicating a percentage error of roughly 12%.  \n",
    "   - The **$R^2$ statistic** records the percentage of variability in the response explained by the predictors.  \n",
    "     The predictors explain almost 90% of the variance in sales.  \n",
    "\n",
    "   The RSE and $R^2$ statistics are displayed in Table 3.6.  \n",
    "\n",
    "3. **Which media are associated with sales?**  \n",
    "   To answer this question, we can examine the p-values associated with each predictor’s t-statistic.  \n",
    "\n",
    "   - In the multiple regression in Table 3.4, the p-values for TV and radio are low.  \n",
    "   - The p-value for newspaper is not.  \n",
    "\n",
    "   This suggests that only TV and radio are related to sales.  \n",
    "\n",
    "4. **How large is the association between each medium and sales?**  \n",
    "   The standard error of $\\hat{\\beta}_j$ can be used to construct confidence intervals for $\\beta_j$.  \n",
    "\n",
    "   For the Advertising data, using Table 3.4:  \n",
    "\n",
    "   - TV: (0.043, 0.049)  \n",
    "   - Radio: (0.172, 0.206)  \n",
    "   - Newspaper: (-0.013, 0.011)  \n",
    "\n",
    "   The intervals for TV and radio are narrow and far from zero → strong evidence of association.  \n",
    "   Newspaper’s interval includes zero → not statistically significant, given TV and radio.  \n",
    "\n",
    "   Collinearity can result in wide standard errors. Could this explain the newspaper result?  \n",
    "   The VIF scores are 1.005 (TV), 1.145 (radio), and 1.145 (newspaper), suggesting no evidence of collinearity.  \n",
    "\n",
    "   Assessing each medium individually (via simple linear regressions, Tables 3.1 and 3.3):  \n",
    "   - Strong association: TV and sales, radio and sales  \n",
    "   - Mild association: newspaper and sales (ignoring TV and radio)  \n",
    "\n",
    "5. **How accurately can we predict future sales?**  \n",
    "   The response can be predicted using (3.21).  \n",
    "\n",
    "   - To predict an individual response: $Y = f(X) + \\epsilon$ → use a **prediction interval**.  \n",
    "   - To predict the average response: $f(X)$ → use a **confidence interval**.  \n",
    "\n",
    "   Prediction intervals are always wider than confidence intervals, since they also account for irreducible error $\\epsilon$.  \n",
    "\n",
    "6. **Is the relationship linear?**  \n",
    "   Residual plots (Section 3.3.3) can reveal non-linearity.  \n",
    "   - If linear: residuals show no pattern.  \n",
    "   - In the Advertising data: Figure 3.5 suggests a non-linear effect (also visible in residual plots).  \n",
    "\n",
    "   Section 3.3.2 discussed including transformations of predictors to handle non-linear relationships.  \n",
    "\n",
    "7. **Is there synergy among the advertising media?**  \n",
    "   Standard linear regression assumes **additivity**: each predictor’s effect is independent of others.  \n",
    "\n",
    "   - Advantage: easy interpretation  \n",
    "   - Limitation: may be unrealistic  \n",
    "\n",
    "   Section 3.3.2 showed how to include **interaction terms** to allow non-additive effects.  \n",
    "   - A small p-value for the interaction term indicates such relationships.  \n",
    "   - In the Advertising data, Figure 3.5 suggested non-additivity.  \n",
    "   - Adding an interaction term raised $R^2$ from ~90% to ~97%.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfff7be",
   "metadata": {},
   "source": [
    "####  Comparison of Linear Regression with K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87df16a",
   "metadata": {},
   "source": [
    "As discussed in Chapter 2, **linear regression** is an example of a **parametric approach** because it assumes a linear functional form for $f(X)$.  \n",
    "\n",
    "**Advantages of parametric methods:**  \n",
    "- Easy to fit: only a small number of coefficients must be estimated.  \n",
    "- Coefficients have simple interpretations.  \n",
    "- Statistical significance tests are straightforward.  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- They impose strong assumptions about the form of $f(X)$.  \n",
    "- If the assumed functional form is far from the truth, prediction accuracy will suffer.  \n",
    "- Example: if we assume a linear relationship between $X$ and $Y$, but the true relationship is highly non-linear, the resulting model will provide a poor fit.  \n",
    "\n",
    "Unlike parametric methods, **non-parametric methods** do not explicitly assume a functional form for $f(X)$, making them more flexible.  \n",
    "\n",
    "KNN regression is one of the simplest non-parametric methods:  \n",
    "1. Given a value of $K$ and a prediction point $x_0$, KNN regression finds the $K$ training observations closest to $x_0$.  \n",
    "   - Let this neighborhood be $N_0$.  \n",
    "2. Estimate $f(x_0)$ as the **average response** of observations in $N_0$:  \n",
    "\n",
    "$$\n",
    "\\hat{f}(x_0) = \\frac{1}{K} \\sum_{x_i \\in N_0} y_i\n",
    "$$\n",
    "\n",
    "Example Fits\n",
    "- **$K=1$**: perfectly interpolates the training data → rough step function.  \n",
    "- **$K=9$**: smoother fit due to averaging over more observations.  \n",
    "\n",
    "**Bias-variance tradeoff:**  \n",
    "- Small $K$: flexible fit → low bias, high variance.  \n",
    "- Large $K$: smoother fit → higher bias, lower variance.  \n",
    "\n",
    "\n",
    "When Does Linear Regression Outperform KNN?\n",
    "\n",
    "- If the **true form of $f$ is close to linear**, linear regression performs better.  \n",
    "- Example: data generated from a one-dimensional linear model.  \n",
    "  - $K=1$: predictions too variable.  \n",
    "  - $K=9$: smoother fit but still worse than linear regression.  \n",
    "- Since the true relationship is linear, **linear regression achieves much lower test MSE**.  \n",
    "\n",
    "\n",
    "Increasing Non-Linearity\n",
    "\n",
    "- If the relationship between $X$ and $Y$ is **slightly non-linear**:  \n",
    "  - Linear regression is still competitive for small $K$.  \n",
    "  - For larger $K$, KNN can outperform linear regression.  \n",
    "- If the relationship is **strongly non-linear**:  \n",
    "  - KNN outperforms linear regression for most $K$.  \n",
    "  - Linear regression suffers from large bias.  \n",
    "\n",
    "\n",
    "Curse of Dimensionality\n",
    "\n",
    "KNN performance degrades rapidly as the number of predictors $p$ increases:  \n",
    "\n",
    "- With $p=1$, 50 observations are sufficient to estimate $f(X)$.  \n",
    "- With $p=20$, the same 50 points are sparse in high-dimensional space.  \n",
    "- Nearest neighbors may be very far from $x_0$, leading to poor predictions.  \n",
    "\n",
    "This phenomenon is called the **curse of dimensionality**.  \n",
    "\n",
    "\n",
    "Practical Implications\n",
    "\n",
    "- **Parametric methods (like linear regression):**  \n",
    "  - More robust with small $n$ (few observations per predictor).  \n",
    "  - Provide interpretable coefficients and statistical inference.  \n",
    "\n",
    "- **KNN:**  \n",
    "  - Can outperform parametric methods when the relationship is highly non-linear.  \n",
    "  - More flexible but less interpretable.  \n",
    "  - Suffers in high dimensions.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddcd20",
   "metadata": {},
   "source": [
    "#### Lab: Linear Regression\n",
    "\n",
    "##### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91278026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d52c4",
   "metadata": {},
   "source": [
    " New imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4531deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c81293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence \\\n",
    "     import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31e5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                        summarize,\n",
    "                        poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcaf2e6",
   "metadata": {},
   "source": [
    " Inspecting Objects and Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71ffe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'MS',\n",
       " 'Out',\n",
       " 'VIF',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__vsc_ipynb_file__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'anova_lm',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'load_data',\n",
       " 'np',\n",
       " 'open',\n",
       " 'pd',\n",
       " 'poly',\n",
       " 'quit',\n",
       " 'sm',\n",
       " 'subplots',\n",
       " 'summarize']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b303f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
