{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d02f63a",
   "metadata": {},
   "source": [
    "### Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38c5e2",
   "metadata": {},
   "source": [
    "In the regression setting, the standard linear model\n",
    "Y = 0+ 1X1+···+ pXp+\n",
    "(6.1)\n",
    "is commonly used to describe the relationship between a response Y and\n",
    "a set of variables X1,X2,...,Xp. We have seen in Chapter 3 that one\n",
    "typically fits this model using least squares.\n",
    "In the chapters that follow, we consider some approaches for extending\n",
    "the linear model framework. In Chapter 7 we generalize (6.1) in order to\n",
    "accommodate non-linear, but still additive, relationships, while in Chap\n",
    "ters 8 and 10 we consider even more general non-linear models. However,\n",
    "the linear model has distinct advantages in terms of inference and, on real\n",
    "world problems, is often surprisingly competitive in relation to non-linear\n",
    "methods. Hence, before moving to the non-linear world, we discuss in this\n",
    "chapter some ways in which the simple linear model can be improved, by re\n",
    "placing plain least squares fitting with some alternative fitting procedures.\n",
    "Why might we want to use another fitting procedure instead of least\n",
    "squares? As we will see, alternative fitting procedures can yield better pre\n",
    "diction accuracy and model interpretability.\n",
    "\n",
    "• Prediction Accuracy: Provided that the true relationship between the\n",
    "response and the predictors is approximately linear, the least squares\n",
    "estimates will have low bias. If n \n",
    "p—that is, if n, the number of\n",
    "observations, is much larger than p, the number of variables—then the\n",
    "least squares estimates tend to also have low variance, and hence will\n",
    "perform well on test observations. However, if n is not much larger\n",
    "than p, then there can be a lot of variability in the least squares fit,\n",
    "resulting in overfitting and consequently poor predictions on future\n",
    "observations not used in model training. And if p>n, then there is no\n",
    "longer a unique least squares coefficient estimate: there are infinitely many solutions. Each of these least squares solutions gives zero error\n",
    "on the training data, but typically very poor test set performance\n",
    "due to extremely high variance.1 By constraining or shrinking the\n",
    "estimated coefficients, we can often substantially reduce the variance\n",
    "at the cost of a negligible increase in bias. This can lead to substantial\n",
    "improvements in the accuracy with which we can predict the response\n",
    "for observations not used in model training.\n",
    "\n",
    "• Model Interpretability: It is often the case that some or many of the\n",
    "variables used in a multiple regression model are in fact not associ\n",
    "ated with the response. Including such irrelevant variables leads to\n",
    "unnecessary complexity in the resulting model. By removing these\n",
    "variables—that is, by setting the corresponding coefficient estimates\n",
    "to zero—we can obtain a model that is more easily interpreted. Now\n",
    "least squares is extremely unlikely to yield any coefficient estimates\n",
    "that are exactly zero. In this chapter, we see some approaches for au\n",
    "tomatically performing feature selection or variable selection—that is, feature\n",
    "for excluding irrelevant variables from a multiple regression model.\n",
    "\n",
    "There are many alternatives, both classical and modern, to using least\n",
    "squares to fit (6.1). In this chapter, we discuss three important classes of\n",
    "methods.\n",
    "\n",
    "• Subset Selection. This approach involves identifying a subset of the p\n",
    "predictors that we believe to be related to the response. We then fit\n",
    "a model using least squares on the reduced set of variables.\n",
    "\n",
    "• Shrinkage. This approach involves fitting a model involving all p pre\n",
    "dictors. However, the estimated coefficients are shrunken towards zero\n",
    "relative to the least squares estimates. This shrinkage (also known as\n",
    "regularization) has the effect of reducing variance. Depending on what\n",
    "type of shrinkage is performed, some of the coefficients may be esti\n",
    "mated to be exactly zero. Hence, shrinkage methods can also perform\n",
    "variable selection.\n",
    "\n",
    "• Dimension Reduction. This approach involves projecting the p predic\n",
    "tors into an M-dimensional subspace, where M<p.This is achieved\n",
    "by computing M different linear combinations, or projections, of the\n",
    "variables. Then these M projections are used as predictors to fit a\n",
    "linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96fdead",
   "metadata": {},
   "source": [
    "#### Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d046b",
   "metadata": {},
   "source": [
    "##### Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac9018",
   "metadata": {},
   "source": [
    "To perform *best subset selection*, we fit a separate least squares regression for each of the models of the p predictors. That is, we fit all p models that contain exactly one predictor, all $ \\binom{p}{2} $ models that contain exactly two predictors, and so on. We look at all of the resulting models, with the goal of identifying the one that is the best.\n",
    "\n",
    "The problem of selecting the best model from among the $ 2^p $ possibilities considered by best subset selection is then usually broken up into two stages, as described in Algorithm 6.1.\n",
    "\n",
    "##### **Algorithm 6.1** *Best subset selection*\n",
    "\n",
    "1. Let $ M_0 $ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\n",
    "\n",
    "2. For $ k = 1, 2, \\ldots, p $:\n",
    "\n",
    "   (a) Fit all $ \\binom{p}{k} $ models that contain exactly k predictors.\n",
    "   \n",
    "   (b) Pick the best among these models, and call it $ M_k $. Here “best” is defined as having the smallest RSS, or equivalently $ R^2 $.\n",
    "\n",
    "3. Select a single best model from among $ M_0, \\ldots, M_p $ using the prediction error on a validation set, $ C_p $ (AIC), BIC, or adjusted $ R^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33b0f1",
   "metadata": {},
   "source": [
    "In Algorithm 6.1, Step 2 identifies the best model (on the training data)\n",
    "for each subset size, in order to reduce the problem from one of 2p possible\n",
    "models to one of p +1possible models. In Figure 6.1, these models form\n",
    "the lower frontier depicted in red.\n",
    "Now in order to select a single best model, we must simply choose among\n",
    "these p +1options. This task must be performed with care, because the\n",
    "RSS of these p +1models decreases monotonically, and the R2 increases\n",
    "monotonically, as the number of features included in the models increases.\n",
    "Therefore, if we use these statistics to select the best model, then we will\n",
    "always end up with a model involving all of the variables. The problem is\n",
    "that a low RSS or a high R2 indicates a model with a low training error,\n",
    "whereas we wish to choose a model that has a low test error. (As shown in\n",
    "Chapter 2 in Figures 2.9–2.11, training error tends to be quite a bit smaller\n",
    "than test error, and a low training error by no means guarantees a low test\n",
    "error.) Therefore, in Step 3, we use the error on a validation set, Cp, BIC, or\n",
    "adjusted R2 in order to select among M0,M1,...,Mp. If cross-validation\n",
    "is used to select the best model, then Step 2 is repeated on each training\n",
    "fold, and the validation errors are averaged to select the best value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd273c",
   "metadata": {},
   "source": [
    "Then the model Mk fit on the full training set is delivered for the chosen\n",
    "k. These approaches are discussed in Section 6.1.3.\n",
    "An application of best subset selection is shown in Figure 6.1. Each\n",
    "plotted point corresponds to a least squares regression model fit using a\n",
    "different subset of the 10 predictors in the Credit data set, discussed in\n",
    "Chapter 3. Here the variable region is a three-level qualitative variable,\n",
    "and so is represented by two dummy variables, which are selected sepa\n",
    "rately in this case. Hence, there are a total of 11 possible variables which\n",
    "can be included in the model. We have plotted the RSS and R2 statistics\n",
    "for each model, as a function of the number of variables. The red curves\n",
    "connect the best models for each model size, according to RSS or R2. The\n",
    "f\n",
    "igure shows that, as expected, these quantities improve as the number of\n",
    "variables increases; however, from the three-variable model on, there is little\n",
    "improvement in RSS and R2 as a result of including additional predictors.\n",
    "Although we have presented best subset selection here for least squares\n",
    "regression, the same ideas apply to other types of models, such as logistic\n",
    "regression. In the case of logistic regression, instead of ordering models by\n",
    "RSS in Step 2 of Algorithm 6.1, we instead use the deviance, a measure deviance\n",
    "that plays the role of RSS for a broader class of models. The deviance is\n",
    "negative two times the maximized log-likelihood; the smaller the deviance,\n",
    "the better the fit.\n",
    "While best subset selection is a simple and conceptually appealing ap\n",
    "proach, it suffers from computational limitations. The number of possible\n",
    "models that must be considered grows rapidly as p increases. In general,\n",
    "there are 2p models that involve subsets of p predictors. So if p = 10,\n",
    "then there are approximately 1,000 possible models to be considered, and if\n",
    "p =20,then there are over one million possibilities! Consequently, best sub\n",
    "set selection becomes computationally infeasible for values of p greater than\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
