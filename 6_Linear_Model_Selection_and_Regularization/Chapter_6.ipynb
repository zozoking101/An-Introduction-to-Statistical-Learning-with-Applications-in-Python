{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d02f63a",
   "metadata": {},
   "source": [
    "### Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38c5e2",
   "metadata": {},
   "source": [
    "In the regression setting, the standard linear model\n",
    "Y = 0+ 1X1+···+ pXp+\n",
    "(6.1)\n",
    "is commonly used to describe the relationship between a response Y and\n",
    "a set of variables X1,X2,...,Xp. We have seen in Chapter 3 that one\n",
    "typically fits this model using least squares.\n",
    "In the chapters that follow, we consider some approaches for extending\n",
    "the linear model framework. In Chapter 7 we generalize (6.1) in order to\n",
    "accommodate non-linear, but still additive, relationships, while in Chap\n",
    "ters 8 and 10 we consider even more general non-linear models. However,\n",
    "the linear model has distinct advantages in terms of inference and, on real\n",
    "world problems, is often surprisingly competitive in relation to non-linear\n",
    "methods. Hence, before moving to the non-linear world, we discuss in this\n",
    "chapter some ways in which the simple linear model can be improved, by re\n",
    "placing plain least squares fitting with some alternative fitting procedures.\n",
    "Why might we want to use another fitting procedure instead of least\n",
    "squares? As we will see, alternative fitting procedures can yield better pre\n",
    "diction accuracy and model interpretability.\n",
    "\n",
    "• Prediction Accuracy: Provided that the true relationship between the\n",
    "response and the predictors is approximately linear, the least squares\n",
    "estimates will have low bias. If n \n",
    "p—that is, if n, the number of\n",
    "observations, is much larger than p, the number of variables—then the\n",
    "least squares estimates tend to also have low variance, and hence will\n",
    "perform well on test observations. However, if n is not much larger\n",
    "than p, then there can be a lot of variability in the least squares fit,\n",
    "resulting in overfitting and consequently poor predictions on future\n",
    "observations not used in model training. And if p>n, then there is no\n",
    "longer a unique least squares coefficient estimate: there are infinitely many solutions. Each of these least squares solutions gives zero error\n",
    "on the training data, but typically very poor test set performance\n",
    "due to extremely high variance.1 By constraining or shrinking the\n",
    "estimated coefficients, we can often substantially reduce the variance\n",
    "at the cost of a negligible increase in bias. This can lead to substantial\n",
    "improvements in the accuracy with which we can predict the response\n",
    "for observations not used in model training.\n",
    "\n",
    "• Model Interpretability: It is often the case that some or many of the\n",
    "variables used in a multiple regression model are in fact not associ\n",
    "ated with the response. Including such irrelevant variables leads to\n",
    "unnecessary complexity in the resulting model. By removing these\n",
    "variables—that is, by setting the corresponding coefficient estimates\n",
    "to zero—we can obtain a model that is more easily interpreted. Now\n",
    "least squares is extremely unlikely to yield any coefficient estimates\n",
    "that are exactly zero. In this chapter, we see some approaches for au\n",
    "tomatically performing feature selection or variable selection—that is, feature\n",
    "for excluding irrelevant variables from a multiple regression model.\n",
    "\n",
    "There are many alternatives, both classical and modern, to using least\n",
    "squares to fit (6.1). In this chapter, we discuss three important classes of\n",
    "methods.\n",
    "\n",
    "• Subset Selection. This approach involves identifying a subset of the p\n",
    "predictors that we believe to be related to the response. We then fit\n",
    "a model using least squares on the reduced set of variables.\n",
    "\n",
    "• Shrinkage. This approach involves fitting a model involving all p pre\n",
    "dictors. However, the estimated coefficients are shrunken towards zero\n",
    "relative to the least squares estimates. This shrinkage (also known as\n",
    "regularization) has the effect of reducing variance. Depending on what\n",
    "type of shrinkage is performed, some of the coefficients may be esti\n",
    "mated to be exactly zero. Hence, shrinkage methods can also perform\n",
    "variable selection.\n",
    "\n",
    "• Dimension Reduction. This approach involves projecting the p predic\n",
    "tors into an M-dimensional subspace, where M<p.This is achieved\n",
    "by computing M different linear combinations, or projections, of the\n",
    "variables. Then these M projections are used as predictors to fit a\n",
    "linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96fdead",
   "metadata": {},
   "source": [
    "#### Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d046b",
   "metadata": {},
   "source": [
    "##### Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac9018",
   "metadata": {},
   "source": [
    "To perform *best subset selection*, we fit a separate least squares regression for each of the models of the p predictors. That is, we fit all p models that contain exactly one predictor, all $ \\binom{p}{2} $ models that contain exactly two predictors, and so on. We look at all of the resulting models, with the goal of identifying the one that is the best.\n",
    "\n",
    "The problem of selecting the best model from among the $ 2^p $ possibilities considered by best subset selection is then usually broken up into two stages, as described in Algorithm 6.1.\n",
    "\n",
    "##### **Algorithm 6.1** *Best subset selection*\n",
    "\n",
    "1. Let $ M_0 $ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\n",
    "\n",
    "2. For $ k = 1, 2, \\ldots, p $:\n",
    "\n",
    "   (a) Fit all $ \\binom{p}{k} $ models that contain exactly k predictors.\n",
    "   \n",
    "   (b) Pick the best among these models, and call it $ M_k $. Here “best” is defined as having the smallest RSS, or equivalently $ R^2 $.\n",
    "\n",
    "3. Select a single best model from among $ M_0, \\ldots, M_p $ using the prediction error on a validation set, $ C_p $ (AIC), BIC, or adjusted $ R^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33b0f1",
   "metadata": {},
   "source": [
    "In Algorithm 6.1, Step 2 identifies the best model (on the training data)\n",
    "for each subset size, in order to reduce the problem from one of 2p possible\n",
    "models to one of p +1possible models. In Figure 6.1, these models form\n",
    "the lower frontier depicted in red.\n",
    "Now in order to select a single best model, we must simply choose among\n",
    "these p +1options. This task must be performed with care, because the\n",
    "RSS of these p +1models decreases monotonically, and the R2 increases\n",
    "monotonically, as the number of features included in the models increases.\n",
    "Therefore, if we use these statistics to select the best model, then we will\n",
    "always end up with a model involving all of the variables. The problem is\n",
    "that a low RSS or a high R2 indicates a model with a low training error,\n",
    "whereas we wish to choose a model that has a low test error. (As shown in\n",
    "Chapter 2 in Figures 2.9–2.11, training error tends to be quite a bit smaller\n",
    "than test error, and a low training error by no means guarantees a low test\n",
    "error.) Therefore, in Step 3, we use the error on a validation set, Cp, BIC, or\n",
    "adjusted R2 in order to select among M0,M1,...,Mp. If cross-validation\n",
    "is used to select the best model, then Step 2 is repeated on each training\n",
    "fold, and the validation errors are averaged to select the best value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd273c",
   "metadata": {},
   "source": [
    "Then the model Mk fit on the full training set is delivered for the chosen\n",
    "k. These approaches are discussed in Section 6.1.3.\n",
    "An application of best subset selection is shown in Figure 6.1. Each\n",
    "plotted point corresponds to a least squares regression model fit using a\n",
    "different subset of the 10 predictors in the Credit data set, discussed in\n",
    "Chapter 3. Here the variable region is a three-level qualitative variable,\n",
    "and so is represented by two dummy variables, which are selected sepa\n",
    "rately in this case. Hence, there are a total of 11 possible variables which\n",
    "can be included in the model. We have plotted the RSS and R2 statistics\n",
    "for each model, as a function of the number of variables. The red curves\n",
    "connect the best models for each model size, according to RSS or R2. The\n",
    "f\n",
    "igure shows that, as expected, these quantities improve as the number of\n",
    "variables increases; however, from the three-variable model on, there is little\n",
    "improvement in RSS and R2 as a result of including additional predictors.\n",
    "Although we have presented best subset selection here for least squares\n",
    "regression, the same ideas apply to other types of models, such as logistic\n",
    "regression. In the case of logistic regression, instead of ordering models by\n",
    "RSS in Step 2 of Algorithm 6.1, we instead use the deviance, a measure deviance\n",
    "that plays the role of RSS for a broader class of models. The deviance is\n",
    "negative two times the maximized log-likelihood; the smaller the deviance,\n",
    "the better the fit.\n",
    "While best subset selection is a simple and conceptually appealing ap\n",
    "proach, it suffers from computational limitations. The number of possible\n",
    "models that must be considered grows rapidly as p increases. In general,\n",
    "there are 2p models that involve subsets of p predictors. So if p = 10,\n",
    "then there are approximately 1,000 possible models to be considered, and if\n",
    "p =20,then there are over one million possibilities! Consequently, best sub\n",
    "set selection becomes computationally infeasible for values of p greater than around 40, even with extremely fast modern computers. There are compu\n",
    "tational shortcuts—so called branch-and-bound techniques—for eliminat\n",
    "ing some choices, but these have their limitations as p gets large. They also\n",
    "only work for least squares linear regression. We present computationally\n",
    "efficient alternatives to best subset selection next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097842b",
   "metadata": {},
   "source": [
    "##### **Algorithm 6.2** *Forward stepwise selection*\n",
    "\n",
    "1. Let $ M_0 $ denote the null model, which contains no predictors.\n",
    "\n",
    "2. For $ k = 0, \\ldots, p - 1 $:\n",
    "\n",
    "   - (a) Consider all $ p - k $ models that augment the predictors in $ M_k $ with one additional predictor.\n",
    "   \n",
    "   - (b) Choose the best among these $ p - k $ models, and call it $ M_{k+1} $. Here best is defined as having the smallest RSS or highest $ R^2 $.\n",
    "\n",
    "3. Select a single best model from among $ M_0, \\ldots, M_p $ using the prediction error on a validation set, $ C_p $ (AIC), BIC, or adjusted $ R^2 $. Or use the cross-validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f255d",
   "metadata": {},
   "source": [
    "##### Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc158be",
   "metadata": {},
   "source": [
    "For computational reasons, best subset selection cannot be applied with\n",
    "very large p. Best subset selection may also suffer from statistical problems\n",
    "when p is large. The larger the search space, the higher the chance of finding\n",
    "models that look good on the training data, even though they might not\n",
    "have any predictive power on future data. Thus an enormous search space\n",
    "can lead to overfitting and high variance of the coefficient estimates.\n",
    "For both of these reasons, stepwise methods, which explore a far more\n",
    "restricted set of models, are attractive alternatives to best subset selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679aab05",
   "metadata": {},
   "source": [
    "Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36191d1",
   "metadata": {},
   "source": [
    "*Forward stepwise selection* is a computationally efficient alternative to best subset selection. While the best subset selection procedure considers all\n",
    "2p possible models containing subsets of the p predictors, forward step\n",
    "wise considers a much smaller set of models. Forward stepwise selection\n",
    "begins with a model containing no predictors, and then adds predictors\n",
    "to the model, one-at-a-time, until all of the predictors are in the model.\n",
    "In particular, at each step the variable that gives the greatest additional\n",
    "improvement to the fit is added to the model. More formally, the forward\n",
    "stepwise selection procedure is given in Algorithm 6.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a161f3",
   "metadata": {},
   "source": [
    "## 6. Linear Model Selection and Regularization\n",
    "\n",
    "Unlike best subset selection, which involved fitting $ 2^p $ models, forward stepwise selection involves fitting one null model, along with $ p - k $ models in the $ k $-th iteration, for $ k = 0, \\ldots, p - 1 $. This amounts to a total of $ 1 + \\sum_{i=0}^{p-1} (p - i) = 1 + p(p + 1)/2 $ models. This is a substantial difference: when $p$ = 20, best subset selection requires fitting $ 1,048,576 $ models, whereas forward stepwise selection requires fitting only $ 21 $ models. \n",
    "\n",
    "In step 2(b) of Algorithm 6.2, we identify the best model from those available, which augment $ M_k $ with one additional predictor. We do this by simply choosing the model with the lowest RSS or the highest $ R^2 $. In this case, we must identify the best model from a set of models with different numbers of variables. This is more challenging, and is discussed in Section 6.1.3.\n",
    "\n",
    "Forward stepwise selection's computational advantage over best subset selection is clear. Though forward stepwise seems to work well in practice, it is important to keep in mind that it is fundamentally a greedy algorithm. In a data set with $ p $ predictors, the best possible model will include predictors $ X_2 $ and $ X_1 $. However, forward stepwise selection is still unable to find the best possible model among $ M_1, M_2, $ and those available with $ X_1 $ together with an additional variable.\n",
    "\n",
    "As shown in Section 6.1.3, the forward stepwise selection on the Credit data set illustrates this phenomenon. Both forward stepwise selection and best subset selection favored models that included the predictors rating and income, whereas the best subset selection also included the variable student.\n",
    "\n",
    "In high-dimensional settings where $ p $ is greater than $ n $, forward stepwise selection can still be applied even when subset selection cannot. If $ p $ is greater than $ n $, each time an additional variable is included, only the subset of $ M_0, \\ldots, M_k $ can be constructed, which avoids overfitting, as each submodel is fit using least squares, which does not yield a unique solution if $ p > n $.\n",
    "\n",
    "\n",
    "\n",
    "| # Variables | Best subset                      | Forward stepwise                  |\n",
    "|-------------|----------------------------------|-----------------------------------|\n",
    "| One         | `rating`                           | `rating`                            |\n",
    "| Two         | `rating`, `income`                   | `rating`, `income`                    |\n",
    "| Three       | `rating`, `income`, `student`              | `rating`, `income`, `student`               |\n",
    "| Four        | `cards`, `income`, `student`, `limit` | `rating`, `income`, `student`, `limit` |\n",
    "\n",
    "The first four selected models for best subset selection and forward stepwise selection on the Credit data set. The first three models are identical but the fourth models differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33c838",
   "metadata": {},
   "source": [
    "Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bc443",
   "metadata": {},
   "source": [
    "Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward step\n",
    "wise selection, it begins with the full least squares model containing all p\n",
    "predictors, and then iteratively removes the least useful predictor, one-at\n",
    "a-time. Details are given in Algorithm 6.3.\n",
    "\n",
    "\n",
    "Like forward stepwise selection, the backward selection approach searches\n",
    "through only 1+p(p+1)/2 models, and so can be applied in settings where\n",
    "p is too large to apply best subset selection.3 Also like forward stepwise\n",
    "selection, backward stepwise selection is not guaranteed to yield the best\n",
    "model containing a subset of the p predictors.\n",
    "Backward selection requires that the number of samples n is larger than\n",
    "the number of variables p (so that the full model can be fit). In contrast,\n",
    "forward stepwise can be used even when n<p, and so is the only viable\n",
    "subset method when p is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df46a",
   "metadata": {},
   "source": [
    "Hybrid Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a7e4e",
   "metadata": {},
   "source": [
    "The best subset, forward stepwise, and backward stepwise selection ap\n",
    "proaches generally give similar but not identical models. As another al\n",
    "ternative, hybrid versions of forward and backward stepwise selection are\n",
    "available, in which variables are added to the model sequentially, in analogy\n",
    "to forward selection. However, after adding each new variable, the method\n",
    "may also remove any variables that no longer provide an improvement in\n",
    "the model fit. Such an approach attempts to more closely mimic best sub\n",
    "set selection while retaining the computational advantages of forward and\n",
    "backward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b066da3",
   "metadata": {},
   "source": [
    "##### Choosing the Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df32254",
   "metadata": {},
   "source": [
    "Best subset selection, forward selection, and backward selection result in\n",
    "the creation of a set of models, each of which contains a subset of the p predictors. To apply these methods, we need a way to determine which of\n",
    "these models is best. As we discussed in Section 6.1.1, the model containing\n",
    "all of the predictors will always have the smallest RSS and the largest R2,\n",
    "since these quantities are related to the training error. Instead, we wish to\n",
    "choose a model with a low test error. As is evident here, and as we show\n",
    "in Chapter 2, the training error can be a poor estimate of the test error.\n",
    "Therefore, RSS and R2 are not suitable for selecting the best model among\n",
    "a collection of models with different numbers of predictors.\n",
    "In order to select the best model with respect to test error, we need to\n",
    "estimate this test error. There are two common approaches:\n",
    "\n",
    "1. We can indirectly estimate test error by making an adjustment to the\n",
    "training error to account for the bias due to overfitting.\n",
    "\n",
    "\n",
    "2. We can directly estimate the test error, using either a validation set\n",
    "approach or a cross-validation approach, as discussed in Chapter 5.\n",
    "\n",
    "We consider both of these approaches below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f37cc",
   "metadata": {},
   "source": [
    "We show in Chapter 2 that the training set $ R^2 $ is generally an under-estimate of the test MSE. (Recall that MSE = RSS/n.) In training RSS, but when we fit a model to the training data using least squares, we (not the test RSS) is as small as possible, this can sometimes misestimate the regression coefficients in a model. Thus, we need to choose from among a set of models with different variables.\n",
    "\n",
    "However, a number of techniques for adjusting the training error can lead to a set of models with different variables. We introduce the following metrics for model selection criteria: \n",
    "\n",
    "- **Akaike information criterion (AIC)**, \n",
    "- **Bayesian information criterion (BIC)**, and \n",
    "- **Adjusted $ R^2 $**. \n",
    "\n",
    "Figure 6.2 gives a comparison of the best model selection criteria and selects the best subset on the Credit data set.\n",
    "\n",
    "For a fitted least squares model containing $ p $ predictors, the $ C_p $ estimate of test MSE is computed using the equation:\n",
    "\n",
    "$$\n",
    "C_p = \\frac{(RSS + 2d\\hat{\\sigma}^2)}{n}\n",
    "$$\n",
    "\n",
    "where $ \\hat{\\sigma}^2 $ is an estimate of the variance of the error $ \\epsilon $ associated with the regression model in (6.1). Typically $ d $ is chosen to be $ p $ or the number of parameters in the model. The $ C_p $ statistic adds a penalty term involving $ d $ in order to adjust for the number of predictors  in the model increases; this is intended to adjust for the corresponding decrease in training RSS. Based on the scope of this book, one can show that if $ \\hat{\\sigma}^2 $ in (6.2), then $ C_p $ is an unbiased estimate of test MSE. As a consequence, the $ C_p $ statistic tends to take a larger value in the model selected with the lowest $ C_p $ value. In Figure 6.2, C_p selects the six-variable model containing the predictors income, lint, age, and risk.\n",
    "\n",
    "The AIC criterion is defined as follows:\n",
    "$$\n",
    "AIC = \\frac{1}{n} (RSS + 2d\\hat{\\sigma}^2)\n",
    "$$\n",
    "where, for simplicity, we have omitted irrelevant constants. Here for least squares models, AIC and $ C_p $ are proportional to each other, as shown in Figure 6.2.\n",
    "\n",
    "BIC is derived from a Bayesian point of view; it ends up looking similar to $ C_p $ and $ AIC $ for the least squares model but with a different penalty term:\n",
    "$$\n",
    "BIC = \\frac{1}{n} (RSS + \\log(n)d\\hat{\\sigma}^2)\n",
    "$$\n",
    "Like $ C_p $, the BIC also penalizes a more complex model for a loss in precision, and generally we select the model with the lowest BIC. Notice that the BIC penalty includes the $ 2d\\ \\log(n) $ term, where $ n $ is the number of observations. Since $ \\log n > 2 $ for any $ n > 7 $, the BIC statistic generally places a heavier penalty on models with many variables, and hence reveals in the selection of smaller models than $ C_p $. In Figure 6.2, we see that the direct task decider is a single model that contains only the four predictors income, lint, age, and assertion. In this case the chosen model does not appear to make much difference in accuracy between the four-variable and six-variable models.\n",
    "\n",
    "The adjusted $ R^2 $ statistic is another popular approach for selecting among a set of models that contain different numbers of variables. Recall from Chapter 3 that the adjusted $ R^2 $ is defined as follows, where $ RSS = \\sum(y_i - \\hat{y})^2 $ is the total sum of squared response. Since $ RSS $ increases as more variables are added to the model, the $ R^2 $ measure becomes biased toward a larger selected model of variables, the adjusted $ R^2 $ is calculated as\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n - 1)}\n",
    "$$\n",
    "\n",
    "Unlike $ C_p $ and BIC, for which a small value indicates a model with a low extent error, a large value of adjusted $ R^2 $ indicates a model with a small test error. Additionally, the adjusted $ R^2 $ is equivalent to how much variability is explained by the model. Consequently, the adjusted $ R^2 $ may increase or decrease depending on the fits of new variables added, and one must be cautious of the laws of diminishing returns when adding additional predictors. \n",
    "\n",
    "Nonetheless, the adjusted $ R^2 $ provides a more robust measure than $ C_p $ and BIC in model selection, owing to the fact that the adjusted $ R^2 $ accounts for degrees of freedom in models where fewer predictor variables yield higher estimates; hence, $ AIC $ and BIC can be less robust than the adjusted $ R^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c8d92",
   "metadata": {},
   "source": [
    "Validation and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f8235",
   "metadata": {},
   "source": [
    "\n",
    "As an alternative to the approaches just discussed, we can directly esti\n",
    "mate the test error using the validation set and cross-validation methods\n",
    "discussed in Chapter 5. We can compute the validation set error or the\n",
    "cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest. This pro\n",
    "cedure has an advantage relative to AIC, BIC, Cp, and adjusted R2, in that\n",
    "it provides a direct estimate of the test error, and makes fewer assumptions\n",
    "about the true underlying model. It can also be used in a wider range of\n",
    "model selection tasks, even in cases where it is hard to pinpoint the model\n",
    "degrees of freedom (e.g. the number of predictors in the model) or hard\n",
    "to estimate the error variance 2. Note that when cross-validation is used,\n",
    "the sequence of models Mk in Algorithms 6.1–6.3 is determined separately\n",
    "for each training fold, and the validation errors are averaged over all folds\n",
    "for each model size k. This means, for example with best-subset regression,\n",
    "that Mk, the best subset of size k, can differ across the folds. Once the\n",
    "best size k is chosen, we find the best model of that size on the full data\n",
    "set.\n",
    "In the past, performing cross-validation was computationally prohibitive\n",
    "for many problems with large p and/or large n, and so AIC, BIC, Cp,\n",
    "and adjusted R2 were more attractive approaches for choosing among a\n",
    "set of models. However, nowadays with fast computers, the computations\n",
    "required to perform cross-validation are hardly ever an issue. Thus, cross\n",
    "validation is a very attractive approach for selecting from among a number\n",
    "of models under consideration.\n",
    "Figure 6.3 displays, as a function of d, the BIC, validation set errors, and\n",
    "cross-validation errors on the Credit data, for the best d-variable model.\n",
    "The validation errors were calculated by randomly selecting three-quarters\n",
    "of the observations as the training set, and the remainder as the valida\n",
    "tion set. The cross-validation errors were computed using k = 10 folds.\n",
    "In this case, the validation and cross-validation methods both result in a\n",
    "six-variable model. However, all three approaches suggest that the four-,\n",
    "f\n",
    "ive-, and six-variable models are roughly equivalent in terms of their test\n",
    "errors.\n",
    "In fact, the estimated test error curves displayed in the center and right\n",
    "hand panels of Figure 6.3 are quite flat. While a three-variable model clearly\n",
    "has lower estimated test error than a two-variable model, the estimated test\n",
    "errors of the 3- to 11-variable models are quite similar. Furthermore, if we repeated the validation set approach using a different split of the data into\n",
    "a training set and a validation set, or if we repeated cross-validation using\n",
    "a different set of cross-validation folds, thentheprecisemodelwiththe\n",
    "lowest estimatedtest errorwouldsurelychange. Inthis setting,we can\n",
    "select amodel using theone-standard-error rule.Wefirst calculate the \n",
    "standarderrorof theestimatedtestMSEfor eachmodel size, andthen\n",
    "selectthesmallestmodel forwhichtheestimatedtesterror iswithinone\n",
    "standarderrorofthelowestpointonthecurve.Therationalehereisthat\n",
    "ifasetofmodelsappeartobemoreor lessequallygood, thenwemight\n",
    "aswell choose the simplestmodel—that is, themodelwiththe smallest\n",
    "number of predictors. Inthis case, applying theone-standard-error rule\n",
    "tothevalidationsetorcross-validationapproachleadstoselectionof the\n",
    "three-variablemodel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c8f6a",
   "metadata": {},
   "source": [
    "#### Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d169a73",
   "metadata": {},
   "source": [
    "The subset selection methods described in Section 6.1 involve using least squares to fit a linear model that contains as many predictors. As an alternative, we can find a model containing all predictors and then calibrate or regularize the coefficient estimates, or equivalently, shrink the estimates toward zero. It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficients can significantly reduce their variance. The two best-known techniques for shrinking the coefficient estimates towards zero are ridge regression and the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861857a",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf6e54",
   "metadata": {},
   "source": [
    "Recall from Chapter 3 that the least squares fitting procedure for $ \\beta_0, \\beta_1, \\ldots, \\beta_p $ using the values that minimize\n",
    "\n",
    "$$\n",
    "RSS = \\sum (y_i - \\beta_0 - \\beta_1 x_{1i} - \\ldots - \\beta_p x_{pi})^2\n",
    "$$\n",
    "\n",
    "Ridge regression is simply a variation of this, where the estimator is determined by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates $ \\hat{\\beta}^R $ are those that minimize\n",
    "\n",
    "$$\n",
    "RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "where $ \\lambda > 0 $ is a tuning parameter, to be determined separately. Expressing $ \\lambda $ trades off too much variance in estimates; ridge regression seeks coefficient estimates that fit the data well, by making them smaller. The second term, $ \\lambda \\sum_{j=1}^p \\beta_j^2 $, penalizes large values of the coefficients, thus shrinking the estimates of $ \\beta_j $ towards zero. The tuning parameter $ \\lambda $ serves to control"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
