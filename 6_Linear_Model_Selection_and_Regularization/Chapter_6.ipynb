{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d02f63a",
   "metadata": {},
   "source": [
    "### Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38c5e2",
   "metadata": {},
   "source": [
    "In the regression setting, the standard linear model\n",
    "Y = 0+ 1X1+···+ pXp+\n",
    "(6.1)\n",
    "is commonly used to describe the relationship between a response Y and\n",
    "a set of variables X1,X2,...,Xp. We have seen in Chapter 3 that one\n",
    "typically fits this model using least squares.\n",
    "In the chapters that follow, we consider some approaches for extending\n",
    "the linear model framework. In Chapter 7 we generalize (6.1) in order to\n",
    "accommodate non-linear, but still additive, relationships, while in Chap\n",
    "ters 8 and 10 we consider even more general non-linear models. However,\n",
    "the linear model has distinct advantages in terms of inference and, on real\n",
    "world problems, is often surprisingly competitive in relation to non-linear\n",
    "methods. Hence, before moving to the non-linear world, we discuss in this\n",
    "chapter some ways in which the simple linear model can be improved, by re\n",
    "placing plain least squares fitting with some alternative fitting procedures.\n",
    "Why might we want to use another fitting procedure instead of least\n",
    "squares? As we will see, alternative fitting procedures can yield better pre\n",
    "diction accuracy and model interpretability.\n",
    "\n",
    "• Prediction Accuracy: Provided that the true relationship between the\n",
    "response and the predictors is approximately linear, the least squares\n",
    "estimates will have low bias. If n \n",
    "p—that is, if n, the number of\n",
    "observations, is much larger than p, the number of variables—then the\n",
    "least squares estimates tend to also have low variance, and hence will\n",
    "perform well on test observations. However, if n is not much larger\n",
    "than p, then there can be a lot of variability in the least squares fit,\n",
    "resulting in overfitting and consequently poor predictions on future\n",
    "observations not used in model training. And if p>n, then there is no\n",
    "longer a unique least squares coefficient estimate: there are infinitely many solutions. Each of these least squares solutions gives zero error\n",
    "on the training data, but typically very poor test set performance\n",
    "due to extremely high variance.1 By constraining or shrinking the\n",
    "estimated coefficients, we can often substantially reduce the variance\n",
    "at the cost of a negligible increase in bias. This can lead to substantial\n",
    "improvements in the accuracy with which we can predict the response\n",
    "for observations not used in model training.\n",
    "\n",
    "• Model Interpretability: It is often the case that some or many of the\n",
    "variables used in a multiple regression model are in fact not associ\n",
    "ated with the response. Including such irrelevant variables leads to\n",
    "unnecessary complexity in the resulting model. By removing these\n",
    "variables—that is, by setting the corresponding coefficient estimates\n",
    "to zero—we can obtain a model that is more easily interpreted. Now\n",
    "least squares is extremely unlikely to yield any coefficient estimates\n",
    "that are exactly zero. In this chapter, we see some approaches for au\n",
    "tomatically performing feature selection or variable selection—that is, feature\n",
    "for excluding irrelevant variables from a multiple regression model.\n",
    "\n",
    "There are many alternatives, both classical and modern, to using least\n",
    "squares to fit (6.1). In this chapter, we discuss three important classes of\n",
    "methods.\n",
    "\n",
    "• Subset Selection. This approach involves identifying a subset of the p\n",
    "predictors that we believe to be related to the response. We then fit\n",
    "a model using least squares on the reduced set of variables.\n",
    "\n",
    "• Shrinkage. This approach involves fitting a model involving all p pre\n",
    "dictors. However, the estimated coefficients are shrunken towards zero\n",
    "relative to the least squares estimates. This shrinkage (also known as\n",
    "regularization) has the effect of reducing variance. Depending on what\n",
    "type of shrinkage is performed, some of the coefficients may be esti\n",
    "mated to be exactly zero. Hence, shrinkage methods can also perform\n",
    "variable selection.\n",
    "\n",
    "• Dimension Reduction. This approach involves projecting the p predic\n",
    "tors into an M-dimensional subspace, where M<p.This is achieved\n",
    "by computing M different linear combinations, or projections, of the\n",
    "variables. Then these M projections are used as predictors to fit a\n",
    "linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96fdead",
   "metadata": {},
   "source": [
    "#### Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d046b",
   "metadata": {},
   "source": [
    "##### Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac9018",
   "metadata": {},
   "source": [
    "To perform *best subset selection*, we fit a separate least squares regression for each of the models of the p predictors. That is, we fit all p models that contain exactly one predictor, all $ \\binom{p}{2} $ models that contain exactly two predictors, and so on. We look at all of the resulting models, with the goal of identifying the one that is the best.\n",
    "\n",
    "The problem of selecting the best model from among the $ 2^p $ possibilities considered by best subset selection is then usually broken up into two stages, as described in Algorithm 6.1.\n",
    "\n",
    "##### **Algorithm 6.1** *Best subset selection*\n",
    "\n",
    "1. Let $ M_0 $ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\n",
    "\n",
    "2. For $ k = 1, 2, \\ldots, p $:\n",
    "\n",
    "   (a) Fit all $ \\binom{p}{k} $ models that contain exactly k predictors.\n",
    "   \n",
    "   (b) Pick the best among these models, and call it $ M_k $. Here “best” is defined as having the smallest RSS, or equivalently $ R^2 $.\n",
    "\n",
    "3. Select a single best model from among $ M_0, \\ldots, M_p $ using the prediction error on a validation set, $ C_p $ (AIC), BIC, or adjusted $ R^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33b0f1",
   "metadata": {},
   "source": [
    "In Algorithm 6.1, Step 2 identifies the best model (on the training data)\n",
    "for each subset size, in order to reduce the problem from one of 2p possible\n",
    "models to one of p +1possible models. In Figure 6.1, these models form\n",
    "the lower frontier depicted in red.\n",
    "Now in order to select a single best model, we must simply choose among\n",
    "these p +1options. This task must be performed with care, because the\n",
    "RSS of these p +1models decreases monotonically, and the R2 increases\n",
    "monotonically, as the number of features included in the models increases.\n",
    "Therefore, if we use these statistics to select the best model, then we will\n",
    "always end up with a model involving all of the variables. The problem is\n",
    "that a low RSS or a high R2 indicates a model with a low training error,\n",
    "whereas we wish to choose a model that has a low test error. (As shown in\n",
    "Chapter 2 in Figures 2.9–2.11, training error tends to be quite a bit smaller\n",
    "than test error, and a low training error by no means guarantees a low test\n",
    "error.) Therefore, in Step 3, we use the error on a validation set, Cp, BIC, or\n",
    "adjusted R2 in order to select among M0,M1,...,Mp. If cross-validation\n",
    "is used to select the best model, then Step 2 is repeated on each training\n",
    "fold, and the validation errors are averaged to select the best value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd273c",
   "metadata": {},
   "source": [
    "Then the model Mk fit on the full training set is delivered for the chosen\n",
    "k. These approaches are discussed in Section 6.1.3.\n",
    "An application of best subset selection is shown in Figure 6.1. Each\n",
    "plotted point corresponds to a least squares regression model fit using a\n",
    "different subset of the 10 predictors in the Credit data set, discussed in\n",
    "Chapter 3. Here the variable region is a three-level qualitative variable,\n",
    "and so is represented by two dummy variables, which are selected sepa\n",
    "rately in this case. Hence, there are a total of 11 possible variables which\n",
    "can be included in the model. We have plotted the RSS and R2 statistics\n",
    "for each model, as a function of the number of variables. The red curves\n",
    "connect the best models for each model size, according to RSS or R2. The\n",
    "f\n",
    "igure shows that, as expected, these quantities improve as the number of\n",
    "variables increases; however, from the three-variable model on, there is little\n",
    "improvement in RSS and R2 as a result of including additional predictors.\n",
    "Although we have presented best subset selection here for least squares\n",
    "regression, the same ideas apply to other types of models, such as logistic\n",
    "regression. In the case of logistic regression, instead of ordering models by\n",
    "RSS in Step 2 of Algorithm 6.1, we instead use the deviance, a measure deviance\n",
    "that plays the role of RSS for a broader class of models. The deviance is\n",
    "negative two times the maximized log-likelihood; the smaller the deviance,\n",
    "the better the fit.\n",
    "While best subset selection is a simple and conceptually appealing ap\n",
    "proach, it suffers from computational limitations. The number of possible\n",
    "models that must be considered grows rapidly as p increases. In general,\n",
    "there are 2p models that involve subsets of p predictors. So if p = 10,\n",
    "then there are approximately 1,000 possible models to be considered, and if\n",
    "p =20,then there are over one million possibilities! Consequently, best sub\n",
    "set selection becomes computationally infeasible for values of p greater than around 40, even with extremely fast modern computers. There are compu\n",
    "tational shortcuts—so called branch-and-bound techniques—for eliminat\n",
    "ing some choices, but these have their limitations as p gets large. They also\n",
    "only work for least squares linear regression. We present computationally\n",
    "efficient alternatives to best subset selection next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097842b",
   "metadata": {},
   "source": [
    "##### **Algorithm 6.2** *Forward stepwise selection*\n",
    "\n",
    "1. Let $ M_0 $ denote the null model, which contains no predictors.\n",
    "\n",
    "2. For $ k = 0, \\ldots, p - 1 $:\n",
    "\n",
    "   - (a) Consider all $ p - k $ models that augment the predictors in $ M_k $ with one additional predictor.\n",
    "   \n",
    "   - (b) Choose the best among these $ p - k $ models, and call it $ M_{k+1} $. Here best is defined as having the smallest RSS or highest $ R^2 $.\n",
    "\n",
    "3. Select a single best model from among $ M_0, \\ldots, M_p $ using the prediction error on a validation set, $ C_p $ (AIC), BIC, or adjusted $ R^2 $. Or use the cross-validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f255d",
   "metadata": {},
   "source": [
    "##### Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc158be",
   "metadata": {},
   "source": [
    "For computational reasons, best subset selection cannot be applied with\n",
    "very large p. Best subset selection may also suffer from statistical problems\n",
    "when p is large. The larger the search space, the higher the chance of finding\n",
    "models that look good on the training data, even though they might not\n",
    "have any predictive power on future data. Thus an enormous search space\n",
    "can lead to overfitting and high variance of the coefficient estimates.\n",
    "For both of these reasons, stepwise methods, which explore a far more\n",
    "restricted set of models, are attractive alternatives to best subset selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679aab05",
   "metadata": {},
   "source": [
    "Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36191d1",
   "metadata": {},
   "source": [
    "*Forward stepwise selection* is a computationally efficient alternative to best subset selection. While the best subset selection procedure considers all\n",
    "2p possible models containing subsets of the p predictors, forward step\n",
    "wise considers a much smaller set of models. Forward stepwise selection\n",
    "begins with a model containing no predictors, and then adds predictors\n",
    "to the model, one-at-a-time, until all of the predictors are in the model.\n",
    "In particular, at each step the variable that gives the greatest additional\n",
    "improvement to the fit is added to the model. More formally, the forward\n",
    "stepwise selection procedure is given in Algorithm 6.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a161f3",
   "metadata": {},
   "source": [
    "## 6. Linear Model Selection and Regularization\n",
    "\n",
    "Unlike best subset selection, which involved fitting $ 2^p $ models, forward stepwise selection involves fitting one null model, along with $ p - k $ models in the $ k $-th iteration, for $ k = 0, \\ldots, p - 1 $. This amounts to a total of $ 1 + \\sum_{i=0}^{p-1} (p - i) = 1 + p(p + 1)/2 $ models. This is a substantial difference: when $p$ = 20, best subset selection requires fitting $ 1,048,576 $ models, whereas forward stepwise selection requires fitting only $ 21 $ models. \n",
    "\n",
    "In step 2(b) of Algorithm 6.2, we identify the best model from those available, which augment $ M_k $ with one additional predictor. We do this by simply choosing the model with the lowest RSS or the highest $ R^2 $. In this case, we must identify the best model from a set of models with different numbers of variables. This is more challenging, and is discussed in Section 6.1.3.\n",
    "\n",
    "Forward stepwise selection's computational advantage over best subset selection is clear. Though forward stepwise seems to work well in practice, it is important to keep in mind that it is fundamentally a greedy algorithm. In a data set with $ p $ predictors, the best possible model will include predictors $ X_2 $ and $ X_1 $. However, forward stepwise selection is still unable to find the best possible model among $ M_1, M_2, $ and those available with $ X_1 $ together with an additional variable.\n",
    "\n",
    "As shown in Section 6.1.3, the forward stepwise selection on the Credit data set illustrates this phenomenon. Both forward stepwise selection and best subset selection favored models that included the predictors rating and income, whereas the best subset selection also included the variable student.\n",
    "\n",
    "In high-dimensional settings where $ p $ is greater than $ n $, forward stepwise selection can still be applied even when subset selection cannot. If $ p $ is greater than $ n $, each time an additional variable is included, only the subset of $ M_0, \\ldots, M_k $ can be constructed, which avoids overfitting, as each submodel is fit using least squares, which does not yield a unique solution if $ p > n $.\n",
    "\n",
    "\n",
    "\n",
    "| # Variables | Best subset                      | Forward stepwise                  |\n",
    "|-------------|----------------------------------|-----------------------------------|\n",
    "| One         | `rating`                           | `rating`                            |\n",
    "| Two         | `rating`, `income`                   | `rating`, `income`                    |\n",
    "| Three       | `rating`, `income`, `student`              | `rating`, `income`, `student`               |\n",
    "| Four        | `cards`, `income`, `student`, `limit` | `rating`, `income`, `student`, `limit` |\n",
    "\n",
    "The first four selected models for best subset selection and forward stepwise selection on the Credit data set. The first three models are identical but the fourth models differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33c838",
   "metadata": {},
   "source": [
    "Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bc443",
   "metadata": {},
   "source": [
    "Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward step\n",
    "wise selection, it begins with the full least squares model containing all p\n",
    "predictors, and then iteratively removes the least useful predictor, one-at\n",
    "a-time. Details are given in Algorithm 6.3.\n",
    "\n",
    "\n",
    "Like forward stepwise selection, the backward selection approach searches\n",
    "through only 1+p(p+1)/2 models, and so can be applied in settings where\n",
    "p is too large to apply best subset selection.3 Also like forward stepwise\n",
    "selection, backward stepwise selection is not guaranteed to yield the best\n",
    "model containing a subset of the p predictors.\n",
    "Backward selection requires that the number of samples n is larger than\n",
    "the number of variables p (so that the full model can be fit). In contrast,\n",
    "forward stepwise can be used even when n<p, and so is the only viable\n",
    "subset method when p is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df46a",
   "metadata": {},
   "source": [
    "Hybrid Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a7e4e",
   "metadata": {},
   "source": [
    "The best subset, forward stepwise, and backward stepwise selection ap\n",
    "proaches generally give similar but not identical models. As another al\n",
    "ternative, hybrid versions of forward and backward stepwise selection are\n",
    "available, in which variables are added to the model sequentially, in analogy\n",
    "to forward selection. However, after adding each new variable, the method\n",
    "may also remove any variables that no longer provide an improvement in\n",
    "the model fit. Such an approach attempts to more closely mimic best sub\n",
    "set selection while retaining the computational advantages of forward and\n",
    "backward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b066da3",
   "metadata": {},
   "source": [
    "##### Choosing the Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df32254",
   "metadata": {},
   "source": [
    "Best subset selection, forward selection, and backward selection result in\n",
    "the creation of a set of models, each of which contains a subset of the p predictors. To apply these methods, we need a way to determine which of\n",
    "these models is best. As we discussed in Section 6.1.1, the model containing\n",
    "all of the predictors will always have the smallest RSS and the largest R2,\n",
    "since these quantities are related to the training error. Instead, we wish to\n",
    "choose a model with a low test error. As is evident here, and as we show\n",
    "in Chapter 2, the training error can be a poor estimate of the test error.\n",
    "Therefore, RSS and R2 are not suitable for selecting the best model among\n",
    "a collection of models with different numbers of predictors.\n",
    "In order to select the best model with respect to test error, we need to\n",
    "estimate this test error. There are two common approaches:\n",
    "\n",
    "1. We can indirectly estimate test error by making an adjustment to the\n",
    "training error to account for the bias due to overfitting.\n",
    "\n",
    "\n",
    "2. We can directly estimate the test error, using either a validation set\n",
    "approach or a cross-validation approach, as discussed in Chapter 5.\n",
    "\n",
    "We consider both of these approaches below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f37cc",
   "metadata": {},
   "source": [
    "We show in Chapter 2 that the training set $ R^2 $ is generally an under-estimate of the test MSE. (Recall that MSE = RSS/n.) In training RSS, but when we fit a model to the training data using least squares, we (not the test RSS) is as small as possible, this can sometimes misestimate the regression coefficients in a model. Thus, we need to choose from among a set of models with different variables.\n",
    "\n",
    "However, a number of techniques for adjusting the training error can lead to a set of models with different variables. We introduce the following metrics for model selection criteria: \n",
    "\n",
    "- **Akaike information criterion (AIC)**, \n",
    "- **Bayesian information criterion (BIC)**, and \n",
    "- **Adjusted $ R^2 $**. \n",
    "\n",
    "Figure 6.2 gives a comparison of the best model selection criteria and selects the best subset on the Credit data set.\n",
    "\n",
    "For a fitted least squares model containing $ p $ predictors, the $ C_p $ estimate of test MSE is computed using the equation:\n",
    "\n",
    "$$\n",
    "C_p = \\frac{(RSS + 2d\\hat{\\sigma}^2)}{n}\n",
    "$$\n",
    "\n",
    "where $ \\hat{\\sigma}^2 $ is an estimate of the variance of the error $ \\epsilon $ associated with the regression model in (6.1). Typically $ d $ is chosen to be $ p $ or the number of parameters in the model. The $ C_p $ statistic adds a penalty term involving $ d $ in order to adjust for the number of predictors  in the model increases; this is intended to adjust for the corresponding decrease in training RSS. Based on the scope of this book, one can show that if $ \\hat{\\sigma}^2 $ in (6.2), then $ C_p $ is an unbiased estimate of test MSE. As a consequence, the $ C_p $ statistic tends to take a larger value in the model selected with the lowest $ C_p $ value. In Figure 6.2, C_p selects the six-variable model containing the predictors income, lint, age, and risk.\n",
    "\n",
    "The AIC criterion is defined as follows:\n",
    "$$\n",
    "AIC = \\frac{1}{n} (RSS + 2d\\hat{\\sigma}^2)\n",
    "$$\n",
    "where, for simplicity, we have omitted irrelevant constants. Here for least squares models, AIC and $ C_p $ are proportional to each other, as shown in Figure 6.2.\n",
    "\n",
    "BIC is derived from a Bayesian point of view; it ends up looking similar to $ C_p $ and $ AIC $ for the least squares model but with a different penalty term:\n",
    "$$\n",
    "BIC = \\frac{1}{n} (RSS + \\log(n)d\\hat{\\sigma}^2)\n",
    "$$\n",
    "Like $ C_p $, the BIC also penalizes a more complex model for a loss in precision, and generally we select the model with the lowest BIC. Notice that the BIC penalty includes the $ 2d\\ \\log(n) $ term, where $ n $ is the number of observations. Since $ \\log n > 2 $ for any $ n > 7 $, the BIC statistic generally places a heavier penalty on models with many variables, and hence reveals in the selection of smaller models than $ C_p $. In Figure 6.2, we see that the direct task decider is a single model that contains only the four predictors income, lint, age, and assertion. In this case the chosen model does not appear to make much difference in accuracy between the four-variable and six-variable models.\n",
    "\n",
    "The adjusted $ R^2 $ statistic is another popular approach for selecting among a set of models that contain different numbers of variables. Recall from Chapter 3 that the adjusted $ R^2 $ is defined as follows, where $ RSS = \\sum(y_i - \\hat{y})^2 $ is the total sum of squared response. Since $ RSS $ increases as more variables are added to the model, the $ R^2 $ measure becomes biased toward a larger selected model of variables, the adjusted $ R^2 $ is calculated as\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n - 1)}\n",
    "$$\n",
    "\n",
    "Unlike $ C_p $ and BIC, for which a small value indicates a model with a low extent error, a large value of adjusted $ R^2 $ indicates a model with a small test error. Additionally, the adjusted $ R^2 $ is equivalent to how much variability is explained by the model. Consequently, the adjusted $ R^2 $ may increase or decrease depending on the fits of new variables added, and one must be cautious of the laws of diminishing returns when adding additional predictors. \n",
    "\n",
    "Nonetheless, the adjusted $ R^2 $ provides a more robust measure than $ C_p $ and BIC in model selection, owing to the fact that the adjusted $ R^2 $ accounts for degrees of freedom in models where fewer predictor variables yield higher estimates; hence, $ AIC $ and BIC can be less robust than the adjusted $ R^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c8d92",
   "metadata": {},
   "source": [
    "Validation and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f8235",
   "metadata": {},
   "source": [
    "\n",
    "As an alternative to the approaches just discussed, we can directly esti\n",
    "mate the test error using the validation set and cross-validation methods\n",
    "discussed in Chapter 5. We can compute the validation set error or the\n",
    "cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest. This pro\n",
    "cedure has an advantage relative to AIC, BIC, Cp, and adjusted R2, in that\n",
    "it provides a direct estimate of the test error, and makes fewer assumptions\n",
    "about the true underlying model. It can also be used in a wider range of\n",
    "model selection tasks, even in cases where it is hard to pinpoint the model\n",
    "degrees of freedom (e.g. the number of predictors in the model) or hard\n",
    "to estimate the error variance 2. Note that when cross-validation is used,\n",
    "the sequence of models Mk in Algorithms 6.1–6.3 is determined separately\n",
    "for each training fold, and the validation errors are averaged over all folds\n",
    "for each model size k. This means, for example with best-subset regression,\n",
    "that Mk, the best subset of size k, can differ across the folds. Once the\n",
    "best size k is chosen, we find the best model of that size on the full data\n",
    "set.\n",
    "In the past, performing cross-validation was computationally prohibitive\n",
    "for many problems with large p and/or large n, and so AIC, BIC, Cp,\n",
    "and adjusted R2 were more attractive approaches for choosing among a\n",
    "set of models. However, nowadays with fast computers, the computations\n",
    "required to perform cross-validation are hardly ever an issue. Thus, cross\n",
    "validation is a very attractive approach for selecting from among a number\n",
    "of models under consideration.\n",
    "Figure 6.3 displays, as a function of d, the BIC, validation set errors, and\n",
    "cross-validation errors on the Credit data, for the best d-variable model.\n",
    "The validation errors were calculated by randomly selecting three-quarters\n",
    "of the observations as the training set, and the remainder as the valida\n",
    "tion set. The cross-validation errors were computed using k = 10 folds.\n",
    "In this case, the validation and cross-validation methods both result in a\n",
    "six-variable model. However, all three approaches suggest that the four-,\n",
    "f\n",
    "ive-, and six-variable models are roughly equivalent in terms of their test\n",
    "errors.\n",
    "In fact, the estimated test error curves displayed in the center and right\n",
    "hand panels of Figure 6.3 are quite flat. While a three-variable model clearly\n",
    "has lower estimated test error than a two-variable model, the estimated test\n",
    "errors of the 3- to 11-variable models are quite similar. Furthermore, if we repeated the validation set approach using a different split of the data into\n",
    "a training set and a validation set, or if we repeated cross-validation using\n",
    "a different set of cross-validation folds, thentheprecisemodelwiththe\n",
    "lowest estimatedtest errorwouldsurelychange. Inthis setting,we can\n",
    "select amodel using theone-standard-error rule.Wefirst calculate the \n",
    "standarderrorof theestimatedtestMSEfor eachmodel size, andthen\n",
    "selectthesmallestmodel forwhichtheestimatedtesterror iswithinone\n",
    "standarderrorofthelowestpointonthecurve.Therationalehereisthat\n",
    "ifasetofmodelsappeartobemoreor lessequallygood, thenwemight\n",
    "aswell choose the simplestmodel—that is, themodelwiththe smallest\n",
    "number of predictors. Inthis case, applying theone-standard-error rule\n",
    "tothevalidationsetorcross-validationapproachleadstoselectionof the\n",
    "three-variablemodel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c8f6a",
   "metadata": {},
   "source": [
    "#### Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d169a73",
   "metadata": {},
   "source": [
    "The subset selection methods described in Section 6.1 involve using least squares to fit a linear model that contains as many predictors. As an alternative, we can find a model containing all predictors and then calibrate or regularize the coefficient estimates, or equivalently, shrink the estimates toward zero. It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficients can significantly reduce their variance. The two best-known techniques for shrinking the coefficient estimates towards zero are ridge regression and the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861857a",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf6e54",
   "metadata": {},
   "source": [
    "Recall from Chapter 3 that the least squares fitting procedure for $ \\beta_0, \\beta_1, \\ldots, \\beta_p $ using the values that minimize\n",
    "\n",
    "$$\n",
    "RSS = \\sum (y_i - \\beta_0 - \\beta_1 x_{1i} - \\ldots - \\beta_p x_{pi})^2\n",
    "$$\n",
    "\n",
    "Ridge regression is simply a variation of this, where the estimator is determined by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates $ \\hat{\\beta}_P $ are those that minimize\n",
    "\n",
    "$$\n",
    "RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "where $ \\lambda > 0 $ is a tuning parameter, to be determined separately. Expressing $ \\lambda $ trades off too much variance in estimates; ridge regression seeks coefficient estimates that fit the data well, by making them smaller. The second term, $ \\lambda \\sum_{j=1}^p \\beta_j^2 $, penalizes large values of the coefficients, thus shrinking the estimates of $ \\beta_j $ towards zero. The tuning parameter $ \\lambda $ serves to control the relative impact of these two terms on the regression coefficient esti\n",
    "mates. When =0, the penalty term has no effect, and ridge regression\n",
    "will produce the least squares estimates. However, as \n",
    ", the impact of\n",
    "the shrinkage penalty grows, and the ridge regression coefficient estimates\n",
    "will approach zero. Unlike least squares, which generates only one set of co\n",
    "efficient estimates, ridge regression will produce a different set of coefficient\n",
    "estimates, ˆR, for each value of . Selecting a good value for is critical;\n",
    "we defer this discussion to Section 6.2.3, where we use cross-validation.\n",
    "Note that in (6.5), the shrinkage penalty is applied to 1,..., p, but\n",
    "not to the intercept 0. We want to shrink the estimated association of\n",
    "each variable with the response; however, we do not want to shrink the\n",
    "intercept, which is simply a measure of the mean value of the response\n",
    "when xi1 = xi2 = ...= xip =0. If we assume that the variables—that is,\n",
    "the columns of the data matrix X—have been centered to have mean zero\n",
    "before ridge regression is performed, then the estimated intercept will take\n",
    "the form ˆ0 =¯y= n\n",
    "i=1yi/n.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bd596",
   "metadata": {},
   "source": [
    "An Application to the Credit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690562a",
   "metadata": {},
   "source": [
    "In Figure 6.4, the ridge regression coefficient estimates for the Credit data\n",
    "set are displayed. In the left-hand panel, each curve corresponds to the\n",
    "ridge regression coefficient estimate for one of the ten variables, plotted\n",
    "as a function of . For example, the black solid line represents the ridge\n",
    "regression estimate for the income coefficient, as is varied. At the extreme\n",
    "left-hand side of the plot, is essentially zero, and so the corresponding\n",
    "ridge coefficient estimates are the same as the usual least squares esti\n",
    "mates. But as increases, the ridge coefficient estimates shrink towards\n",
    "zero. When is extremely large, then all of the ridge coefficient estimates\n",
    "are basically zero; this corresponds to the null model that contains no predictors. In this plot, the income, limit, rating, and student variables are\n",
    "displayed in distinct colors, since these variables tend to have by far the\n",
    "largest coefficient estimates. While the ridge coefficient estimates tend to\n",
    "decrease in aggregate as increases, individual coefficients, such as rating\n",
    "and income, may occasionally increase as increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ff260",
   "metadata": {},
   "source": [
    "The right-hand panel of Figure 6.4 displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying on the x-axis, we now display $\\hat{R}^2/\\hat{2}$, where $\\hat{}$ denotes the vector of least squares coefficient estimates. The notation $\\|\\cdot\\|_2$ denotes the 2 norm (pronounced “ell 2”) of a vector, and is defined as $\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^{p} \\beta_j^2}$. It measures the distance of $\\beta$ from zero. As $\\lambda$ increases, the 2 norm of $\\hat{R}$ will always decrease, and so will $\\hat{R}^2/\\hat{2}$. The latter quantity ranges from 1 (when $\\lambda=0$, in which case the ridge regression coefficient estimate is the same as the least squares estimate, and so their 2 norms are the same) to 0 (when $\\lambda = \\infty$, in which case the ridge regression coefficient estimate is a vector of zeros, with 2 norm equal to zero). Therefore, we can think of the x-axis in the right-hand panel of Figure 6.4 as the amount that the ridge regression coefficient estimates have been shrunken towards zero; a small value indicates that they have been shrunken very close to zero.\n",
    "\n",
    "The standard least squares coefficient estimates discussed in Chapter 3 are scale equivariant: multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. In other words, regardless of how the jth predictor is scaled, $X_j \\hat{\\beta}_j$ will remain the same. In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. For instance, consider the income variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result in a reduction in the observed values of income by a factor of 1,000. Now due to the sum of squared coefficients term in the ridge regression formulation (6.5), such a change in scale will not simply cause the ridge regression coefficient estimate for income to change by a factor of 1,000. In other words, $X_j \\hat{R}_j$ will depend not only on the value of $\\lambda$, but also on the scaling of the jth predictor. In fact, the value of $X_j \\hat{R}_j$ may even depend on the scaling of the other predictors! Therefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n",
    "\n",
    "$$\n",
    "\\tilde{x}_{ij} = \\frac{x_{ij}}{1 / n \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}\n",
    "$$\n",
    "\n",
    "so that they are all on the same scale. In (6.6), the denominator is the estimated standard deviation of the jth predictor. Consequently, all of the standardized predictors will have a standard deviation of one. As a result, the final fit will not depend on the scale on which the predictors are measured. In Figure 6.4, the y-axis displays the standardized ridge regression coefficient estimates—that is, the coefficient estimates that result from performing ridge regression using standardized predictors.\n",
    "\n",
    "Why Does Ridge Regression Improve Over Least Squares?\n",
    "\n",
    "Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. This is illustrated in the left-hand panel of Figure 6.5, using a simulated data set containing $p = 45$ predictors and $n = 50$ observations. The green curve in the left-hand panel of Figure 6.5 displays the variance of the ridge regression predictions as a function of $\\lambda$. At the least squares coefficient estimates, which correspond to ridge regression with $\\lambda = 0$, the variance is high but there is no bias. But as $\\lambda$ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. Recall that the least mean squared error (MSE), plotted in bias-variance bias, is closely related to the variance plus the squared bias. For values of $\\lambda$ that are not too small, the variance generally remains very low, as shown in the figure, plotted in black. However, as $\\lambda$ increases from 0 to 10. Beyond this point, the decrease in variance is no longer sufficient to offset the increased bias, and the MSE can begin to be significantly underestimated, resulting in a large increase in the bias. \n",
    "\n",
    "The minimum MSE is achieved at and around the value of $\\lambda$ that results in the smallest MSE associated with the least squares fit, when using the same hyperparameter that will yield the best fit for any model designed for use with $\\lambda$. However, for an inflexible estimator, the MSE is considerably higher.\n",
    "\n",
    "In general, as the number of observations increases, the ridge regression estimates become more stable against the errors in the left-hand curve; however, the fitted values may still have high variance. This means that while ridge regression can improve the fitted values' stability by controlling their variance, the coefficients of the variables in the model may be increasingly biased.\n",
    "\n",
    "In Figure 6.5, the least squares estimates continue to outperform ridge regression predictions, even when $\\lambda > 0$, because the least squares estimation can still perform well by trading off a small increase in bias for a large decrease in variance. Hence, ridge regression works best in situations\n",
    "where the least squares estimates have high variance.\n",
    "Ridge regression also has substantial computational advantages over best\n",
    "subset selection, which requires searching through $2^p$ models. As we dis\n",
    "cussed previously, even for moderate values of $p$, such a search can be\n",
    "computationally infeasible. In contrast, for any fixed value of $\\lambda$, ridge re\n",
    "gression only fits a single model, and the model-fitting procedure can be\n",
    "performed quite quickly. In fact, one can show that the computations re\n",
    "quired to solve (6.5), simultaneously for all values of $\\lambda$, are almost identical\n",
    "to those for fitting a model using least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ade56",
   "metadata": {},
   "source": [
    "#### The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21238e86",
   "metadata": {},
   "source": [
    "Ridge regression does have one obvious disadvantage. Unlike best subset,\n",
    "forward stepwise, and backward stepwise selection, which will generally\n",
    "select models that involve just a subset of the variables, ridge regression\n",
    "will include all $p$ predictors in the final model. The penalty \n",
    "$$\n",
    "\\sum_{j=1}^{p} \\beta_j^2\n",
    "$$ \n",
    "in (6.5) will shrink all of the coefficients towards zero, but it will not set any of them\n",
    "exactly to zero (unless $\\lambda = \\infty$). This may not be a problem for prediction\n",
    "accuracy, but it can create a challenge in model interpretation in settings in\n",
    "which the number of variables $p$ is quite large. For example, in the Credit\n",
    "data set, it appears that the most important variables are income, limit,\n",
    "rating, and student. So we might wish to build a model including just\n",
    "these predictors. However, ridge regression will always generate a model\n",
    "involving all ten predictors. Increasing the value of $\\lambda$ will tend to reduce\n",
    "the magnitudes of the coefficients, but will not result in exclusion of any of\n",
    "the variables.\n",
    "\n",
    "The lasso is a relatively recent alternative to ridge regression that over-\n",
    "comes this disadvantage. The lasso coefficients, $\\hat{\\beta}_L$, minimize the quantity\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p} | \\beta_j | = \\text{RSS} + \\lambda \\sum_{j=1}^{p} | \\beta_j |.\n",
    "$$\n",
    "Comparing (6.7) to (6.5), we see that the lasso and ridge regression have\n",
    "similar formulations. The only difference is that the $\\beta_j^2$ term in the ridge\n",
    "regression penalty (6.5) has been replaced by $|\\beta_j|$ in the lasso penalty (6.7).\n",
    "In statistical parlance, the lasso uses an $L_1$ (pronounced “ell 1”) penalty\n",
    "instead of an $L_2$ penalty. The $L_1$ norm of a coefficient vector is given by\n",
    "$$\n",
    "||\\beta||_1 = \\sum_{j=1}^{p} | \\beta_j |.\n",
    "$$\n",
    "\n",
    "As with ridge regression, the lasso shrinks the coefficient estimates to\n",
    "wards zero. However, in the case of the lasso, the $L_1$ penalty has the effect\n",
    "of forcing some of the coefficient estimates to be exactly equal to zero when\n",
    "the tuning parameter is sufficiently large. Hence, much like best subset se\n",
    "lection, the lasso performs variable selection. As a result, models generated\n",
    "from the lasso are generally much easier to interpret than those produced\n",
    "by ridge regression. We say that the lasso yields sparse models—that is,\n",
    "models that involve only a subset of the variables. As in ridge regression,\n",
    "selecting a good value of $\\lambda$ for the lasso is critical; we defer this discussion\n",
    "to Section 6.2.3, where we use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1b249",
   "metadata": {},
   "source": [
    "As an example, consider the coefficient plots in Figure 6.6, which are gen\n",
    "erated from applying the lasso to the Credit dataset. When $\\lambda = 0$, then\n",
    "the lasso simply gives the least squares fit, and when $\\lambda$ becomes sufficiently\n",
    "large, the lasso gives the null model in which all coefficient estimates equal\n",
    "zero. However, in between these two extremes, the ridge regression and\n",
    "lasso models are quite different from each other. Moving from left to right\n",
    "in the right-hand panel of Figure 6.6, we observe that at first the lasso re\n",
    "sults in a model that contains only the rating predictor. Then student and\n",
    "limit enter the model almost simultaneously, shortly followed by income.\n",
    "Eventually, the remaining variables enter the model. Hence, depending on\n",
    "the value of $\\lambda$, the lasso can produce a model involving any number of vari\n",
    "ables. In contrast, ridge regression will always include all of the variables in\n",
    "the model, although the magnitude of the coefficient estimates will depend\n",
    "on $\\lambda$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6683a9",
   "metadata": {},
   "source": [
    "Another Formulation for Ridge Regression and the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a85bb",
   "metadata": {},
   "source": [
    "One can show that the lasso and ridge regression coefficient estimates solve\n",
    "the problems:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} & \\quad \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2\n",
    "\\text{subject to} & \\quad \\sum_{j=1}^{p} | \\beta_j | \\leq s \\tag{6.8}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} & \\quad \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2\n",
    "\\text{subject to} & \\quad \\sum_{j=1}^{p} \\beta_j^2 \\leq s \\tag{6.9}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "respectively. In other words, for every value of $\\lambda$, there is some $s$ such that\n",
    "the Equations (6.7) and (6.8) will give the same lasso coefficient estimates.\n",
    "Similarly, for every value of $\\lambda$ there is a corresponding $s$ such that Equations (6.5) and (6.9) will give the same ridge regression coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b2589",
   "metadata": {},
   "source": [
    "When $ p = 2 $, then (6.8) indicates that the lasso coefficient estimates have\n",
    "the smallest RSS out of all points that lie within the diamond defined by\n",
    "$| \\beta_1| + | \\beta_2| \\leq s$. Similarly, the ridge regression estimates have the smallest\n",
    "RSS out of all points that lie within the circle defined by $ \\beta_1^2 + \\beta_2^2 \\leq s $.\n",
    "We can think of (6.8) as follows. When we perform the lasso we are trying\n",
    "to find the set of coefficient estimates that lead to the smallest RSS, subject\n",
    "to the constraint that there is a budget $ s $ for how large $ \\sum_{j=1}^{p} | \\beta_j | $ can be.\n",
    "When $ s $ is extremely large, then this budget is not very restrictive, and so\n",
    "the coefficient estimates can be large. In fact, if $ s $ is large enough that the\n",
    "least squares solution falls within the budget, then (6.8) will simply yield\n",
    "the least squares solution. In contrast, if $ s $ is small, then $ \\sum_{j=1}^{p} | \\beta_j | $ must be\n",
    "small in order to avoid violating the budget. Similarly, (6.9) indicates that\n",
    "when we perform ridge regression, we seek a set of coefficient estimates\n",
    "such that the RSS is as small as possible, subject to the requirement that\n",
    "$ \\sum_{j=1}^{p} \\beta_j^2 $ not exceed the budget $ s $.\n",
    "\n",
    "The formulations (6.8) and (6.9) reveal a close connection between the\n",
    "lasso, ridge regression, and best subset selection. Consider the problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} & \\quad \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \n",
    "\\text{subject to} & \\quad \\sum_{j=1}^{p} I( \\beta_j ≠ 0) \\leq s. \\tag{6.10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $ I( \\beta_j ≠ 0) $ is an indicator variable: it takes on a value of 1 if $ \\beta_j = 0 $, and\n",
    "equals zero otherwise. Then (6.10) amounts to finding a set of coefficient\n",
    "estimates such that RSS is as small as possible, subject to the constraint\n",
    "that no more than $ s $ coefficients can be nonzero. The problem (6.10) is\n",
    "equivalent to best subset selection. Unfortunately, solving (6.10) is com\n",
    "putationally infeasible when $ p $ is large, since it requires considering all $ p \\choose s $\n",
    "models containing $ s $ predictors. Therefore, we can interpret ridge regression\n",
    "and the lasso as computationally feasible alternatives to best subset selec\n",
    "tion that replace the intractable form of the budget in (6.10) with forms\n",
    "that are much easier to solve. Of course, the lasso is much more closely\n",
    "related to best subset selection, since the lasso performs feature selection\n",
    "for $ s $ sufficiently small in (6.8), while ridge regression does not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0b030",
   "metadata": {},
   "source": [
    "The Variable Selection Property of the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d7ada",
   "metadata": {},
   "source": [
    "Why is it that the lasso, unlike ridge regression, results in coefficient esti\n",
    "mates that are exactly equal to zero? The formulations (6.8) and (6.9) can\n",
    "be used to shed light on the issue. Figure 6.7 illustrates the situation. The\n",
    "least squares solution is marked as $ \\hat{\\beta} $, while the blue diamond and circle\n",
    "represent the lasso and ridge regression constraints in (6.8) and (6.9), re\n",
    "spectively. If $ s $ is sufficiently large, then the constraint regions will contain\n",
    "$ \\hat{\\beta} $, and so the ridge regression and lasso estimates will be the same as the\n",
    "least squares estimates. (Such a large value of $ s $ corresponds to $ \\lambda = 0 $ in\n",
    "(6.5) and (6.7).) However, in Figure 6.7 the least squares estimates lie out\n",
    "side of the diamond and the circle, and so the least squares estimates are\n",
    "not the same as the lasso and ridge regression estimates.\n",
    "\n",
    "Each of the ellipses centered around $ \\hat{\\beta} $ represents a contour: this means\n",
    "that all of the points on a particular ellipse have the same RSS value. As the ellipses expand away from the least squares coefficient estimates, the\n",
    "RSS increases. Equations (6.8) and (6.9) indicate that the lasso and ridge\n",
    "regression coefficient estimates are given by the first point at which an\n",
    "ellipse contacts the constraint region. Since ridge regression has a circular\n",
    "constraint with no sharp points, this intersection will not generally occur on\n",
    "an axis, and so the ridge regression coefficient estimates will be exclusively\n",
    "non-zero. However, the lasso constraint has corners at each of the axes, and\n",
    "so the ellipse will often intersect the constraint region at an axis. When this\n",
    "occurs, one of the coefficients will equal zero. In higher dimensions, many of\n",
    "the coefficient estimates may equal zero simultaneously. In Figure 6.7, the\n",
    ".\n",
    "intersection occurs at 1 =0, and so the resulting model will only include $ \\beta_2$\n",
    "\n",
    "\n",
    "In Figure 6.7, we considered the simple case of p =2. When p =3,\n",
    "then the constraint region for ridge regression becomes a sphere, and the\n",
    "constraint region for the lasso becomes a polyhedron. When p>3, the\n",
    "constraint for ridge regression becomes a hypersphere, and the constraint\n",
    "for the lasso becomes a polytope. However, the key ideas depicted in Fig\n",
    "ure 6.7 still hold. In particular, the lasso leads to feature selection when\n",
    "p>2 due to the sharp corners of the polyhedron or polytope.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb29395",
   "metadata": {},
   "source": [
    "Comparing the Lasso and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32aaaab",
   "metadata": {},
   "source": [
    "\n",
    "It is clear that the lasso has a major advantage over ridge regression, in\n",
    "that it produces simpler and more interpretable models that involve only a\n",
    "subset of the predictors. However, which method leads to better prediction\n",
    "accuracy? Figure 6.8 displays the variance, squared bias, and test MSE of\n",
    "the lasso applied to the same simulated data as in Figure 6.5. Clearly the\n",
    "lasso leads to qualitatively similar behavior to ridge regression, in that as $ \\lambda $\n",
    "increases, the variance decreases and the bias increases. In the right-hand panel of Figure 6.8, the dotted lines represent the ridge regression fits.\n",
    "Here we plot both against their R2 on the training data. This is another\n",
    "useful way to index models, and can be used to compare models with\n",
    "different types of regularization, as is the case here. In this example, the\n",
    "lasso and ridge regression result in almost identical biases. However, the\n",
    "variance of ridge regression is slightly lower than the variance of the lasso.\n",
    "Consequently, the minimum MSE of ridge regression is slightly smaller than\n",
    "that of the lasso.\n",
    "\n",
    "However, the data in Figure 6.8 were generated in such a way that all 45\n",
    "predictors were related to the response—that is, none of the true coefficients\n",
    "1,..., 45 equaled zero. The lasso implicitly assumes that a number of the\n",
    "coefficients truly equal zero. Consequently, it is not surprising that ridge\n",
    "regression outperforms the lasso in terms of prediction error in this setting.\n",
    "Figure 6.9 illustrates a similar situation, except that now the response is a\n",
    "function of only 2 out of 45 predictors. Now the lasso tends to outperform\n",
    "ridge regression in terms of bias, variance, and MSE.\n",
    "\n",
    "These two examples illustrate that neither ridge regression nor the lasso\n",
    "will universally dominate the other. In general, one might expect the lasso\n",
    "to perform better in a setting where a relatively small number of predictors\n",
    "have substantial coefficients, and the remaining predictors have coefficients\n",
    "that are very small or that equal zero. Ridge regression will perform better\n",
    "when the response is a function of many predictors, all with coefficients of\n",
    "roughly equal size. However, the number of predictors that is related to the\n",
    "response is never known a priori for real data sets. A technique such as\n",
    "cross-validation can be used in order to determine which approach is better\n",
    "on a particular data set.\n",
    "\n",
    "As with ridge regression, when the least squares estimates have exces\n",
    "sively high variance, the lasso solution can yield a reduction in variance\n",
    "at the expense of a small increase in bias, and consequently can gener\n",
    "ate more accurate predictions. Unlike ridge regression, the lasso performs\n",
    "variable selection, and hence results in models that are easier to interpret.\n",
    "There are very efficient algorithms for fitting both ridge and lasso models;\n",
    "in both cases the entire coefficient paths can be computed with about the\n",
    "same amount of work as a single least squares fit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da35da",
   "metadata": {},
   "source": [
    "A Simple Special Case for Ridge Regression and the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85b585",
   "metadata": {},
   "source": [
    "In order to obtain a better intuition about the behavior of ridge regression\n",
    "and the lasso, consider a simple special case with $ n = p $, and $ X $ a diag\n",
    "onal matrix with 1’s on the diagonal and 0’s in all off-diagonal elements.\n",
    "To simplify the problem further, assume also that we are performing regres\n",
    "sion without an intercept. With these assumptions, the usual least squares\n",
    "problem simplifies to finding $ \\beta_1, \\ldots, \\beta_p $ that minimize\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{p} (y_j - \\beta_j)^2.\n",
    "$$\n",
    "\n",
    "In this case, the least squares solution is given by\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_j = y_j. \\tag{6.11}\n",
    "$$\n",
    "\n",
    "And in this setting, ridge regression amounts to finding $ \\beta_1, \\ldots, \\beta_p $ such that\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{p} (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\tag{6.12}\n",
    "$$\n",
    "\n",
    "is minimized, and the lasso amounts to finding the coefficients such that is minimized. \n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{p} (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\tag{6.13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfecd4",
   "metadata": {},
   "source": [
    "One can show that in this setting, the ridge regression esti\n",
    "mates take the form\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_j^R = \\frac{y_j}{1 + \\lambda},\n",
    "$$\n",
    "\n",
    "and the lasso estimates take the form\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_j^L =\n",
    "\\begin{cases}\n",
    "y_j - \\lambda/2 & \\text{if } y_j > \\lambda/2, \\\\\n",
    "y_j + \\lambda/2 & \\text{if } y_j < -\\lambda/2, \\\\\n",
    "0 & \\text{if } |y_j| \\leq \\lambda/2.\n",
    "\\end{cases}\n",
    "\\tag{6.14}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Figure 6.10 displays the situation. We can see that ridge regression and\n",
    "the lasso perform two very different types of shrinkage. In ridge regression,\n",
    "each least squares coefficient estimate is shrunken by the same proportion.\n",
    "In contrast, the lasso shrinks each least squares coefficient towards zero by\n",
    "a constant amount, $\\lambda/2$; the least squares coefficients that are less than\n",
    "$\\lambda/2$ in absolute value are shrunken entirely to zero. The type of shrink\n",
    "age performed by the lasso in this simple setting (6.15) is known as soft\n",
    "thresholding. The fact that some lasso coefficients are shrunken entirely to\n",
    "zero explains why the lasso performs feature selection.\n",
    "\n",
    "In the case of a more general data matrix $X$, the story is a little more\n",
    "complicated than what is depicted in Figure 6.10, but the main ideas still\n",
    "hold approximately: ridge regression more or less shrinks every dimension\n",
    "of the data by the same proportion, whereas the lasso more or less shrinks\n",
    "all coefficients toward zero by a similar amount, and sufficiently small co\n",
    "efficients are shrunken all the way to zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37411582",
   "metadata": {},
   "source": [
    "We now show that one can view ridge regression and the lasso through\n",
    "a Bayesian lens. A Bayesian viewpoint for regression assumes that the\n",
    "coefficient vector $\\beta$ has some prior distribution, say $p(\\beta)$, where \n",
    "$\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^T$. The likelihood of the data can be written as $f(Y|X, \\beta)$, where $ X = (X_1,\\ldots,X_p) $. Multiplying the prior distribution by the likelihood gives us (up to a proportionality constant) the posterior distribution, which takes the form\n",
    "\n",
    "$$\n",
    "p(\\beta | X, Y) \\propto f(Y | X, \\beta)p(\\beta | X) = f(Y | X, \\beta)p(\\beta),\n",
    "$$\n",
    "\n",
    "where the proportionality above follows from Bayes’ theorem, and the\n",
    "equality above follows from the assumption that $X$ is fixed.\n",
    "\n",
    "We assume the usual linear model,\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + X_1 \\beta_1 + \\cdots + X_p \\beta_p + \\epsilon,\n",
    "$$\n",
    "\n",
    "and suppose that the errors are independent and drawn from a normal distribution. Furthermore, assume that \n",
    "\n",
    "$$\n",
    "p(\\beta) = \\prod_{j=1}^p g(\\beta_j),\n",
    "$$\n",
    "\n",
    "for some density function $g$. It turns out that ridge regression and the lasso follow naturally from two special cases of $g$:\n",
    "\n",
    "- If $g$ is a Gaussian distribution with mean zero and standard deviation a function of $\\lambda$, then it follows that the posterior mode for $\\beta$—that is, the most likely value for $\\beta$, given the data—is given by the ridge regression solution. (In fact, the ridge regression solution is also the posterior mean.)\n",
    "  \n",
    "- If $g$ is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of $\\lambda$, then it follows that the posterior mode for $\\beta$ is the lasso solution. (However, the lasso solution is not the posterior mean, and in fact, the posterior mean does not yield a sparse coefficient vector.)\n",
    "\n",
    "The Gaussian and double-exponential priors are displayed in Figure 6.11. Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow directly from assuming the usual linear model with normal errors, together with a simple prior distribution for $\\beta$. Notice that the lasso prior is steeply peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge assumes the coefficients are randomly distributed about zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96060134",
   "metadata": {},
   "source": [
    "Selecting the Tuning Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e3e82",
   "metadata": {},
   "source": [
    "Just as the subset selection approaches considered in Section 6.1 require\n",
    "a method to determine which of the models under consideration is best,\n",
    "implementing ridge regression and the lasso requires a method for selecting\n",
    "a value for the tuning parameter in $ \\lambda $ (6.5) and (6.7), or equivalently, the\n",
    "value of the constraint s in (6.9) and (6.8). Cross-validation provides a sim\n",
    "ple way to tackle this problem. We choose a grid of $ \\lambda $ values, and compute\n",
    "the cross-validation error for each value of $ \\lambda $, as described in Chapter 5.We\n",
    "then select the tuning parameter value for which the cross-validation error\n",
    "is smallest. Finally, the model is re-fit using all of the available observations\n",
    "and the selected value of the tuning parameter.\n",
    "\n",
    "  Figure 6.12 displays the choice of $ \\lambda $ that results from performing leave\n",
    "one-out cross-validation on the ridge regression fits from the `Credit` data\n",
    "set. The dashed vertical lines indicate the selected value of $ \\lambda $. In this case\n",
    "the value is relatively small, indicating that the optimal fit only involves a\n",
    "small amount of shrinkage relative to the least squares solution. In addition,\n",
    "the dip is not very pronounced, so there is rather a wide range of values\n",
    "that would give a very similar error. In a case like this we might simply use\n",
    "the least squares solution.\n",
    "    \n",
    "  Figure 6.13 provides an illustration of ten-fold cross-validation applied to\n",
    "the lasso fits on the sparse simulated data from Figure 6.9. The left-hand\n",
    "panel of Figure 6.13 displays the cross-validation error, while the right-hand\n",
    "panel displays the coefficient estimates. The vertical dashed lines indicate\n",
    "the point at which the cross-validation error is smallest. The two colored\n",
    "lines in the right-hand panel of Figure 6.13 represent the two predictors\n",
    "that are related to the response, while the grey lines represent the unre\n",
    "lated predictors; these are often referred to as $signal$ and $noise$ variables,\n",
    "respectively. Not only has the lasso correctly given much larger coeffi\n",
    "cient estimates to the two signal predictors, but also the minimum cross\n",
    "validation error corresponds to a set of coefficient estimates for which only\n",
    "the signal variables are non-zero. Hence cross-validation together with the\n",
    "lasso has correctly identified the two signal variables in the model, even\n",
    "though this is a challenging setting, with $p = 45$ variables and only $n = 50$ observations. In contrast, the least squares solution—displayed on the far\n",
    "right of the right-hand panel of Figure 6.13—assigns a large coefficient\n",
    "estimate to only one of the two signal variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81886c",
   "metadata": {},
   "source": [
    "#### Dimension Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28170071",
   "metadata": {},
   "source": [
    "The methods that we have discussed so far in this chapter have controlled variance in two different ways, either by using a subset of the original variables, or by shrinking their coefficients toward zero. All of these methods are defined using the original predictors, $X_1, X_2, \\ldots, X_p$. We now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods.\n",
    "\n",
    "Let $Z_1, Z_2, \\ldots, Z_M$ represent $M$ linear combinations of our original $p$ predictors. That is,\n",
    "\n",
    "$$\n",
    "Z_m = \\sum_{j=1}^p \\gamma_{jm} X_j \\tag{6.16}\n",
    "$$\n",
    "\n",
    "for some constants $\\gamma_{1m}, \\gamma_{2m}, \\ldots, \\gamma_{pm}$, $m = 1, \\ldots, M$. We can then fit the linear regression model\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\sum_{m=1}^M \\beta_m Z_{im} + \\epsilon_i, \\quad i = 1, \\ldots, n, \\tag{6.17}\n",
    "$$\n",
    "\n",
    "using least squares. Note that in (6.17), the regression coefficients are given by $\\beta_0, \\beta_1, \\ldots, \\beta_M$. If the constants $\\gamma_{1m}, \\gamma_{2m}, \\ldots, \\gamma_{pm}$ are chosen wisely, then such dimension reduction approaches can often outperform least squares regression. In other words, fitting (6.17) using least squares can lead to better results than fitting (6.1) using least squares.\n",
    "\n",
    "The term dimension reduction comes from the fact that this approach reduces the problem of estimating the $p+1$ coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_p$ to the simpler problem of estimating the $M + 1$ coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_M$, where $M < p$. In other words, the dimension of the problem has been reduced from $p + 1$ to $M + 1$.\n",
    "\n",
    "Notice that from (6.16),\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^M \\beta_m Z_{im} = \\sum_{m=1}^M \\beta_m \\left( \\sum_{j=1}^p \\gamma_{jm} X_j \\right) = \\sum_{j=1}^p \\left( \\sum_{m=1}^M \\beta_m \\gamma_{jm} \\right) X_j,\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\beta_j = \\sum_{m=1}^M \\beta_m \\gamma_{jm}. \\tag{6.18}\n",
    "$$\n",
    "\n",
    "Hence (6.17) can be thought of as a special case of the original linear regression model given by (6.1). Dimension reduction serves to constrain the estimated $\\beta_j$ coefficients, since now they must take the form (6.18). This constraint on the form of the coefficients has the potential to bias the coefficient estimates. However, in situations where $p$ is larger relative to $M$, selecting a value of $M<p$ can significantly reduce the variance of the fitted coefficients. If $M=p$, and all the $Z_m$ are linearly independent, then (6.18) poses no constraints. In this case, no dimension reduction occurs, and so fitting (6.17) is equivalent to performing least squares on the original $p$ predictors.\n",
    "\n",
    "All dimension reduction methods work in two steps. First, the transformed predictors $Z_1, Z_2, \\ldots, Z_M$ are obtained. Second, the model is fit using these $M$ predictors. However, the choice of $Z_1, Z_2, \\ldots, Z_M$, or equivalently, the selection of the $\\gamma_m$’s, can be achieved in different ways. In this chapter, we will consider two approaches for this task: principal components and partial least squares.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85566728",
   "metadata": {},
   "source": [
    "Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c77d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$Principal$ $components$ $analysis$ (PCA) is a popular approach for deriving a low-dimensional set of features from a large set of variables. PCA is discussed in greater detail as a tool for unsupervised learning in Chapter 12. Here we describe its use as a dimension reduction technique for regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76958a6b",
   "metadata": {},
   "source": [
    "\n",
    "An Overview of Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7ad75",
   "metadata": {},
   "source": [
    "PCA is a technique for reducing the dimension of an $n \\times p$ data matrix $X$. The first principal component direction of the data is that along which the observations vary the most. For instance, consider Figure 6.14, which shows population size ($pop$) in tens of thousands of people, and ad spending for a particular company ($ad$) in thousands of dollars, for 100 cities. The green solid line represents the first principal component direction of the data. We can see by eye that this is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance; projecting the observations onto any other line would yield projected observations with lower variance. Projecting a point onto a line simply involves finding the location on the line which is closest to the point.\n",
    "\n",
    "The first principal component is displayed graphically in Figure 6.14, but how can it be summarized mathematically? It is given by the formula\n",
    "\n",
    "$$\n",
    "Z_1 = 0.839 (pop - \\bar{pop}) + 0.544 (ad - \\bar{ad}). \\tag{6.19}\n",
    "$$\n",
    "\n",
    "Here $\\gamma_{11} = 0.839$ and $\\gamma_{21} = 0.544$ are the principal component loadings, which define the direction referred to above. In (6.19), $pop$ indicates the mean of all $pop$ values in this data set, and $ad$ indicates the mean of all advertising spending. The idea is that out of every possible linear combination of $pop$ and $ad$ such that $\\gamma_{11}^2 + \\gamma_{21}^2 = 1$, this particular linear combination yields the highest variance: i.e., this is the linear combination for which \n",
    "\n",
    "$$\n",
    "Var( \\gamma_{11} (pop - \\bar{pop}) + \\gamma_{21} (ad - \\bar{ad})) \\text{ is maximized.}\n",
    "$$\n",
    "\n",
    "It is necessary to consider only linear combinations of the form $\\gamma_{11}^2 + \\gamma_{21}^2 = 1$, since otherwise we could increase $\\gamma_{11}$ and $\\gamma_{21}$ arbitrarily in order to blow up the variance. In (6.19), the two loadings are both positive and have similar size, and so $Z_1$ is almost an average of the two variables.\n",
    "\n",
    "Since $n = 100$, $pop$ and $ad$ are vectors of length 100, and so is $Z_1$ in (6.19). For instance,\n",
    "\n",
    "$$\n",
    "z_{i1} = 0.839 (pop_i - \\bar{pop}) + 0.544 (ad_i - \\bar{ad}). \\tag{6.20}\n",
    "$$\n",
    "\n",
    "The values of $z_{11}, \\ldots, z_{n1}$ are known as the principal component scores, and can be seen in the right-hand panel of Figure 6.15.\n",
    "\n",
    "There is also another interpretation of PCA: the first principal component vector defines the line that is as close as possible to the data. For instance, in Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line. These distances are plotted as dashed line segments in the left-hand panel of Figure 6.15, in which the crosses represent the projection of each point onto the first principal component line. The first principal component has been chosen so that the projected observations are as close as possible to the original observations.\n",
    "\n",
    "In the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x-axis. It is possible to show that the first principal component score for the $i$th observation, given in (6.20), is the distance in the x-direction of the $i$th cross from zero. So for example, the point in the bottom-left corner of the left-hand panel of Figure 6.15 has a large negative principal component score, $z_{i1} = -26.1$, while the point in the top-right corner has a large positive score, $z_{i1} = 18.7$. These scores can be computed directly using (6.20).\n",
    "\n",
    "We can think of the values of the principal component $Z_1$ as single number summaries of the joint $pop$ and $ad$ budgets for each location. In this example, if $z_{i1} = 0.839 (pop_i - \\bar{pop}) + 0.544 (ad_i - \\bar{ad}) < 0$, then this indicates a city with below-average population size and below-average ad spending. A positive score suggests the opposite. How well can a single number represent both $pop$ and $ad$? In this case, Figure 6.14 indicates that $pop$ and $ad$ have approximately a linear relationship, and so we might expect that a single-number summary will work well. Figure 6.16 displays $z_{i1}$ versus both $pop$ and $ad$. The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to capture most of the information contained in the $pop$ and $ad$ predictors.\n",
    "\n",
    "So far we have concentrated on the first principal component. In general, one can construct up to $p$ distinct principal components. The second principal component $Z_2$ is a linear combination of the variables that is uncorrelated with $Z_1$, and has largest variance subject to this constraint. The second principal component direction is illustrated as a dashed blue line in Figure 6.14. It turns out that the zero correlation condition of $Z_1$ with $Z_2$ is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction. The second principal component is given by the formula\n",
    "\n",
    "$$\n",
    "Z_2 = 0.544 \\cdot (pop - \\bar{pop}) - 0.839 \\cdot (ad - \\bar{ad}).\n",
    "$$\n",
    "\n",
    "Since the advertising data has two predictors, the first two principal components contain all of the information that is in $pop$ and $ad$. However, by construction, the first component will contain the most information. Consider, for example, the much larger variability of $z_{i1}$ (the x-axis) versus $z_{i2}$ (the y-axis) in the right-hand panel of Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information. As another illustration, Figure 6.17 displays $z_{i2}$ versus $pop$ and $ad$. There is little relationship between the second principal component and these two predictors, again suggesting that in this case, one only needs the first principal component in order to accurately represent the $pop$ and $ad$ budgets.\n",
    "\n",
    "With two-dimensional data, such as in our advertising example, we can construct at most two principal components. However, if we had other predictors, such as population age, income level, education, and so forth, then additional components could be constructed. They would successively maximize variance, subject to the constraint of being uncorrelated with the preceding components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188ed20",
   "metadata": {},
   "source": [
    "The Principal Components Regression Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69ab99",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The principal components regression (PCR) approach involves constructing the first $M$ principal components, $Z_1, \\ldots, Z_M$, and then using these components as the predictors in a linear regression model that is fit using least squares. The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that the directions in which $X_1, \\ldots, X_p$ show the most variation are the directions that are associated with $Y$. While this assumption is not guaranteed to be true, it often turns out to be a reasonable enough approximation to give good results.\n",
    "\n",
    "If the assumption underlying PCR holds, then fitting a least squares model to $Z_1, \\ldots, Z_M$ will lead to better results than fitting a least squares model to $X_1, \\ldots, X_p$, since most or all of the information in the data that relates to the response is contained in $Z_1, \\ldots, Z_M$, and by estimating only $M < p$ coefficients we can mitigate overfitting. In the advertising data, the first principal component explains most of the variance in both $pop$ and $ad$, so a principal component regression that uses this single variable to predict some response of interest, such as sales, will likely perform quite well.\n",
    "\n",
    "Figure 6.18 displays the PCR fits on the simulated data sets from Figures 6.8 and 6.9. Recall that both data sets were generated using $n = 50$ observations and $p = 45$ predictors. However, while the response in the first data set was a function of all the predictors, the response in the second data set was generated using only two of the predictors. The curves are plotted as a function of $M$, the number of principal components used as predictors in the regression model. As more principal components are used in the regression model, the bias decreases, but the variance increases. This\n",
    "results in a typical U-shape for the mean squared error. When $M$ = p = 45$,\n",
    "then PCR amounts simply to a least squares fit using all of the original\n",
    "predictors. The figure indicates that performing PCR with an appropriate\n",
    "choice of $M$ can result in a substantial improvement over least squares, es\n",
    "pecially in the left-hand panel. However, by examining the ridge regression\n",
    "and lasso results in Figures 6.5, 6.8, and 6.9, we see that PCR does not\n",
    "perform as well as the two shrinkage methods in this example.\n",
    "\n",
    "The relatively worse performance of PCR in Figure 6.18 is a consequence\n",
    "of the fact that the data were generated in such a way that many princi\n",
    "pal components are required in order to adequately model the response.\n",
    "In contrast, PCR will tend to do well in cases when the first few principal\n",
    "components are sufficient to capture most of the variation in the predictors\n",
    "as well as the relationship with the response. The left-hand panel of Fig\n",
    "ure 6.19 illustrates the results from another simulated data set designed to\n",
    "be more favorable to PCR. Here the response was generated in such a way\n",
    "that it depends exclusively on the first five principal components. Now the\n",
    "bias drops to zero rapidly as $M$, the number of principal components used\n",
    "in PCR, increases. The mean squared error displays a clear minimum at\n",
    "$M =5$.The right-hand panel of Figure 6.19 displays the results on these\n",
    "data using ridge regression and the lasso. All three methods offer a signif\n",
    "icant improvement over least squares. However, PCR and ridge regression\n",
    "slightly outperform the lasso.\n",
    "\n",
    "We note that even though PCR provides a simple way to perform re\n",
    "gression using $M<p$ predictors, it is not a feature selection method. This\n",
    "is because each of the $M$ principal components used in the regression is a\n",
    "linear combination of all p of the original features. For instance, in (6.19),\n",
    "$Z_1$ was a linear combination of both `pop` and `ad`. Therefore, while PCR of\n",
    "ten performs quite well in many practical settings, it does not result in the development of a model that relies upon a small set of the original features.\n",
    "In this sense, PCR is more closely related to ridge regression than to the\n",
    "lasso. In fact, one can show that PCR and ridge regression are very closely\n",
    "related. One can even think of ridge regression as a continuous version of\n",
    "PCR!\n",
    "\n",
    "In PCR, the number of principal components, $M$, is typically chosen by\n",
    "cross-validation. The results of applying PCR to the `Credit` data set are\n",
    "shown in Figure 6.20; the right-hand panel displays the cross-validation er\n",
    "rors obtained, as a function of $M$. On these data, the lowest cross-validation\n",
    "error occurs when there are $M = 10$ components; this corresponds to al\n",
    "most no dimension reduction at all, since PCR with $M = 11$ is equivalent\n",
    "to simply performing least squares.\n",
    "\n",
    "When performing PCR, we generally recommend $standardizing$ each pre\n",
    "dictor, using (6.6), prior to generating the principal components. This stan\n",
    "dardization ensures that all variables are on the same scale. In the absence\n",
    "of standardization, the high-variance variables will tend to play a larger\n",
    "role in the principal components obtained, and the scale on which the vari\n",
    "ables are measured will ultimately have an effect on the final PCR model.\n",
    "However, if the variables are all measured in the same units (say, kilograms,\n",
    "or inches), then one might choose not to standardize them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6923400",
   "metadata": {},
   "source": [
    "#### Partial Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed841a",
   "metadata": {},
   "source": [
    "\n",
    "The PCR approach that we just described involves identifying linear combi\n",
    "nations, or directions, that best represent the predictors $X1,...,Xp$. These\n",
    "directions are identified in an $unsupervised$ way, since the response Y is\n",
    "not used to help determine the principal component directions. That is,\n",
    "the response does not supervise the identification of the principal compo\n",
    "nents. Consequently, PCR suffers from a drawback: there is no guarantee\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
