{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c645b505",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb0214",
   "metadata": {},
   "source": [
    " The linear regression model discussed in Chapter 3 assumes that the re\n",
    "sponse variable Y is quantitative. But in many situations, the response\n",
    " variable is instead qualitative. For example, eye color is qualitative. Of- qualitative\n",
    " ten qualitative variables are referred to as categorical; we will use these\n",
    " terms interchangeably. In this chapter, we study approaches for predicting\n",
    " qualitative responses, a process that is known as classification. Predicting classification\n",
    " a qualitative response for an observation can be referred to as classifying\n",
    " that observation, since it involves assigning the observation to a category,\n",
    " or class. On the other hand, often the methods used for classification first\n",
    " predict the probability that the observation belongs to each of the cate\n",
    "gories of a qualitative variable, as the basis for making the classification.\n",
    " In this sense they also behave like regression methods.\n",
    " There are many possible classification techniques, or classifiers, that one classifier\n",
    " might use to predict a qualitative response. We touched on some of these\n",
    " in Sections 2.1.5 and 2.2.3. In this chapter we discuss some widely-used\n",
    " classifiers: logistic regression, linear discriminant analysis, quadratic dis- logistic\n",
    " criminant analysis, naive Bayes, and K-nearest neighbors. The discussion\n",
    " of logistic regression is used as a jumping-off point for a discussion of gen\n",
    "eralized linear models, and in particular, Poisson regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44eab83",
   "metadata": {},
   "source": [
    "#### An Overview of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94704776",
   "metadata": {},
   "source": [
    " Classification problems occur often, perhaps even more so than regression problems. Some examples include:\n",
    "  1. A person arrives at the emergency room with a set of symptoms\n",
    " that could possibly be attributed to one of three medical conditions.\n",
    " Which of the three conditions does the individual have?\n",
    " 2. An online banking service must be able to determine whether or not\n",
    " a transaction being performed on the site is fraudulent, on the basis\n",
    " of the user’s IP address, past transaction history, and so forth.\n",
    " 3. On the basis of DNA sequence data for a number of patients with\n",
    " and without a given disease, a biologist would like to figure out which\n",
    " DNA mutations are deleterious (disease-causing) and which are not.\n",
    "\n",
    " Just as in the regression setting, in the classification setting we have a\n",
    " set of training observations (x1,y1),...,(xn,yn) that we can use to build\n",
    " a classifier. We want our classifier to perform well not only on the training\n",
    " data, but also on test observations that were not used to train the classifier.\n",
    " \n",
    " In this chapter, we will illustrate the concept of classification using the\n",
    " simulated Default data set. We are interested in predicting whether an\n",
    " individual will default on his or her credit card payment, on the basis of\n",
    " annual income and monthly credit card balance. The data set is displayed\n",
    " in Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual\n",
    " income and monthly credit card balance for a subset of 10,000 individuals.\n",
    " The individuals who defaulted in a given month are shown in orange, and\n",
    " those who did not in blue. (The overall default rate is about 3%, so we\n",
    " have plotted only a fraction of the individuals who did not default.) It\n",
    " appears that individuals who defaulted tended to have higher credit card\n",
    " balances than those who did not. In the center and right-hand panels of\n",
    " Figure 4.1, two pairs of boxplots are shown. The first shows the distribution\n",
    " of balance split by the binary default variable; the second is a similar plot\n",
    " for income. In this chapter, we learn how to build a model to predict default\n",
    " (Y ) for any given value of balance (X1) and income (X2). Since Y is not\n",
    " quantitative, the simple linear regression model of Chapter 3 is not a good\n",
    " choice: we will elaborate on this further in Section 4.2.\n",
    " \n",
    " It is worth noting that Figure 4.1 displays a very pronounced relation\n",
    "ship between the predictor balance and the response default. In most real\n",
    " applications, the relationship between the predictor and the response will\n",
    " not be nearly so strong. However, for the sake of illustrating the classifica\n",
    "tion procedures discussed in this chapter, we use an example in which the\n",
    " relationship between the predictor and the response is somewhat exaggerated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5956a8",
   "metadata": {},
   "source": [
    "#### Why Not Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38612c",
   "metadata": {},
   "source": [
    " Suppose that we are trying to predict the medical condition of a patient\n",
    " in the emergency room on the basis of her symptoms. In this simplified\n",
    " example, there are three possible diagnoses: stroke, drug overdose, and epilepticseizure.Wecouldconsiderencodingthesevaluesasaquantita\n",
    "tiveresponsevariable,Y,asfollows:\n",
    " Y=\n",
    " 1 ifstroke;\n",
    " 2 ifdrugoverdose;\n",
    " 3 ifepilepticseizure.\n",
    " Usingthiscoding,leastsquarescouldbeusedtofitalinearregressionmodel\n",
    " topredictY onthebasisofasetofpredictorsX1,...,Xp.Unfortunately,\n",
    " thiscodingimpliesanorderingontheoutcomes,puttingdrug overdosein\n",
    " betweenstrokeandepilepticseizure, and insistingthat thedifference\n",
    " betweenstrokeanddrugoverdose is thesameas thedifferencebetween\n",
    " drugoverdoseandepilepticseizure. Inpractice there isnoparticular\n",
    " reasonthat thisneeds tobethecase.For instance, onecouldchoosean\n",
    " equallyreasonablecoding,\n",
    " Y=\n",
    " 1 ifepilepticseizure;\n",
    " 2 ifstroke;\n",
    " 3 ifdrugoverdose,\n",
    " whichwouldimplyatotallydifferentrelationshipamongthethreecondi\n",
    "tions.Eachof thesecodingswouldproducefundamentallydifferent linear\n",
    " modelsthatwouldultimatelyleadtodifferentsetsofpredictionsontest\n",
    " observations.\n",
    " If theresponsevariable’svaluesdidtakeonanaturalordering, suchas\n",
    " mild,moderate,andsevere,andwefeltthegapbetweenmildandmoderate\n",
    " wassimilartothegapbetweenmoderateandsevere,thena1,2,3coding\n",
    " wouldbereasonable.Unfortunately, ingeneral thereisnonaturalwayto\n",
    " convert a qualitative response variable with more than two levels into a\n",
    " quantitative response that is ready for linear regression.\n",
    " For a binary (two level) qualitative response, the situation is better. For binary\n",
    " instance, perhaps there are only two possibilities for the patient’s medical\n",
    " condition: stroke and drug overdose. We could then potentially use the\n",
    " dummyvariable approach from Section 3.3.1 to code the response as follows:\n",
    " Y = 0 ifstroke;\n",
    " 1 if drug overdose.\n",
    " We could then fit a linear regression to this binary response, and predict\n",
    " drug overdose if ˆY>0.5 and stroke otherwise. In the binary case it is not\n",
    " hard to show that even if we flip the above coding, linear regression will\n",
    " produce the same final predictions.\n",
    " For a binary response with a 0/1 coding as above, regression by least\n",
    " squares is not completely unreasonable: it can be shown that the X ˆob\n",
    "tained using linear regression is in fact an estimate of Pr(drug overdose|X)\n",
    " in this special case. However, if we use linear regression, some of our es\n",
    "timates might be outside the [0,1] interval (see Figure 4.2), making them\n",
    " hard to interpret as probabilities! Nevertheless, the predictions provide an\n",
    " ordering and can be interpreted as crude probability estimates. Curiously,\n",
    " it turns out that the classifications that we get if we use linear regression\n",
    " to predict a binary response will be the same as for the linear discriminant\n",
    " analysis (LDA) procedure we discuss in Section 4.4.\n",
    " To summarize, there are at least two reasons not to perform classifica\n",
    "tion using a regression method: (a) a regression method cannot accommo\n",
    "date a qualitative response with more than two classes; (b) a regression\n",
    " method will not provide meaningful estimates of Pr(Y |X), even with just\n",
    " two classes. Thus, it is preferable to use a classification method that is\n",
    " truly suited for qualitative response values. In the next section, we present\n",
    " logistic regression, which is well-suited for the case of a binary qualita\n",
    "tive response; in later sections we will cover classification methods that are\n",
    " appropriate when the qualitative response has two or more classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3996a",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf416a40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " Consider again the Default data set, where the response default falls into\n",
    " one of two categories, Yes or No. Rather than modeling this response Y\n",
    " directly, logistic regression models the probability that Y belongs to a par\n",
    "ticular category.\n",
    " For the Default data, logistic regression models the probability of default.\n",
    " For example, the probability of default given balance can be written as\n",
    " Pr(default = Yes|balance).\n",
    " Thevalues of Pr(default = Yes|balance), which we abbreviate p(balance),\n",
    " will range between 0 and 1. Then for any given value of balance, a prediction\n",
    " can be made for default. For example, one might predict default = Yes\n",
    "  foranyindividual forwhomp(balance)>0.5.Alternatively, ifacompany\n",
    " wishestobeconservativeinpredictingindividualswhoareatriskforde\n",
    "fault,thentheymaychoosetousealowerthreshold,suchasp(balance)>\n",
    " 0.1.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955d644",
   "metadata": {},
   "source": [
    "##### The Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c310444",
   "metadata": {},
   "source": [
    " Howshouldwemodel therelationshipbetweenp(X)=Pr(Y=1|X)and\n",
    " X?(Forconvenienceweareusingthegeneric0/1codingfortheresponse.)\n",
    " InSection4.2weconsideredusinga linearregressionmodel torepresent\n",
    " theseprobabilities:\n",
    " p(X)= 0+ 1X. (4.1)\n",
    " Ifweuse this approachtopredict default=Yesusingbalance, thenwe\n",
    " obtainthemodel showninthe left-handpanelofFigure4.2.Herewesee\n",
    " theproblemwiththisapproach: forbalances close tozerowepredict a\n",
    " negativeprobabilityofdefault;ifweweretopredictforverylargebalances,\n",
    " wewouldgetvaluesbiggerthan1.Thesepredictionsarenotsensible,since\n",
    " ofcoursethetrueprobabilityofdefault, regardlessofcreditcardbalance,\n",
    " mustfallbetween0and1.Thisproblemisnotuniquetothecreditdefault\n",
    " data.Anytimeastraight line isfittoabinaryresponsethat iscodedas\n",
    " 0or1, inprinciplewecanalwayspredictp(X)<0forsomevaluesofX\n",
    " andp(X)>1forothers(unlesstherangeofXislimited).\n",
    " Toavoidthisproblem,wemustmodelp(X)usingafunctionthatgives\n",
    " outputsbetween0and1 for all values ofX.Many functionsmeet this\n",
    " description. Inlogisticregression,weusethelogisticfunction, logistic\n",
    " function\n",
    " p(X)= e 0+1X\n",
    " 1+e 0+1X\n",
    " . (4.2)\n",
    " Tofitthemodel (4.2),weuseamethodcalledmaximumlikelihood,which maximum\n",
    " likelihood wediscussinthenextsection.Theright-handpanelofFigure4.2illustrates\n",
    " thefitofthelogisticregressionmodeltotheDefaultdata.Noticethatfor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfde67",
   "metadata": {},
   "source": [
    " low balances we now predict the probability of default as close to, but never\n",
    " below, zero. Likewise, for high balances we predict a default probability\n",
    " close to, but never above, one. The logistic function will always produce\n",
    " an S-shaped curve of this form, and so regardless of the value of X,we\n",
    " will obtain a sensible prediction. We also see that the logistic model is\n",
    " better able to capture the range of probabilities than is the linear regression\n",
    " model in the left-hand plot. The average fitted probability in both cases is\n",
    " 0.0333 (averaged over the training data), which is the same as the overall\n",
    " proportion of defaulters in the data set.\n",
    " After a bit of manipulation of (4.2), we find that\n",
    " p(X)\n",
    " 1 p(X) =e 0+1X.\n",
    " (4.3)\n",
    " The quantity p(X)/[1 p(X)] is called the odds, and can take on any value odds\n",
    " between 0 and . Values of the odds close to 0 and indicate very low\n",
    " and very high probabilities of default, respectively. For example, on average\n",
    " 1 in 5 people with an odds of 1/4 will default, since p(X)=0.2 implies an\n",
    " odds of 0.2\n",
    " 1 0.2 =1/4. Likewise, on average nine out of every ten people with\n",
    " an odds of 9 will default, since p(X)=0.9 implies an odds of 0.9\n",
    " 1 0.9 =9.\n",
    " Odds are traditionally used instead of probabilities in horse-racing, since\n",
    " they relate more naturally to the correct betting strategy.\n",
    " By taking the logarithm of both sides of (4.3), we arrive at\n",
    " log\n",
    " p(X)\n",
    " 1 p(X) = 0+ 1X.\n",
    " (4.4)\n",
    " The left-hand side is called the log odds or logit. We see that the logistic log odds\n",
    " regression model (4.2) has a logit that is linear in X.\n",
    " Recall from Chapter 3 that in a linear regression model, 1 gives the\n",
    " average change in Y associated with a one-unit increase in X. By contrast,\n",
    " in a logistic regression model, increasing X by one unit changes the log\n",
    " odds by 1 (4.4). Equivalently, it multiplies the odds by e 1 (4.3). However,\n",
    " because the relationship between p(X) and X in (4.2) is not a straight line,\n",
    " 1 does not correspond to the change in p(X) associated with a one-unit\n",
    " increase in X. The amount that p(X) changes due to a one-unit change in\n",
    " X depends on the current value of X. But regardless of the value of X, if\n",
    " 1 is positive then increasing X will be associated with increasing p(X),\n",
    " and if 1 is negative then increasing X will be associated with decreasing\n",
    " p(X). The fact that there is not a straight-line relationship between p(X)\n",
    " and X, and the fact that the rate of change in p(X) per unit change in X\n",
    " depends on the current value of X, can also be seen by inspection of the\n",
    " right-hand panel of Figure 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb6c35",
   "metadata": {},
   "source": [
    "##### Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332f154",
   "metadata": {},
   "source": [
    " The coefficients 0 and 1 in (4.2) are unknown, and must be estimated\n",
    " based on the available training data. In Chapter 3, we used the least squares\n",
    " approach to estimate the unknown linear regression coefficients. Although\n",
    " we could use (non-linear) least squares to fit the model (4.4), the more\n",
    " general method of maximum likelihood is preferred, since it has better sta\n",
    "tistical properties. The basic intuition behind using maximum likelihood  to fit a logistic regression model is as follows: we seek estimates for 0 and\n",
    " 1 such that the predicted probability ˆp(xi) of default for each individual,\n",
    " using (4.2), corresponds as closely as possible to the individual’s observed\n",
    " default status. In other words, we try to find ˆ0 and ˆ1 such that plugging\n",
    " these estimates into the model for p(X), given in (4.2), yields a number\n",
    " close to one for all individuals who defaulted, and a number close to zero\n",
    " for all individuals who did not. This intuition can be formalized using a\n",
    " mathematical equation called a likelihood function:\n",
    " ( 0, 1)=\n",
    " p(xi)\n",
    " i:yi=1\n",
    " (1 p(xi)).\n",
    " i :yi=0\n",
    " (4.5)\n",
    " The estimates ˆ0 and ˆ1 are chosen to maximize this likelihood function.\n",
    " Maximum likelihood is a very general approach that is used to fit many\n",
    " of the non-linear models that we examine throughout this book. In the\n",
    " linear regression setting, the least squares approach is in fact a special case\n",
    " of maximum likelihood. The mathematical details of maximum likelihood\n",
    " are beyond the scope of this book. However, in general, logistic regression\n",
    " and other models can be easily fit using statistical software such as R, and\n",
    " so we do not need to concern ourselves with the details of the maximum\n",
    " likelihood fitting procedure.\n",
    " Table 4.1 shows the coefficient estimates and related information that\n",
    " result from fitting a logistic regression model on the Default data in order\n",
    " to predict the probability of default=Yes using balance. We see that ˆ1 =\n",
    " 0.0055; this indicates that an increase in balance is associated with an\n",
    " increase in the probability of default. To be precise, a one-unit increase in\n",
    " balance is associated with an increase in the log odds of default by 0.0055\n",
    " units.\n",
    " Many aspects of the logistic regression output shown in Table 4.1 are\n",
    " similar to the linear regression output of Chapter 3. For example, we can\n",
    " measure the accuracy of the coefficient estimates by computing their stan\n",
    "dard errors. The z-statistic in Table 4.1 plays the same role as the t-statistic\n",
    " in the linear regression output, for example in Table 3.1 on page 77. For\n",
    " instance, the z-statistic associated with 1 is equal to ˆ1/SE(ˆ1), and so a\n",
    " large (absolute) value of the z-statistic indicates evidence against the null\n",
    " hypothesis H0 : 1 =0. This null hypothesis implies that p(X)= e 0\n",
    " 1+e 0 \n",
    ": in\n",
    " other words, that the probability of default does not depend on balance.\n",
    " Since the p-value associated with balance in Table 4.1 is tiny, we can reject\n",
    " H0. In other words, we conclude that there is indeed an association between\n",
    " balance and probability of default. The estimated intercept in Table 4.1\n",
    " is typically not of interest; its main purpose is to adjust the average fitted\n",
    " probabilities to the proportion of ones in the data (in this case, the overall\n",
    " default rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f41578",
   "metadata": {},
   "source": [
    "##### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c98477",
   "metadata": {},
   "source": [
    " Once the coefficients have been estimated, we can compute the probability\n",
    " of default for any given credit card balance. For example, using the coeffi\n",
    "cient estimates given in Table 4.1, we predict that the default probability  Classification\n",
    " Coefficient Std. error z-statistic\n",
    " p-value\n",
    " Intercept 10.6513\n",
    " balance 0.0055\n",
    " 0.3612\n",
    " 0.0002\n",
    " 29.5 <0.0001\n",
    " 24.9 <0.0001\n",
    " TABLE 4.1. For the Default data, estimated coefficients of the logistic regres\n",
    "sion model that predicts the probability of default using balance. A one-unit\n",
    " increase in balance is associated with an increase in the log odds of default by\n",
    " 0.0055 units.\n",
    " Coefficient Std. error z-statistic\n",
    " p-value\n",
    " Intercept\n",
    " student[Yes]\n",
    " 3.5041\n",
    " 0.4049\n",
    " 0.0707\n",
    " 0.1150\n",
    " 49.55 <0.0001\n",
    " 3.52\n",
    " 0.0004\n",
    " TABLE4.2.FortheDefault data, estimated coefficients of the logistic regression\n",
    " model that predicts the probability of default using student status. Student status\n",
    " is encoded as a dummy variable, with a value of 1 for a student and a value of 0\n",
    " for a non-student, and represented by the variable student[Yes] in the table.\n",
    " for an individual with a balance of $1,000 is\n",
    " ˆ\n",
    " p(X)= eˆ0+ˆ1X\n",
    " 1+eˆ0+ˆ1X \n",
    "= e 10.6513+0.0055 1,000\n",
    " 1+e 10.6513+0.0055 1,000 \n",
    "=0.00576,\n",
    " which is below 1%. In contrast, the predicted probability of default for an\n",
    " individual with a balance of $2,000 is much higher, and equals 0.586 or\n",
    " 58.6%.\n",
    " One can use qualitative predictors with the logistic regression model us\n",
    "ing the dummy variable approach from Section 3.3.1. As an example, the\n",
    " Default data set contains the qualitative variable student. To fit a model\n",
    " that uses student status as a predictor variable, we simply create a dummy\n",
    " variable that takes on a value of 1 for students and 0 for non-students. The\n",
    " logistic regression model that results from predicting probability of default\n",
    " from student status can be seen in Table 4.2. The coefficient associated\n",
    " with the dummy variable is positive, and the associated p-value is statisti\n",
    "cally significant. This indicates that students tend to have higher default\n",
    " probabilities than non-students:\n",
    " Pr(default=Yes|student=Yes)= e 3.5041+0.4049 1\n",
    " 1+e 3.5041+0.4049 1 \n",
    "=0.0431,\n",
    " Pr(default=Yes|student=No)= e 3.5041+0.4049 0\n",
    " 1+e 3.5041+0.4049 0 \n",
    "=0.0292.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a207b9a",
   "metadata": {},
   "source": [
    "##### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f13fb",
   "metadata": {},
   "source": [
    "Wenowconsider the problem of predicting a binary response using multiple\n",
    " predictors. By analogy with the extension from simple to multiple linear\n",
    " regression in Chapter 3, we can generalize (4.4) as follows:\n",
    " log\n",
    " p(X)\n",
    " 1 p(X) = 0+ 1X1+···+ pXp,\n",
    " (4.6)\n",
    " where X =(X1,...,Xp) are p predictors. Equation 4.6 can be rewritten as\n",
    " p(X)= e 0+1X1+···+pXp\n",
    " 1+e 0+1X1+···+pXp\n",
    " .\n",
    " (4.7)\n",
    "  Coefficient Std. error z-statistic\n",
    " p-value\n",
    " Intercept\n",
    " balance\n",
    " income\n",
    " student[Yes]\n",
    " 10.8690\n",
    " 0.0057\n",
    " 0.0030\n",
    " 0.6468\n",
    " 0.4923\n",
    " 0.0002\n",
    " 0.0082\n",
    " 0.2362\n",
    " 22.08 <0.0001\n",
    " 24.74 <0.0001\n",
    " 0.37\n",
    " 2.74\n",
    " 0.7115\n",
    " 0.0062\n",
    " TABLE4.3.FortheDefault data, estimated coefficients of the logistic regression\n",
    " model that predicts the probability of default using balance, income, and student\n",
    " status. Student status is encoded as a dummy variable student[Yes], with a value\n",
    " of 1 for a student and a value of 0 for a non-student. In fitting this model, income\n",
    " was measured in thousands of dollars.\n",
    " 0\n",
    " Just as in Section 4.3.2, we use the maximum likelihood method to estimate\n",
    " , 1,..., p.\n",
    " Table 4.3 shows the coefficient estimates for a logistic regression model\n",
    " that uses balance, income (in thousands of dollars), and student status to\n",
    " predict probability of default. There is a surprising result here. The p\n",
    "values associated with balance and the dummy variable for student status\n",
    " are very small, indicating that each of these variables is associated with\n",
    " the probability of default. However, the coefficient for the dummy variable\n",
    " is negative, indicating that students are less likely to default than non\n",
    "students. In contrast, the coefficient for the dummy variable is positive in\n",
    " Table 4.2. How is it possible for student status to be associated with an\n",
    " increase in probability of default in Table 4.2 and a decrease in probability\n",
    " of default in Table 4.3? The left-hand panel of Figure 4.3 provides a graph\n",
    "ical illustration of this apparent paradox. The orange and blue solid lines\n",
    " show the average default rates for students and non-students, respectively,\n",
    " as a function of credit card balance. The negative coefficient for student in\n",
    " the multiple logistic regression indicates that for a fixed value of balance\n",
    " and income, a student is less likely to default than a non-student. Indeed,\n",
    " we observe from the left-hand panel of Figure 4.3 that the student default\n",
    " rate is at or below that of the non-student default rate for every value of\n",
    " balance. But the horizontal broken lines near the base of the plot, which\n",
    " show the default rates for students and non-students averaged over all val\n",
    "ues of balance and income, suggest the opposite effect: the overall student\n",
    " default rate is higher than the non-student default rate. Consequently, there\n",
    " is a positive coefficient for student in the single variable logistic regression\n",
    " output shown in Table 4.2.\n",
    " The right-hand panel of Figure 4.3 provides an explanation for this dis\n",
    "crepancy. The variables student and balance are correlated. Students tend\n",
    " to hold higher levels of debt, which is in turn associated with higher prob\n",
    "ability of default. In other words, students are more likely to have large\n",
    " credit card balances, which, as we know from the left-hand panel of Fig\n",
    "ure 4.3, tend to be associated with high default rates. Thus, even though\n",
    " an individual student with a given credit card balance will tend to have a\n",
    " lower probability of default than a non-student with the same credit card\n",
    " balance, the fact that students on the whole tend to have higher credit card\n",
    " balances means that overall, students tend to default at a higher rate than\n",
    " non-students. This is an important distinction for a credit card company\n",
    " that is trying to determine to whom they should offer credit. A student is\n",
    " riskier than a non-student if no information about the student’s credit card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42808938",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72d94c6f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
