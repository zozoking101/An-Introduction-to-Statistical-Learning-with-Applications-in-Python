{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b282d152",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a40ef",
   "metadata": {},
   "source": [
    "This chapter covers the important topic of deep learning. At the time of\n",
    "writing (2020), deep learning is a very active area of research in the machine\n",
    "learning and artificial intelligence communities. The cornerstone of deep\n",
    "learning is the neural network.\n",
    "\n",
    "Neural networks rose to fame in the late 1980s. There was a lot of excite\n",
    "ment and a certain amount of hype associated with this approach, and they\n",
    "were the impetus for the popular Neural Information Processing Systems\n",
    "meetings (NeurIPS, formerly NIPS) held every year, typically in exotic\n",
    "places like ski resorts. This was followed by a synthesis stage, where the\n",
    "properties of neural networks were analyzed by machine learners, math\n",
    "ematicians and statisticians; algorithms were improved, and the method\n",
    "ology stabilized. Then along came SVMs, boosting, and random forests,\n",
    "and neural networks fell somewhat from favor. Part of the reason was that\n",
    "neural networks required a lot of tinkering, while the new methods were\n",
    "more automatic. Also, on many problems the new methods outperformed\n",
    "poorly-trained neural networks. This was the status quo for the first decade\n",
    "in the new millennium.\n",
    "\n",
    "All the while, though, a core group of neural-network enthusiasts were\n",
    "pushing their technology harder on ever-larger computing architectures and\n",
    "data sets. Neural networks resurfaced after 2010 with the new name deep\n",
    "learning, with new architectures, additional bells and whistles, and a string\n",
    "of success stories on some niche problems such as image and video classifi\n",
    "cation, speech and text modeling. Many in the field believe that the major\n",
    "reason for these successes is the availability of ever-larger training datasets,\n",
    "made possible by the wide-scale use of digitization in science and industry.\n",
    "\n",
    "In this chapter we discuss the basics of neural networks and deep learn\n",
    "ing, and then go into some of the specializations for specific problems, such\n",
    "as convolutional neural networks (CNNs) for image classification, and re\n",
    "current neural networks (RNNs) for time series and other sequences. We will also demonstrate these models using the Python torch package, along\n",
    "with a number of helper packages.\n",
    "\n",
    "The material in this chapter is slightly more challenging than elsewhere\n",
    "in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477b457",
   "metadata": {},
   "source": [
    "####  Single Layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fd309",
   "metadata": {},
   "source": [
    "A neural network takes an input vector of $ p $ variables $ X = (X_1, X_2, \\ldots, X_p) $ and builds a nonlinear function $ f(X) $ to predict the response $ Y $. We have built nonlinear prediction models in earlier chapters, using trees, boosting, and generalized additive models. What distinguishes neural networks from these methods is the particular structure of the model. \n",
    "\n",
    "Figure 10.1 shows a simple feed-forward neural network for modeling a quantitative response using $ p = 4 $ predictors. In the terminology of neural networks, the four features $ X_1, \\ldots, X_4 $ make up the units in the input layer. The arrows indicate that each of the inputs from the input layer feeds into each of the $ K $ hidden units (we get to pick $ K $; here we chose 5). The neural network model has hidden units in the form\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_0 + \\sum_{k=1}^K \\beta_k h_k(X) = \\beta_0 + \\sum_{k=1}^K \\beta_k g(w_{k0} + \\sum_{j=1}^p w_{kj} X_j).\n",
    "$$\n",
    "\n",
    "It is built up here in two steps. First, the $ K $ activations $ A_k $, $ k=1, \\ldots, K $, in the hidden layer are computed as functions of the input features $ X_1, \\ldots, X_p $,\n",
    "\n",
    "$$\n",
    "A_k = h_k(X) = g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} X_j\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204539b",
   "metadata": {},
   "source": [
    "where $ g(z) $ is a nonlinear activation function that is specified in advance. We can think of each $ A_k $ as a different transformation $ h_k(X) $ of the original features, much like the basis functions of Chapter 7. These $ K $ activations from the hidden layer then feed into the output layer, resulting in\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_0 + \\sum_{k=1}^K \\beta_k A_k,\n",
    "$$\n",
    "\n",
    "a linear regression model in the $ K = 5 $ activations. All the parameters $ \\beta_0, \\ldots, \\beta_K $ and $ w_{k0}, \\ldots, w_{Kp} $ need to be estimated from data. In the early instances of neural networks, the sigmoid activation function was favored,\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{e^z}{1 + e^z} = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "which is the same function used in logistic regression to convert a linear function into probabilities between zero and one (see Figure 10.2). The preferred choice in modern neural networks is the ReLU (rectified linear unit) activation function, which takes the form\n",
    "\n",
    "$$\n",
    "g(z) = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } z < 0 \\\\ \n",
    "z & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A ReLU activation can be computed and stored more efficiently than a sigmoid activation. Although it thresholds at zero, because we apply it to a linear function (10.2), the constant term $ w_{k0} $ will shift this inflection point. \n",
    "\n",
    "So in words, the model depicted in Figure 10.1 derives five new features by computing five different linear combinations of $ X $, and then squashes each through an activation function $ g(\\cdot) $ to transform it. The final model is linear in these derived variables.\n",
    "\n",
    "The name neural network originally derived from thinking of these hidden units as analogous to neurons in the brain — values of the activations $ A_k = h_k(X) $ close to one are firing, while those close to zero are silent (using the sigmoid activation function).\n",
    "\n",
    "The nonlinearity in the activation function $ g(\\cdot) $ is essential, since without it the model $ f(X) $ in (10.1) would collapse into a simple linear model in $X_1, \\ldots, X_p$. Moreover, having a nonlinear activation function allows the model to capture complex nonlinearities and interaction effects. Consider a very simple example with $ p = 2 $ input variables $ X = (X_1, X_2) $, and $ K = 2 $ hidden units $ h_1(X) $ and $ h_2(X) $ with $ g(z) = z^2 $. We specify the other parameters as\n",
    "\n",
    "$$\n",
    "\\beta_0 = 0, \\quad \\beta_1 = \\frac{1}{4}, \\quad \\beta_2 = \\frac{1}{4}, \n",
    "$$\n",
    "$$\n",
    "w_{10} = 0, \\quad w_{11} = 1, \\quad w_{12} = 1,\n",
    "$$\n",
    "$$\n",
    "w_{20} = 0, \\quad w_{21} = 1, \\quad w_{22} = 1.\n",
    "$$\n",
    "\n",
    "From (10.2), this means that\n",
    "\n",
    "$$\n",
    "h_1(X) = (0 + X_1 + X_2)^2,\n",
    "$$\n",
    "$$\n",
    "h_2(X) = (0 + X_1 X_2)^2.\n",
    "$$\n",
    "\n",
    "Then plugging (10.7) into (10.1), we get\n",
    "\n",
    "$$\n",
    "f(X) = 0 + \\frac{1}{4} \\cdot (0 + X_1 + X_2)^2 - \\frac{1}{4} \\cdot (0 + X_1 X_2)^2\n",
    "= \\frac{1}{4} (X_1 + X_2)^2 - (X_1 X_2)^2\n",
    "= X_1 X_2.\n",
    "$$\n",
    "\n",
    "So the sum of two nonlinear transformations of linear functions can give us an interaction! In practice, we would not use a quadratic function for $ g(z) $, since we would always get a second-degree polynomial in the original coordinates $ X_1, \\ldots, X_p $. The sigmoid or ReLU activations do not have such a limitation.\n",
    "\n",
    "Fitting a neural network requires estimating the unknown parameters in (10.1). For a quantitative response, typically squared-error loss is used, so that the parameters are chosen to minimize\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (y_i - f(x_i))^2.\n",
    "$$\n",
    "\n",
    "Details about how to perform this minimization are provided in Section 10.7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718049c",
   "metadata": {},
   "source": [
    "####  Multilayer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2ca3c",
   "metadata": {},
   "source": [
    "Modern neural networks typically have more than one hidden layer, and often many units per layer. In theory a single hidden layer with a large number of units has the ability to approximate most functions. However, the learning task of discovering a good solution is made much easier with multiple layers each of modest size.\n",
    "\n",
    "We will illustrate a large dense network on the famous and publicly available MNIST handwritten digit dataset. Figure 10.3 shows examples of these digits. The idea is to build a model to classify the images into their correct digit class $ 0-9 $. Every image has $ p = 28 \\times 28 = 784 $ pixels, each of which is an eight-bit grayscale value between $ 0 $ and $ 255 $ representing the relative amount of the written digit in that tiny square. These pixels are stored in the input vector $ X $ (in, say, column order). The output is the class label, represented by a vector $ Y = (Y_0, Y_1, \\ldots, Y_9) $ of 10 dummy variables, with a one in the position corresponding to the label, and zeros elsewhere. In the machine learning community, this is known as one-hot encoding. There are 60,000 training images, and 10,000 test images.\n",
    "\n",
    "On a historical note, digit recognition problems were the catalyst that accelerated the development of neural network technology in the late 1980s at AT&T Bell Laboratories and elsewhere. Pattern recognition tasks of this kind are relatively simple for humans. Our visual system occupies a large fraction of our brains, and good recognition is an evolutionary force for survival. These tasks are not so simple for machines, and it has taken more than 30 years to refine the neural-network architectures to match human performance.\n",
    "\n",
    "Figure 10.4 shows a multilayer network architecture that works well for solving the digit-classification task. It differs from Figure 10.1 in several ways:\n",
    "\n",
    "- It has two hidden layers $ L_1 $ (256 units) and $ L_2 $ (128 units) rather than one. Later we will see a network with seven hidden layers.\n",
    "- It has ten output variables, rather than one. In this case, the ten variables really represent a single qualitative variable and so are quite dependent. (We have indexed them by the digit class $ 0 $–$ 9 $ rather than $ 1 $–$ 10 $, for clarity.) More generally, in multi-task learning one can predict different responses simultaneously with a single network; they all have a say in the formation of the hidden layers.\n",
    "- The loss function used for training the network is tailored for the the relative amount of the written digit in that tiny square. These pixels are stored in the input vector $ X $ (in, say, column order). The output is the class label, represented by a vector $ Y = (Y_0, Y_1, \\ldots, Y_9) $ of 10 dummy variables, with a one in the position corresponding to the label, and zeros elsewhere. In the machine learning community, this is known as one-hot encoding. There are 60,000 training images, and 10,000 test images.\n",
    "\n",
    "On a historical note, digit recognition problems were the catalyst that accelerated the development of neural network technology in the late 1980s at AT&T Bell Laboratories and elsewhere. Pattern recognition tasks of this kind are relatively simple for humans. Our visual system occupies a large fraction of our brains, and good recognition is an evolutionary force for survival. These tasks are not so simple for machines, and it has taken more than 30 years to refine the neural-network architectures to match human performance.\n",
    "\n",
    "Figure 10.4 shows a multilayer network architecture that works well for solving the digit-classification task. It differs from Figure 10.1 in several ways:\n",
    "\n",
    "- It has two hidden layers $ L_1 $ (256 units) and $ L_2 $ (128 units) rather than one. Later we will see a network with seven hidden layers.\n",
    "- It has ten output variables, rather than one. In this case, the ten variables really represent a single qualitative variable and so are quite dependent. (We have indexed them by the digit class $ 0 $–$ 9 $ rather than $ 1 $–$ 10 $, for clarity.) More generally, in multi-task learning one can predict different responses simultaneously with a single network; they all have a say in the formation of the hidden layers.\n",
    "- The loss function used for training the network is tailored for the multiclass classification task\n",
    "\n",
    "The first hidden layer is as in (10.2), with\n",
    "\n",
    "$$\n",
    "A^{(1)}_k = h^{(1)}_k(X) = g\\left(w^{(1)}_{k0} + \\sum_{j=1}^{p} w^{(1)}_{kj} X_j\\right) \n",
    "\\tag{10.10}\n",
    "$$\n",
    "\n",
    "for $ k = 1, \\ldots, K_1 $. The second hidden layer treats the activations $ A^{(1)}_k $ of the first hidden layer as inputs and computes new activations\n",
    "\n",
    "$$\n",
    "A^{(2)} = h^{(2)}(X) = g\\left(w^{(2)}_0 + \\sum_{k=1}^{K_1} w^{(2)}_k A^{(1)}_k\\right)\n",
    "\\tag{10.11}\n",
    "$$\n",
    "\n",
    "for $ \\ell = 1, \\ldots, K_2 $. Notice that each of the activations in the second layer $ A^{(2)} = h^{(2)}(X) $ is a function of the input vector $ X $. This is the case because while they are explicitly a function of the activations $ A^{(1)}_k $ from layer $ L_1 $, these in turn are functions of $ X $. This would also be the case with more hidden layers. Thus, through a chain of transformations, the network is able to build up fairly complex transformations of $ X $ that ultimately feed into the output layer as features.\n",
    "\n",
    "We have introduced additional superscript notation such as $ h^{(2)}(X) $ and $ w^{(2)}_j $ in (10.10) and (10.11) to indicate to which layer the activations and weights (coefficients) belong, in this case layer 2. The notation $ W_1 $ in Figure 10.4 represents the entire matrix of weights that feed from the input layer to the first hidden layer $ L_1 $. This matrix will have $ 785 \\times 256 = 200,960 $ elements; there are 785 rather than 784 because we must account for the intercept or bias term.\n",
    "\n",
    "Each element $ A^{(1)}_k $ feeds to the second hidden layer $ L_2 $ via the matrix of weights $ W_2 $ of dimension $ 257 \\times 128 = 32,896 $.\n",
    "\n",
    "We now get to the output layer, where we now have ten responses rather than one. The first step is to compute ten different linear models similar to our single model (10.1),\n",
    "\n",
    "$$\n",
    "Z_m = m_0 + \\sum_{k=1}^{K_2} m h^{(2)}(X) = m_0 + \\sum_{k=1}^{K_2} m A^{(2)},\n",
    "\\tag{10.12}\n",
    "$$\n",
    "\n",
    "for $ m = 0, 1, \\ldots, 9 $. The matrix $ B $ stores all $ 129 \\times 10 = 1,290 $ of these weights.\n",
    "\n",
    "If these were all separate quantitative responses, we would simply set each $ f_m(X) = Z_m $ and be done. However, we would like our estimates to represent class probabilities $ f_m(X) = Pr(Y = m | X) $, just like in multinomial logistic regression in Section 4.3.5. So we use the special softmax activation function (see (4.13) on page 145),\n",
    "\n",
    "$$\n",
    "f_m(X) = Pr(Y = m | X) = \\frac{e^{Z_m}}{\\sum_{j=0}^{9} e^{Z_j}},\n",
    "\\tag{10.13}\n",
    "$$\n",
    "\n",
    "for $ m = 0, 1, \\ldots, 9 $. This ensures that the 10 numbers behave like probabilities (non-negative and sum to one). Even though the goal is to build a classifier, our model actually estimates a probability for each of the 10 classes. The classifier then assigns the image to the class with the highest probability.\n",
    "\n",
    "To train this network, since the response is qualitative, we look for coefficient estimates that minimize the negative multinomial log-likelihood \n",
    "\n",
    "$$\n",
    "-\\sum_{i=1}^{n} \\sum_{m=0}^{9} y_{im} \\log(f_m(x_i)),\n",
    "\\tag{10.14}\n",
    "$$\n",
    "\n",
    "also known as the cross-entropy. This is a generalization of the criterion (4.5) for two-class logistic regression. Details on how to minimize this objective are given in Section 10.7. If the response were quantitative, we would instead minimize squared-error loss as in (10.9).\n",
    "\n",
    "Table 10.1 compares the test performance of the neural network with two simple models presented in Chapter 4 that make use of linear decision boundaries: multinomial logistic regression and linear discriminant analysis. The improvement of neural networks over both of these linear methods is dramatic: the network with dropout regularization achieves a test error rate below 2% on the 10,000 test images. (We describe dropout regularization in Section 10.7.3.) In Section 10.9.2 of the lab, we present the code for fitting this model, which runs in just over two minutes on a laptop computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ba2e6",
   "metadata": {},
   "source": [
    "| $Method$                                         | $Test$ $Error$ |\n",
    "|------------------------------------------------|------------|\n",
    "| Neural Network + Ridge Regularization          | 2.3%      |\n",
    "| Neural Network + Dropout Regularization        | 1.8%      |\n",
    "| Multinomial Logistic Regression                 | 7.2%      |\n",
    "| Linear Discriminant Analysis                    | 12.7%     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db221e8",
   "metadata": {},
   "source": [
    "Test error rate on the MNIST data, for neural networks with two forms of regularization, as well as multinomial logistic regression and linear discriminant analysis. In this example, the extra complexity of the neural network leads to a marked improvement in test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d004c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Adding the number of coefficients in $ W_1, W_2, $ and $ B $, we get 235,146 in all, more than 33 times the number $ 785 \\times 9 = 7,065 $ needed for multinomial logistic regression. Recall that there are 60,000 images in the training set. While this might seem like a large training set, there are almost four times as many coefficients in the neural network model as there are observations in the training set! To avoid overfitting, some regularization is needed. In this example, we used two forms of regularization: ridge regularization, which is similar to ridge regression from Chapter 6, and dropout regularization. We discuss both forms of regularization in Section 10.7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061aae53",
   "metadata": {},
   "source": [
    "####  Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60f10b",
   "metadata": {},
   "source": [
    "Neural networks rebounded around 2010 with big successes in image classification. Around that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes. This database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale). Each image has a resolution of $ 32 \\times 32 $ pixels, with three eight-bit numbers per pixel representing red, green, and blue. The numbers for each image are organized in a three-dimensional array called a feature map. The first two axes are spatial (both are 32-dimensional), and the third is the channel\n",
    "axis,5 representing the three colors. There is a designated training set of\n",
    "50,000 images, and a test set of 10,000.\n",
    "\n",
    "A special family of convolutional neural networks (CNNs) has evolved for \n",
    "classifying images such as these, and has shown spectacular success on a\n",
    "wide range of problems. CNNs mimic to some degree how humans classify\n",
    "images, by recognizing specific features or patterns anywhere in the image\n",
    "that distinguish each particular object class. In this section we give a brief\n",
    "overview of how they work.\n",
    "\n",
    "Figure 10.6 illustrates the idea behind a convolutional neural network on\n",
    "a cartoon image of a tiger.6\n",
    "\n",
    "The network first identifies low-level features in the input image, such\n",
    "as small edges, patches of color, and the like. These low-level features are\n",
    "then combined to form higher-level features, such as parts of ears, eyes,\n",
    "and so on. Eventually, the presence or absence of these higher-level features\n",
    "contributes to the probability of any given output class.\n",
    "\n",
    " How does a convolutional neural network build up this hierarchy? It com\n",
    "bines two specialized types of hidden layers, called convolution layers and\n",
    "pooling layers. Convolution layers search for instances of small patterns in\n",
    "the image, whereas pooling layers downsample these to select a prominent\n",
    "subset. In order to achieve state-of-the-art results, contemporary neural\n",
    "network architectures make use of many convolution and pooling layers.\n",
    "We describe convolution and pooling layers next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5321c94",
   "metadata": {},
   "source": [
    "##### Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4fd1a",
   "metadata": {},
   "source": [
    "A convolution layer is made up of a large number of convolution filters, each"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
