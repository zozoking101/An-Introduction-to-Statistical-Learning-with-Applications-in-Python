{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b282d152",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a40ef",
   "metadata": {},
   "source": [
    "This chapter covers the important topic of deep learning. At the time of\n",
    "writing (2020), deep learning is a very active area of research in the machine\n",
    "learning and artificial intelligence communities. The cornerstone of deep\n",
    "learning is the neural network.\n",
    "\n",
    "Neural networks rose to fame in the late 1980s. There was a lot of excite\n",
    "ment and a certain amount of hype associated with this approach, and they\n",
    "were the impetus for the popular Neural Information Processing Systems\n",
    "meetings (NeurIPS, formerly NIPS) held every year, typically in exotic\n",
    "places like ski resorts. This was followed by a synthesis stage, where the\n",
    "properties of neural networks were analyzed by machine learners, math\n",
    "ematicians and statisticians; algorithms were improved, and the method\n",
    "ology stabilized. Then along came SVMs, boosting, and random forests,\n",
    "and neural networks fell somewhat from favor. Part of the reason was that\n",
    "neural networks required a lot of tinkering, while the new methods were\n",
    "more automatic. Also, on many problems the new methods outperformed\n",
    "poorly-trained neural networks. This was the status quo for the first decade\n",
    "in the new millennium.\n",
    "\n",
    "All the while, though, a core group of neural-network enthusiasts were\n",
    "pushing their technology harder on ever-larger computing architectures and\n",
    "data sets. Neural networks resurfaced after 2010 with the new name deep\n",
    "learning, with new architectures, additional bells and whistles, and a string\n",
    "of success stories on some niche problems such as image and video classifi\n",
    "cation, speech and text modeling. Many in the field believe that the major\n",
    "reason for these successes is the availability of ever-larger training datasets,\n",
    "made possible by the wide-scale use of digitization in science and industry.\n",
    "\n",
    "In this chapter we discuss the basics of neural networks and deep learn\n",
    "ing, and then go into some of the specializations for specific problems, such\n",
    "as convolutional neural networks (CNNs) for image classification, and re\n",
    "current neural networks (RNNs) for time series and other sequences. We will also demonstrate these models using the Python torch package, along\n",
    "with a number of helper packages.\n",
    "\n",
    "The material in this chapter is slightly more challenging than elsewhere\n",
    "in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477b457",
   "metadata": {},
   "source": [
    "####  Single Layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fd309",
   "metadata": {},
   "source": [
    "A neural network takes an input vector of $ p $ variables $ X = (X_1, X_2, \\ldots, X_p) $ and builds a nonlinear function $ f(X) $ to predict the response $ Y $. We have built nonlinear prediction models in earlier chapters, using trees, boosting, and generalized additive models. What distinguishes neural networks from these methods is the particular structure of the model. \n",
    "\n",
    "Figure 10.1 shows a simple feed-forward neural network for modeling a quantitative response using $ p = 4 $ predictors. In the terminology of neural networks, the four features $ X_1, \\ldots, X_4 $ make up the units in the input layer. The arrows indicate that each of the inputs from the input layer feeds into each of the $ K $ hidden units (we get to pick $ K $; here we chose 5). The neural network model has hidden units in the form\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_0 + \\sum_{k=1}^K \\beta_k h_k(X) = \\beta_0 + \\sum_{k=1}^K \\beta_k g(w_{k0} + \\sum_{j=1}^p w_{kj} X_j).\n",
    "$$\n",
    "\n",
    "It is built up here in two steps. First, the $ K $ activations $ A_k $, $ k=1, \\ldots, K $, in the hidden layer are computed as functions of the input features $ X_1, \\ldots, X_p $,\n",
    "\n",
    "$$\n",
    "A_k = h_k(X) = g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} X_j\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204539b",
   "metadata": {},
   "source": [
    "where $ g(z) $ is a nonlinear activation function that is specified in advance. We can think of each $ A_k $ as a different transformation $ h_k(X) $ of the original features, much like the basis functions of Chapter 7. These $ K $ activations from the hidden layer then feed into the output layer, resulting in\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_0 + \\sum_{k=1}^K \\beta_k A_k,\n",
    "$$\n",
    "\n",
    "a linear regression model in the $ K = 5 $ activations. All the parameters $ \\beta_0, \\ldots, \\beta_K $ and $ w_{k0}, \\ldots, w_{Kp} $ need to be estimated from data. In the early instances of neural networks, the sigmoid activation function was favored,\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{e^z}{1 + e^z} = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "which is the same function used in logistic regression to convert a linear function into probabilities between zero and one (see Figure 10.2). The preferred choice in modern neural networks is the ReLU (rectified linear unit) activation function, which takes the form\n",
    "\n",
    "$$\n",
    "g(z) = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } z < 0 \\\\ \n",
    "z & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A ReLU activation can be computed and stored more efficiently than a sigmoid activation. Although it thresholds at zero, because we apply it to a linear function (10.2), the constant term $ w_{k0} $ will shift this inflection point. \n",
    "\n",
    "So in words, the model depicted in Figure 10.1 derives five new features by computing five different linear combinations of $ X $, and then squashes each through an activation function $ g(\\cdot) $ to transform it. The final model is linear in these derived variables.\n",
    "\n",
    "The name neural network originally derived from thinking of these hidden units as analogous to neurons in the brain — values of the activations $ A_k = h_k(X) $ close to one are firing, while those close to zero are silent (using the sigmoid activation function).\n",
    "\n",
    "The nonlinearity in the activation function $ g(\\cdot) $ is essential, since without it the model $ f(X) $ in (10.1) would collapse into a simple linear model in $X_1, \\ldots, X_p$. Moreover, having a nonlinear activation function allows the model to capture complex nonlinearities and interaction effects. Consider a very simple example with $ p = 2 $ input variables $ X = (X_1, X_2) $, and $ K = 2 $ hidden units $ h_1(X) $ and $ h_2(X) $ with $ g(z) = z^2 $. We specify the other parameters as\n",
    "\n",
    "$$\n",
    "\\beta_0 = 0, \\quad \\beta_1 = \\frac{1}{4}, \\quad \\beta_2 = \\frac{1}{4}, \n",
    "$$\n",
    "$$\n",
    "w_{10} = 0, \\quad w_{11} = 1, \\quad w_{12} = 1,\n",
    "$$\n",
    "$$\n",
    "w_{20} = 0, \\quad w_{21} = 1, \\quad w_{22} = 1.\n",
    "$$\n",
    "\n",
    "From (10.2), this means that\n",
    "\n",
    "$$\n",
    "h_1(X) = (0 + X_1 + X_2)^2,\n",
    "$$\n",
    "$$\n",
    "h_2(X) = (0 + X_1 X_2)^2.\n",
    "$$\n",
    "\n",
    "Then plugging (10.7) into (10.1), we get\n",
    "\n",
    "$$\n",
    "f(X) = 0 + \\frac{1}{4} \\cdot (0 + X_1 + X_2)^2 - \\frac{1}{4} \\cdot (0 + X_1 X_2)^2\n",
    "= \\frac{1}{4} (X_1 + X_2)^2 - (X_1 X_2)^2\n",
    "= X_1 X_2.\n",
    "$$\n",
    "\n",
    "So the sum of two nonlinear transformations of linear functions can give us an interaction! In practice, we would not use a quadratic function for $ g(z) $, since we would always get a second-degree polynomial in the original coordinates $ X_1, \\ldots, X_p $. The sigmoid or ReLU activations do not have such a limitation.\n",
    "\n",
    "Fitting a neural network requires estimating the unknown parameters in (10.1). For a quantitative response, typically squared-error loss is used, so that the parameters are chosen to minimize\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (y_i - f(x_i))^2.\n",
    "$$\n",
    "\n",
    "Details about how to perform this minimization are provided in Section 10.7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718049c",
   "metadata": {},
   "source": [
    "####  Multilayer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2ca3c",
   "metadata": {},
   "source": [
    "Modern neural networks typically have more than one hidden layer, and often many units per layer. In theory a single hidden layer with a large number of units has the ability to approximate most functions. However, the learning task of discovering a good solution is made much easier with multiple layers each of modest size.\n",
    "\n",
    "We will illustrate a large dense network on the famous and publicly available MNIST handwritten digit dataset. Figure 10.3 shows examples of these digits. The idea is to build a model to classify the images into their correct digit class $ 0-9 $. Every image has $ p = 28 \\times 28 = 784 $ pixels, each of which is an eight-bit grayscale value between $ 0 $ and $ 255 $ representing the relative amount of the written digit in that tiny square. These pixels are stored in the input vector $ X $ (in, say, column order). The output is the class label, represented by a vector $ Y = (Y_0, Y_1, \\ldots, Y_9) $ of 10 dummy variables, with a one in the position corresponding to the label, and zeros elsewhere. In the machine learning community, this is known as one-hot encoding. There are 60,000 training images, and 10,000 test images.\n",
    "\n",
    "On a historical note, digit recognition problems were the catalyst that accelerated the development of neural network technology in the late 1980s at AT&T Bell Laboratories and elsewhere. Pattern recognition tasks of this kind are relatively simple for humans. Our visual system occupies a large fraction of our brains, and good recognition is an evolutionary force for survival. These tasks are not so simple for machines, and it has taken more than 30 years to refine the neural-network architectures to match human performance.\n",
    "\n",
    "Figure 10.4 shows a multilayer network architecture that works well for solving the digit-classification task. It differs from Figure 10.1 in several ways:\n",
    "\n",
    "- It has two hidden layers $ L_1 $ (256 units) and $ L_2 $ (128 units) rather than one. Later we will see a network with seven hidden layers.\n",
    "- It has ten output variables, rather than one. In this case, the ten variables really represent a single qualitative variable and so are quite dependent. (We have indexed them by the digit class $ 0 $–$ 9 $ rather than $ 1 $–$ 10 $, for clarity.) More generally, in multi-task learning one can predict different responses simultaneously with a single network; they all have a say in the formation of the hidden layers.\n",
    "- The loss function used for training the network is tailored for the the relative amount of the written digit in that tiny square. These pixels are stored in the input vector $ X $ (in, say, column order). The output is the class label, represented by a vector $ Y = (Y_0, Y_1, \\ldots, Y_9) $ of 10 dummy variables, with a one in the position corresponding to the label, and zeros elsewhere. In the machine learning community, this is known as one-hot encoding. There are 60,000 training images, and 10,000 test images.\n",
    "\n",
    "On a historical note, digit recognition problems were the catalyst that accelerated the development of neural network technology in the late 1980s at AT&T Bell Laboratories and elsewhere. Pattern recognition tasks of this kind are relatively simple for humans. Our visual system occupies a large fraction of our brains, and good recognition is an evolutionary force for survival. These tasks are not so simple for machines, and it has taken more than 30 years to refine the neural-network architectures to match human performance.\n",
    "\n",
    "Figure 10.4 shows a multilayer network architecture that works well for solving the digit-classification task. It differs from Figure 10.1 in several ways:\n",
    "\n",
    "- It has two hidden layers $ L_1 $ (256 units) and $ L_2 $ (128 units) rather than one. Later we will see a network with seven hidden layers.\n",
    "- It has ten output variables, rather than one. In this case, the ten variables really represent a single qualitative variable and so are quite dependent. (We have indexed them by the digit class $ 0 $–$ 9 $ rather than $ 1 $–$ 10 $, for clarity.) More generally, in multi-task learning one can predict different responses simultaneously with a single network; they all have a say in the formation of the hidden layers.\n",
    "- The loss function used for training the network is tailored for the multiclass classification task\n",
    "\n",
    "The first hidden layer is as in (10.2), with\n",
    "\n",
    "$$\n",
    "A^{(1)}_k = h^{(1)}_k(X) = g\\left(w^{(1)}_{k0} + \\sum_{j=1}^{p} w^{(1)}_{kj} X_j\\right) \n",
    "\\tag{10.10}\n",
    "$$\n",
    "\n",
    "for $ k = 1, \\ldots, K_1 $. The second hidden layer treats the activations $ A^{(1)}_k $ of the first hidden layer as inputs and computes new activations\n",
    "\n",
    "$$\n",
    "A^{(2)} = h^{(2)}(X) = g\\left(w^{(2)}_0 + \\sum_{k=1}^{K_1} w^{(2)}_k A^{(1)}_k\\right)\n",
    "\\tag{10.11}\n",
    "$$\n",
    "\n",
    "for $ \\ell = 1, \\ldots, K_2 $. Notice that each of the activations in the second layer $ A^{(2)} = h^{(2)}(X) $ is a function of the input vector $ X $. This is the case because while they are explicitly a function of the activations $ A^{(1)}_k $ from layer $ L_1 $, these in turn are functions of $ X $. This would also be the case with more hidden layers. Thus, through a chain of transformations, the network is able to build up fairly complex transformations of $ X $ that ultimately feed into the output layer as features.\n",
    "\n",
    "We have introduced additional superscript notation such as $ h^{(2)}(X) $ and $ w^{(2)}_j $ in (10.10) and (10.11) to indicate to which layer the activations and weights (coefficients) belong, in this case layer 2. The notation $ W_1 $ in Figure 10.4 represents the entire matrix of weights that feed from the input layer to the first hidden layer $ L_1 $. This matrix will have $ 785 \\times 256 = 200,960 $ elements; there are 785 rather than 784 because we must account for the intercept or bias term.\n",
    "\n",
    "Each element $ A^{(1)}_k $ feeds to the second hidden layer $ L_2 $ via the matrix of weights $ W_2 $ of dimension $ 257 \\times 128 = 32,896 $.\n",
    "\n",
    "We now get to the output layer, where we now have ten responses rather than one. The first step is to compute ten different linear models similar to our single model (10.1),\n",
    "\n",
    "$$\n",
    "Z_m = m_0 + \\sum_{k=1}^{K_2} m h^{(2)}(X) = m_0 + \\sum_{k=1}^{K_2} m A^{(2)},\n",
    "\\tag{10.12}\n",
    "$$\n",
    "\n",
    "for $ m = 0, 1, \\ldots, 9 $. The matrix $ B $ stores all $ 129 \\times 10 = 1,290 $ of these weights.\n",
    "\n",
    "If these were all separate quantitative responses, we would simply set each $ f_m(X) = Z_m $ and be done. However, we would like our estimates to represent class probabilities $ f_m(X) = Pr(Y = m | X) $, just like in multinomial logistic regression in Section 4.3.5. So we use the special softmax activation function (see (4.13) on page 145),\n",
    "\n",
    "$$\n",
    "f_m(X) = Pr(Y = m | X) = \\frac{e^{Z_m}}{\\sum_{j=0}^{9} e^{Z_j}},\n",
    "\\tag{10.13}\n",
    "$$\n",
    "\n",
    "for $ m = 0, 1, \\ldots, 9 $. This ensures that the 10 numbers behave like probabilities (non-negative and sum to one). Even though the goal is to build a classifier, our model actually estimates a probability for each of the 10 classes. The classifier then assigns the image to the class with the highest probability.\n",
    "\n",
    "To train this network, since the response is qualitative, we look for coefficient estimates that minimize the negative multinomial log-likelihood \n",
    "\n",
    "$$\n",
    "-\\sum_{i=1}^{n} \\sum_{m=0}^{9} y_{im} \\log(f_m(x_i)),\n",
    "\\tag{10.14}\n",
    "$$\n",
    "\n",
    "also known as the cross-entropy. This is a generalization of the criterion (4.5) for two-class logistic regression. Details on how to minimize this objective are given in Section 10.7. If the response were quantitative, we would instead minimize squared-error loss as in (10.9).\n",
    "\n",
    "Table 10.1 compares the test performance of the neural network with two simple models presented in Chapter 4 that make use of linear decision boundaries: multinomial logistic regression and linear discriminant analysis. The improvement of neural networks over both of these linear methods is dramatic: the network with dropout regularization achieves a test error rate below 2% on the 10,000 test images. (We describe dropout regularization in Section 10.7.3.) In Section 10.9.2 of the lab, we present the code for fitting this model, which runs in just over two minutes on a laptop computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ba2e6",
   "metadata": {},
   "source": [
    "| $Method$                                         | $Test$ $Error$ |\n",
    "|------------------------------------------------|------------|\n",
    "| Neural Network + Ridge Regularization          | 2.3%      |\n",
    "| Neural Network + Dropout Regularization        | 1.8%      |\n",
    "| Multinomial Logistic Regression                 | 7.2%      |\n",
    "| Linear Discriminant Analysis                    | 12.7%     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db221e8",
   "metadata": {},
   "source": [
    "Test error rate on the MNIST data, for neural networks with two forms of regularization, as well as multinomial logistic regression and linear discriminant analysis. In this example, the extra complexity of the neural network leads to a marked improvement in test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d004c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Adding the number of coefficients in $ W_1, W_2, $ and $ B $, we get 235,146 in all, more than 33 times the number $ 785 \\times 9 = 7,065 $ needed for multinomial logistic regression. Recall that there are 60,000 images in the training set. While this might seem like a large training set, there are almost four times as many coefficients in the neural network model as there are observations in the training set! To avoid overfitting, some regularization is needed. In this example, we used two forms of regularization: ridge regularization, which is similar to ridge regression from Chapter 6, and dropout regularization. We discuss both forms of regularization in Section 10.7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061aae53",
   "metadata": {},
   "source": [
    "####  Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60f10b",
   "metadata": {},
   "source": [
    "Neural networks rebounded around 2010 with big successes in image classification. Around that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes. This database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale). Each image has a resolution of $ 32 \\times 32 $ pixels, with three eight-bit numbers per pixel representing red, green, and blue. The numbers for each image are organized in a three-dimensional array called a feature map. The first two axes are spatial (both are 32-dimensional), and the third is the channel\n",
    "axis,5 representing the three colors. There is a designated training set of\n",
    "50,000 images, and a test set of 10,000.\n",
    "\n",
    "A special family of convolutional neural networks (CNNs) has evolved for \n",
    "classifying images such as these, and has shown spectacular success on a\n",
    "wide range of problems. CNNs mimic to some degree how humans classify\n",
    "images, by recognizing specific features or patterns anywhere in the image\n",
    "that distinguish each particular object class. In this section we give a brief\n",
    "overview of how they work.\n",
    "\n",
    "Figure 10.6 illustrates the idea behind a convolutional neural network on\n",
    "a cartoon image of a tiger.6\n",
    "\n",
    "The network first identifies low-level features in the input image, such\n",
    "as small edges, patches of color, and the like. These low-level features are\n",
    "then combined to form higher-level features, such as parts of ears, eyes,\n",
    "and so on. Eventually, the presence or absence of these higher-level features\n",
    "contributes to the probability of any given output class.\n",
    "\n",
    " How does a convolutional neural network build up this hierarchy? It com\n",
    "bines two specialized types of hidden layers, called convolution layers and\n",
    "pooling layers. Convolution layers search for instances of small patterns in\n",
    "the image, whereas pooling layers downsample these to select a prominent\n",
    "subset. In order to achieve state-of-the-art results, contemporary neural\n",
    "network architectures make use of many convolution and pooling layers.\n",
    "We describe convolution and pooling layers next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5321c94",
   "metadata": {},
   "source": [
    "##### Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4fd1a",
   "metadata": {},
   "source": [
    "A convolution layer is made up of a large number of convolution filters, each of which is a template that determines whether a particular local feature is\n",
    "present in an image. A convolution filter relies on a very simple operation,\n",
    "called a convolution, which basically amounts to repeatedly multiplying\n",
    "matrix elements and then adding the results.\n",
    "\n",
    "To understand how a convolution filter works, consider a very simple\n",
    "example of a 4 3 image:\n",
    "\n",
    "$$\\text{Original Image} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ j & k & l \\end{bmatrix} \\quad $$\n",
    "\n",
    "Now consider a 2 $\\times$ 2 filter of the form\n",
    "\n",
    "$$\\text{Convolution Filter} = \\begin{bmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "When we convolve the image with the filter, we get the result:\n",
    "\n",
    "$$\\text{Convolved Image} = \\begin{bmatrix} (a\\alpha + b\\beta + d\\gamma + e\\delta) & (b\\alpha + c\\beta + e\\gamma + f\\delta) \\\\ (d\\alpha + e\\beta + g\\gamma + h\\delta) & (e\\alpha + f\\beta + h\\gamma + i\\delta) \\\\ (g\\alpha + h\\beta + j\\gamma + k\\delta) & (h\\alpha + i\\beta + k\\gamma + l\\delta) \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "For instance, the top-left element comes from multiplying each element in the $2 \\times 2$ filter by the corresponding element in the top-left $2 \\times 2$ portion of the image, and adding the results. The other elements are obtained in a similar way: the convolution filter is applied to every $2 \\times 2$ submatrix of the original image in order to obtain the convolved image. If a $2 \\times 2$ submatrix of the original image resembles the convolution filter, then it will have a large value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that resemble the convolution filter. We have used $2 \\times 2$ as an example; in general, convolution filters are small $l_1 \\times l_2$ arrays, with $l_1$ and $l_2$ small positive integers that are not necessarily equal.\n",
    "\n",
    "Figure 10.7 illustrates the application of two convolution filters to a $192 \\times 179$ image of a tiger, shown on the left-hand side. Each convolution filter is a $15 \\times 15$ image containing mostly zeros (black), with a narrow strip of ones (white) oriented either vertically or horizontally within the image. When each filter is convolved with the image of the tiger, areas of the tiger that resemble the filter (i.e., that have either horizontal or vertical stripes or edges) are given large values, and areas of the tiger that do not resemble the feature are given small values. The convolved images are displayed on the right-hand side. We see that the horizontal stripe filter picks out horizontal stripes and edges in the original image, whereas the vertical stripe filter picks out vertical stripes and edges in the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce89f1",
   "metadata": {},
   "source": [
    "We have used a large image and two large filters in Figure 10.7 for illus\n",
    "tration. For the CIFAR100 database there are 32 32 color pixels per image,\n",
    "and we use 3 3 convolution filters.\n",
    "\n",
    "In a convolution layer, we use a whole bank of filters to pick out a variety\n",
    "of differently-oriented edges and shapes in the image. Using predefined\n",
    "filters in this way is standard practice in image processing. By contrast,\n",
    "with CNNs the filters are learned for the specific classification task. We can\n",
    "think of the filter weights as the parameters going from an input layer to a\n",
    "hidden layer, with one hidden unit for each pixel in the convolved image.\n",
    "This is in fact the case, though the parameters are highly structured and\n",
    "constrained (see Exercise 4 for more details). They operate on localized\n",
    "patches in the input image (so there are many structural zeros), and the\n",
    "same weights in a given filter are reused for all possible patches in the image\n",
    "(so the weights are constrained).9\n",
    "\n",
    "We now give some additional details.\n",
    "\n",
    "- Since the input image is in color, it has three channels represented\n",
    "by a three-dimensional feature map (array). Each channel is a two\n",
    "dimensional (32 32) feature map — one for red, one for green, and\n",
    "one for blue. A single convolution filter will also have three channels,\n",
    "one per color, each of dimension 3 3, with potentially different filter\n",
    "weights. The results of the three convolutions are summed to form\n",
    "a two-dimensional output feature map. Note that at this point the\n",
    "color information has been used, and is not passed on to subsequent\n",
    "layers except through its role in the convolution.\n",
    "\n",
    "- If we use K different convolution filters at this first hidden layer, we get K two-dimensional output feature maps, which together are treated as a single three-dimensional feature map. We view each of the K output feature maps as a separate channel of information, so now we have K channels in contrast to the three color channels of the original input feature map. The three-dimensional feature map is just like the activations in a hidden layer of a simple neural network, except organized and produced in a spatially structured way.\n",
    "\n",
    "- We typically apply the ReLU activation function (10.5) to the convolved image. This step is sometimes viewed as a separate layer in the convolutional neural network, in which case it is referred to as a detector layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005aaa49",
   "metadata": {},
   "source": [
    "##### Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3534b60",
   "metadata": {},
   "source": [
    "A pooling layer provides a way to condense a large image into a smaller summary image. While there are a number of possible ways to perform pooling, the max pooling operation summarizes each non-overlapping 2 × 2 block of pixels in an image using the maximum value in the block. This reduces the size of the image by a factor of two in each direction, and it also provides some location invariance: i.e. as long as there is a large value in one of the four pixels in the block, the whole block registers as a large value in the reduced image.\n",
    "\n",
    "Here is a simple example of max pooling:\n",
    "\n",
    "$$\\text{Max pool} = \\begin{bmatrix} 1 & 2 & 5 & 3 \\\\ 3 & 0 & 1 & 2 \\\\ 2 & 1 & 3 & 4 \\\\ 1 & 1 & 2 & 0\\end{bmatrix}\\quad \\to    \\begin{bmatrix} 3 & 5 \\\\ 2 & 4\\end{bmatrix} \\quad $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7b302",
   "metadata": {},
   "source": [
    "\n",
    "##### Architecture of a Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d6991",
   "metadata": {},
   "source": [
    "\n",
    "So far we have defined a single convolution layer — each filter produces a new two-dimensional feature map. The number of convolution filters in a convolution layer is akin to the number of units at a particular hidden layer in a fully-connected neural network of the type we saw in Section 10.2. This number also defines the number of channels in the resulting three-dimensional feature map. We have also described a pooling layer, which reduces the first two dimensions of each three-dimensional feature map. Deep CNNs have many such layers. Figure 10.8 shows a typical architecture for a CNN for the CIFAR100 image classification task.\n",
    "\n",
    "At the input layer, we see the three-dimensional feature map of a color image, where the channel axis represents each color by a 32 × 32 two-dimensional feature map of pixels. Each convolution filter produces a new channel at the first hidden layer, each of which is a 32 × 32 feature map (after some padding at the edges). After this first round of convolutions, we now have a new “image”; a feature map with considerably more channels than the three color input channels (six in the figure, since we used six convolution filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c71ab",
   "metadata": {},
   "source": [
    "This is followed by a max-pool layer, which reduces the size of the feature map in each channel by a factor of four: two in each dimension. This convolve-then-pool sequence is now repeated for the next two layers. Some details are as follows:\n",
    "\n",
    "- Each subsequent convolve layer is similar to the first. It takes as input the three-dimensional feature map from the previous layer and treats it like a single multi-channel image. Each convolution filter learned has as many channels as this feature map.\n",
    "- Since the channel feature maps are reduced in size after each pool layer, we usually increase the number of filters in the next convolve layer to compensate.\n",
    "- Sometimes we repeat several convolve layers before a pool layer. This effectively increases the dimension of the filter.\n",
    "\n",
    "These operations are repeated until the pooling has reduced each channel feature map down to just a few pixels in each dimension. At this point the three-dimensional feature maps are flattened — the pixels are treated as separate units — and fed into one or more fully-connected layers before reaching the output layer, which is a softmax activation for the 100 classes (as in (10.13)).\n",
    "\n",
    "There are many tuning parameters to be selected in constructing such a network, apart from the number, nature, and sizes of each layer. Dropout learning can be used at each layer, as well as lasso or ridge regularization (see Section 10.7). The details of constructing a convolutional neural network can seem daunting. Fortunately, terrific software is available, with extensive examples and vignettes that provide guidance on sensible choices for the parameters. For the CIFAR100 official test set, the best accuracy as of this writing is just above 75%, but undoubtedly this performance will continue to improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513c753",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "##### Data Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5241241",
   "metadata": {},
   "source": [
    "An additional important trick used with image modeling is data augmentation. Essentially, each training image is replicated many times, with each replicate randomly distorted in a natural way such that human recognition is unaffected. Figure 10.9 shows some examples. Typical distortions are zoom, horizontal and vertical shift, shear, small rotations, and in this case horizontal flips. At face value this is a way of increasing the training set considerably with somewhat different examples, and thus protects against overfitting. In fact we can see this as a form of regularization: we build a cloud of images around each original image, all with the same label. This kind of fattening of the data is similar in spirit to ridge regularization. We will see in Section 10.7.2 that the stochastic gradient descent algorithms for fitting deep learning models repeatedly process randomly selected batches of, say, 128 training images at a time. This works hand-in-glove with augmentation, because we can distort each image in the batch on the fly, and hence do not have to store all the new images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73f0a6",
   "metadata": {},
   "source": [
    "\n",
    "##### Results Using a Pretrained Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191c3c9",
   "metadata": {},
   "source": [
    "Here we use an industry-level pretrained classifier to predict the class of some new images. The resnet50 classifier is a convolutional neural network that was trained using the imagenet data set, which consists of millions of images that belong to an ever-growing number of categories. Figure 10.10 demonstrates the performance of resnet50 on six photographs (private collection of one of the authors). The CNN does a reasonable job classifying the hawk in the second image. If we zoom out as in the third image, it gets confused and chooses the fountain rather than the hawk. In the final image a “jacamar” is a tropical bird from South and Central America with similar coloring to the South African Cape Weaver. We give more details on this example in Section 10.9.4.\n",
    "\n",
    "Much of the work in fitting a CNN is in learning the convolution filters at the hidden layers; these are the coefficients of a CNN. For models fit to massive corpora such as imagenet with many classes, the output of these filters can serve as features for general natural-image classification problems. One can use these pretrained hidden layers for new problems with much smaller training sets (a process referred to as weight freezing), and just train the last few layers of the network, which requires much less data. The vignettes and book12 that accompany the keras package give more details on such applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f76b3",
   "metadata": {},
   "source": [
    "#### Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ecdf8",
   "metadata": {},
   "source": [
    "In this section we introduce a new type of example that has important\n",
    "applications in industry and science: predicting attributes of documents.\n",
    "Examples of documents include articles in medical journals, Reuters news\n",
    "feeds, emails, tweets, and so on. Our example will be IMDb (Internet Movie\n",
    "Database) ratings — short documents where viewers have written critiques\n",
    "of movies.13 The response in this case is the sentiment of the review, which\n",
    "will be positive or negative.\n",
    "\n",
    "*Here is the beginning of a rather amusing negative review:*\n",
    "\n",
    "> *This has to be one of the worst films of the 1990s. When my friends & I were watching this film (being the target audience it was aimed at) we just sat & watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn ...*\n",
    ">\n",
    "\n",
    "Each review can be a different length, include slang or non-words, and contain spelling errors. We need to find a way to **featurize** such a document. *Featurize* is modern parlance for defining a set of predictors.\n",
    "\n",
    "The simplest and most common featurization is the bag-of-words model. bag-of-words  \n",
    "We score each document for the presence or absence of each of the words in a language dictionary — in this case an English dictionary. If the dictionary contains M words, that means for each document we create a binary feature vector of length M, and score a 1 for every word present, and 0 otherwise. That can be a very wide feature vector, so we limit the dictionary — in this case to the 10,000 most frequently occurring words in the training corpus of 25,000 reviews. Fortunately there are nice tools for doing this automatically. Here is the beginning of a positive review that has been redacted in this way:\n",
    "\n",
    "> <START> this film was just brilliant casting location scenery story direction everyone’s really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved ... \n",
    ">\n",
    "\n",
    "Here we can see many words have been omitted, and some unknown words (UNK) have been marked as such. With this reduction the binary feature vector has length 10,000, and consists mostly of 0’s and a smattering of 1’s in the positions corresponding to words that are present in the document. We have a training set and test set, each with 25,000 examples, and each balanced with regard to sentiment. The resulting training feature matrix X has dimension 25,000 × 10,000, but only 1.3% of the binary entries are nonzero. We call such a matrix sparse, because most of the values are the same (zero in this case); it can be stored efficiently in sparse matrix format.14 sparse\n",
    "\n",
    "There are a variety of ways to account for the document length; here we only score a word as in or out of the document, but for example one could instead record the relative frequency of words. We split off a validation set of size 2,000 from the 25,000 training observations (for model tuning), and fit two model sequences:\n",
    "\n",
    "- A lasso logistic regression using the glmnet package;\n",
    "- A two-class neural network with two hidden layers, each with 16 ReLU units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab587985",
   "metadata": {},
   "source": [
    "Both methods produce a sequence of solutions. The lasso sequence is indexed by the regularization parameter λ. The neural-net sequence is indexed by the number of gradient-descent iterations used in the fitting, as measured by training epochs or passes through the training set (Section 10.7). Notice that the training accuracy in Figure 10.11 (black points) increases monotonically in both cases. We can use the validation error to pick a good solution from each sequence (blue points in the plots), which would then be used to make predictions on the test data set.\n",
    "\n",
    "Note that a two-class neural network amounts to a nonlinear logistic regression model. From (10.12) and (10.13) we can see that\n",
    "$$\n",
    "\\frac{\\log \\Pr(Y = 1 \\mid X)}{\\Pr(Y = 0 \\mid X)}\n",
    "= Z_1 - Z_0 \\tag{10.15}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\beta_{10} - \\beta_{00})\n",
    "+ \\sum_{\\ell=1}^{K_2} (\\beta_{1\\ell} - \\beta_{0\\ell}) A^{(2)}_{\\ell}.\n",
    "$$\n",
    "\n",
    "(This shows the redundancy in the softmax function; for K classes we really only need to estimate K − 1 sets of coefficients. See Section 4.3.5.) In Figure 10.11 we show accuracy (fraction correct) rather than classification error (fraction incorrect), the former being more popular in the machine learning community. Both models achieve a test-set accuracy of about 88%.\n",
    "\n",
    "The bag-of-words model summarizes a document by the words present, and ignores their context. There are at least two popular ways to take the context into account:\n",
    "\n",
    "- The bag-of-n-grams model. For example, a bag of 2-grams records the consecutive co-occurrence of every distinct pair of words. “Bliss\n",
    "fully long” can be seen as a positive phrase in a movie review, while\n",
    "“blissfully short” a negative.\n",
    "- Treat the document as a sequence, taking account of all the words in\n",
    "the context of those that preceded and those that follow.\n",
    "\n",
    "In the next section we explore models for sequences of data, which have\n",
    "applications in weather forecasting, speech recognition, language transla\n",
    "tion, and time-series prediction, to name a few. We continue with this IMDb\n",
    "example there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80531def",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe2c0e8",
   "metadata": {},
   "source": [
    "Many data sources are sequential in nature, and call for special treatment when building predictive models. Examples include:\n",
    "\n",
    "- Documents such as book and movie reviews, newspaper articles, and tweets. The sequence and relative positions of words in a document capture the narrative, theme and tone, and can be exploited in tasks such as topic classification, sentiment analysis, and language translation.\n",
    "- Time series of temperature, rainfall, wind speed, air quality, and so on. We may want to forecast the weather several days ahead, or climate several decades ahead.\n",
    "- Financial time series, where we track market indices, trading volumes, stock and bond prices, and exchange rates. Here prediction is often difficult, but as we will see, certain indices can be predicted with reasonable accuracy.\n",
    "- Recorded speech, musical recordings, and other sound recordings. We may want to give a text transcription of a speech, or perhaps a language translation. We may want to assess the quality of a piece of music, or assign certain attributes.\n",
    "- Handwriting, such as doctor’s notes, and handwritten digits such as zip codes. Here we want to turn the handwriting into digital text, or read the digits (optical character recognition).\n",
    "\n",
    "In a recurrent neural network (RNN), the input object $X$ is a sequence. Consider a corpus of documents, such as the collection of IMDb movie reviews. Each document can be represented as a sequence of $L$ words, so\n",
    "$$\n",
    "X = \\{X_1, X_2, \\ldots, X_L\\},\n",
    "$$\n",
    "where each $X_i$ represents a word. The order of the words, and closeness of certain words in a sentence, convey semantic meaning. RNNs are designed to accommodate and take advantage of the sequential nature of such input objects, much like convolutional neural networks accommodate the spatial structure of image inputs. The output $Y$ can also be a sequence (such as in language translation), but often is a scalar, like the binary sentiment label of a movie review document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2768ca",
   "metadata": {},
   "source": [
    "Figure 10.12 illustrates the structure of a very basic RNN with a sequence\n",
    "$X = \\{X_1, X_2, \\ldots, X_L\\}$ as input, a simple output $Y$, and a hidden-layer\n",
    "sequence $\\{A\\}_{1}^{L} = \\{A_1, A_2, \\ldots, A_L\\}$. Each $X$ is a vector; in the document\n",
    "example $X$ could represent a one-hot encoding for the $i$-th word based on\n",
    "the language dictionary for the corpus (see the top panel in Figure 10.13\n",
    "for a simple example). As the sequence is processed one vector $X_t$ at a\n",
    "time, the network updates the activations $A_t$ in the hidden layer, taking\n",
    "as input the vector $X_t$ and the activation vector $A_{t-1}$ from the previous\n",
    "step in the sequence. Each $A_t$ feeds into the output layer and produces a\n",
    "prediction $O_t$ for $Y$. $O_L$, the last of these, is the most relevant.\n",
    "\n",
    "In detail, suppose each vector $X_t$ of the input sequence has $p$ components\n",
    "$X_t^{T} = (X_{t,1}, X_{t,2}, \\ldots, X_{t,p})$, and the hidden layer consists of $K$ units $A_t^{T} =\n",
    "(A_{t,1}, A_{t,2}, \\ldots, A_{t,K})$. As in Figure 10.4, we represent the collection of $K$\n",
    "$(p+1)$ shared weights $w_{kj}$ for the input layer by a matrix $W$, and similarly\n",
    "$U$ is a $K\\times K$ matrix of the weights $u_{ks}$ for the hidden-to-hidden layers,\n",
    "and $B$ is a $(K+1)$-vector of weights $b_k$ for the output layer. Then\n",
    "\n",
    "$$\n",
    "A_{t,k} \\;=\\; g\\!\\left( w_{k0} \\;+\\; \\sum_{j=1}^{p} w_{kj}\\,X_{t,j} \\;+\\; \\sum_{s=1}^{K} u_{ks}\\,A_{t-1,s} \\right)\n",
    "\\tag{10.16}\n",
    "$$\n",
    "\n",
    "and the output $O_t$ is computed as\n",
    "\n",
    "$$\n",
    "O_t \\;=\\; b_0 \\;+\\; \\sum_{k=1}^{K} b_k\\,A_{t,k}\n",
    "\\tag{10.17}\n",
    "$$\n",
    "\n",
    "for a quantitative response, or with an additional sigmoid activation function for a binary response, for example. Here $g(\\cdot)$ is an activation function such as ReLU. Notice that the same weights $W$, $U$ and $B$ are used as we process each element in the sequence, i.e. they are not functions of t. This is a form of weight sharing used by RNNs, and similar to the use of filters in convolutional neural networks (Section 10.3.1). As we proceed from beginning to end, the activations A accumulate a history of what has been seen before, so that the learned context can be used for prediction.\n",
    "\n",
    "For regression problems the loss function for an observation $(X,Y)$ is\n",
    "$$\n",
    "(Y - O_L)^2,\n",
    "\\tag{10.18}\n",
    "$$\n",
    "which only references the final output $O_L = b_0 + \\sum_{k=1}^K b_k A_{L,k}$. Thus $O_1, O_2, \\ldots, O_{L-1}$ are not used. When we fit the model, each element $X_t$ of the input sequence $X$ contributes to $O_L$ via the chain (10.16), and hence contributes indirectly to learning the shared parameters $W$, $U$ and $B$ via the loss (10.18). With $n$ input sequence/response pairs $(x_i,y_i)$, the parameters are found by minimizing the sum of squares\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - o_{iL})^2\n",
    "\\;=\\;\n",
    "\\sum_{i=1}^n \\Bigg( y_i - \\Bigg( \\beta_0 + \\sum_{k=1}^K \\beta_k g\\Big( w_{k0} + \\sum_{j=1}^p w_{kj} x_{iLj} + \\sum_{s=1}^K u_{ks} a_{i,L-1,s} \\Big) \\Bigg) \\Bigg)^2\n",
    "\\tag{10.19}\n",
    "$$\n",
    "\n",
    "If you want the equation without the outer tag number, remove the \\tag{10.19} part.\n",
    "Here we use lowercase letters for the observed $y_i$ and vector sequences $x_i = \\{x_{i1}, x_{i2}, \\ldots, x_{iL}\\}$,15 as well as the derived activations.\n",
    "\n",
    "Since the intermediate outputs $O_t$ are not used, one may well ask why they are there at all. First of all, they come for free, since they use the same output weights $B$ needed to produce $O_L$, and provide an evolving prediction for the output. Furthermore, for some learning tasks the response is also a sequence, and so the output sequence $\\{O_1, O_2, \\ldots, O_L\\}$ is explicitly needed.\n",
    "\n",
    "When used at full strength, recurrent neural networks can be quite complex. We illustrate their use in two simple applications. In the first, we continue with the IMDb sentiment analysis of the previous section, where we process the words in the reviews sequentially. In the second application, we illustrate their use in a financial time series forecasting problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab3345",
   "metadata": {},
   "source": [
    "##### Sequential Models for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aad21f",
   "metadata": {},
   "source": [
    "Here we return to our classification task with the IMDb reviews. Our approach in Section 10.4 was to use the bag-of-words model. Here the plan is to use instead the sequence of words occurring in a document to make predictions about the label for the entire document.\n",
    "\n",
    "We have, however, a dimensionality problem: each word in our document is represented by a one-hot-encoded vector (dummy variable) with 10,000 elements (one per word in the dictionary)! An approach that has become popular is to represent each word in a much lower-dimensional embedding space. This means that rather than representing each word by a binary vector with 9,999 zeros and a single one in some position, we will represent it instead by a set of $m$ real numbers, none of which are typically zero. Here $m$ is the embedding dimension, and can be in the low 100s, or even less.\n",
    "\n",
    "This means (in our case) that we need a matrix $E$ of dimension $m \\times 10{,}000$, where each column is indexed by one of the 10,000 words in our dictionary, and the values in that column give the m coordinates for that word in the embedding space.\n",
    "\n",
    "Figure 10.13 illustrates the idea (with a dictionary of 16 rather than 10,000, and m = 5). Where does E come from? If we have a large corpus of labeled documents, we can have the neural network learn E as part of the optimization. In this case E is referred to as an embedding layer, and a specialized E is learned for the task at hand. Otherwise we can insert a precomputed matrix E in the embedding layer, a process known as weight freezing. Two pretrained embeddings, word2vec and GloVe, are widely used. These are built from a very large corpus of documents by a variant of principal components analysis (Section 12.2). The idea is that the positions of words in the embedding space preserve semantic meaning; e.g. synonyms should appear near each other.\n",
    "\n",
    "So far, so good. Each document is now represented as a sequence of m vectors that represents the sequence of words. The next step is to limit each document to the last L words. Documents that are shorter than L get padded with zeros upfront. So now each document is represented by a series consisting of L vectors $X = \\{X_1, X_2, ..., X_L\\}$, and each $X$ in the sequence has m components.\n",
    "\n",
    "We now use the RNN structure in Figure 10.12. The training corpus consists of n separate series (documents) of length L, each of which gets processed sequentially from left to right. In the process, a parallel series of hidden activation vectors $A_t,\\, t=1,\\ldots,L$ is created as in (10.16) for each document. $A_t$ feeds into the output layer to produce the evolving prediction $O_t$. We use the final value $O_L$ to predict the response: the sentiment of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925d02e",
   "metadata": {},
   "source": [
    "This is a simple RNN, and has relatively few parameters. If there are $K$ hidden units, the common weight matrix $W$ has $K(m+1)$ parameters, the matrix $U$ has $K\\times K$ parameters, and $B$ has $2(K+1)$ for the two-class logistic regression as in (10.15). These are used repeatedly as we process the sequence $X=\\{X\\}_1^L$ from left to right, much like we use a single convolution filter to process each patch in an image (Section 10.3.1). If the embedding layer $E$ is learned, that adds an additional $m\\times D$ parameters ($D=10{,}000$ here), and is by far the biggest cost.\n",
    "\n",
    "We fit the RNN as described in Figure 10.12 and the accompanying text to the IMDb data. The model had an embedding matrix $E$ with $m=32$ (which was learned in training as opposed to precomputed), followed by a single recurrent layer with $K=32$ hidden units. The model was trained with dropout regularization on the 25,000 reviews in the designated training set, and achieved a disappointing 76% accuracy on the IMDb test data. A network using the GloVe pretrained embedding matrix $E$ performed slightly worse.\n",
    "\n",
    "For ease of exposition we have presented a very simple RNN. More elaborate versions use long term and short term memory (LSTM). Two tracks of hidden-layer activations are maintained, so that when the activation $A_t$ is computed, it gets input from hidden units both further back in time, and closer in time — a so-called LSTM RNN. With long sequences, this LSTM RNN overcomes the problem of early signals being washed out by the time they get propagated through the chain to the final activation vector $A_L$.\n",
    "\n",
    "When we refit our model using the LSTM architecture for the hidden layer, the performance improved to 87% on the IMDb test data. This is comparable with the 88% achieved by the bag-of-words model in Section 10.4. We give details on fitting these models in Section 10.9.6.\n",
    "\n",
    "Despite this added LSTM complexity, our RNN is still somewhat “entry level”. We could probably achieve slightly better results by changing the size of the model, changing the regularization, and including additional hidden layers. However, LSTM models take a long time to train, which makes exploring many architectures and parameter optimization tedious.\n",
    "\n",
    "RNNs provide a rich framework for modeling data sequences, and they continue to evolve. There have been many advances in the development of RNNs — in architecture, data augmentation, and in the learning algorithms. At the time of this writing (early 2020) the leading RNN configurations report accuracy above 95% on the IMDb data. The details are beyond the scope of this book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c096466",
   "metadata": {},
   "source": [
    "##### Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3059a46",
   "metadata": {},
   "source": [
    "Figure 10.14 shows historical trading statistics from the New York Stock Exchange. Shown are three daily time series covering the period December 3, 1962 to December 31, 1986:\n",
    "- Log trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\n",
    "\n",
    "- Dow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\n",
    "\n",
    "- Log volatility. This is based on the absolute values of daily price movements.\n",
    "\n",
    "Predicting stock prices is a notoriously hard problem, but it turns out that predicting trading volume based on recent past history is more manageable (and is useful for planning trading strategies).\n",
    "\n",
    "An observation here consists of the measurements $(v_t, r_t, z_t)$ on day $t$, in this case the values for log_volume, DJ_return and log_volatility. There are a total of $T = 6{,}051$ such triples, each of which is plotted as a time series in Figure 10.14. One feature that strikes us immediately is that the day-to-day observations are not independent of each other. The series exhibit auto-correlation — in this case values nearby in time tend to be similar to each other. This distinguishes time series from other data sets we have encountered, in which observations can be assumed to be independent of each other. To be clear, consider pairs of observations $(v_t, v_{t-\\ell})$, a lag of $\\ell$ days apart. If we take all such pairs in the $v_t$ series and compute their correlation coefficient, this gives the autocorrelation at lag $\\ell$. Figure 10.15 shows the autocorrelation function for all lags up to 37, and we see considerable correlation.\n",
    "\n",
    "Another interesting characteristic of this forecasting problem is that the response variable $v_t$ — log_volume — is also a predictor! In particular, we will use the past values of log_volume to predict values in the future.\n",
    "\n",
    "RNN forecaster\n",
    "\n",
    "We wish to predict a value $v_t$ from past values $v_{t-1}, v_{t-2}, \\ldots$, and also to make use of past values of the other series $r_{t-1}, r_{t-2}, \\ldots$ and $z_{t-1}, z_{t-2}, \\ldots$. Although our combined data is quite a long series with 6,051 trading days, the structure of the problem is different from the previous document classification example.\n",
    "\n",
    "- We only have one series of data, not 25,000.\n",
    "- We have an entire series of targets $v_t$, and the inputs include past values of this series.\n",
    "\n",
    "How do we represent this problem in terms of the structure displayed in Figure 10.12? The idea is to extract many short mini-series of input sequences $X = \\{X_1, X_2, \\ldots, X_L\\}$ with a predefined length $L$ (called the lag in this context), and a corresponding target $Y$. They have the form\n",
    "\n",
    "$$\n",
    "X_1 = \\begin{pmatrix} v_{t-L} \\\\ r_{t-L} \\\\ z_{t-L} \\end{pmatrix},\\quad\n",
    "X_2 = \\begin{pmatrix} v_{t-L+1} \\\\ r_{t-L+1} \\\\ z_{t-L+1} \\end{pmatrix},\\quad\n",
    "\\ldots,\\quad\n",
    "X_L = \\begin{pmatrix} v_{t-1} \\\\ r_{t-1} \\\\ z_{t-1} \\end{pmatrix},\\quad\n",
    "\\text{and}\\quad Y = v_t.\n",
    "\\tag{10.20}\n",
    "$$\n",
    "\n",
    "So here the target $Y$ is the value of log_volume $v_t$ at a single timepoint $t$, and the input sequence $X$ is the series of 3-vectors $\\{X_\\ell\\}_{1}^{L}$ each consisting of the three measurements `log_volume`, `DJ_return` and `log_volatility` from day $t-L,\\, t-L+1,$ up to $t-1$. Each value of $t$ makes a separate $(X,Y)$ pair, for $t$ running from $L+1$ to $T$. For the NYSE data we will use the past five trading days to predict the next day’s trading volume. Hence, we use L = 5. Since T = 6,051, we can create 6,046 such (X, Y) pairs. Clearly L is a parameter that should be chosen with care, perhaps using validation data.\n",
    "\n",
    "We fit this model with K = 12 hidden units using the 4,281 training sequences derived from the data before January 2, 1980 (see Figure 10.14), and then used it to forecast the 1,770 values of `log_volume` after this date. We achieve an $R^2$ = 0.42 on the test data. Details are given in Section 10.9.6. As a strawman, using yesterday’s value for `log_volume` as the prediction for today has $R^2$ = 0.18. Figure 10.16 shows the forecast results. We have plotted the observed values of the daily `log_volume` for the test period 1980–1986 in black, and superimposed the predicted series in orange. The correspondence seems rather good.\n",
    "\n",
    "In forecasting the value of `log_volume` in the test period, we have to use the test data itself informing the input sequences X. This may feel like cheating, but in fact it is not; we are always using past data to predict the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb2689",
   "metadata": {},
   "source": [
    "\n",
    "Autoregression\n",
    "\n",
    "The RNN we just fit has much in common with a traditional autoregression (AR) linear model, which we present now for comparison. We first consider the response sequence v_t alone, and construct a response vector y and a matrix M of predictors for least-squares regression as follows:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "v_{L+1} \\\\\n",
    "v_{L+2} \\\\\n",
    "v_{L+3} \\\\\n",
    "\\vdots \\\\\n",
    "v_T\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "M =\n",
    "\\begin{pmatrix}\n",
    "1 & v_L & v_{L-1} & \\cdots & v_1 \\\\\n",
    "1 & v_{L+1} & v_L & \\cdots & v_2 \\\\\n",
    "1 & v_{L+2} & v_{L+1} & \\cdots & v_3 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & v_{T-1} & v_{T-2} & \\cdots & v_{T-L}\n",
    "\\end{pmatrix}\n",
    "\\tag{10.21}\n",
    "$$\n",
    "\n",
    "M and y each have T − L rows, one per observation. We see that the predictors for any given response v_t on day t are the previous L values of the same series. Fitting a regression of y on M amounts to fitting the model\n",
    "\n",
    "$$\n",
    "\\hat v_t \\;=\\; \\hat\\beta_0 + \\hat\\beta_1 v_{t-1} + \\hat\\beta_2 v_{t-2} + \\cdots + \\hat\\beta_L v_{t-L},\n",
    "\\tag{10.22}\n",
    "$$\n",
    "\n",
    "and is called an order-L autoregressive model, or simply AR(L). For the NYSE data we can include lagged versions of DJ_return and log_volatility, $r_t$ and $z_t$, in the predictor matrix $M$, resulting in $3L+1$ columns. An AR model with $L=5$ achieves a test $R^2$ of 0.41, slightly inferior to the 0.42 achieved by the RNN.\n",
    "\n",
    "Of course the RNN and AR models are very similar. They both use the same response $Y$ and input sequences $X$ of length $L=5$ and dimension $p=3$ in this case. The RNN processes this sequence from left to right with the same weights $W$ (for the input layer), while the AR model simply treats all $L$ elements of the sequence equally as a vector of $L\\! \\times\\! p$ predictors — a process called flattening in the neural network literature.\n",
    "\n",
    "Of course the RNN also includes the hidden layer activations $A$ which transfer information along the sequence, and introduces additional nonlinearity. From (10.19) with $K=12$ hidden units, we see that the RNN has $ 13 + 12(1 + 3 + 12) = 205 $ parameters, compared to the 16 for the AR(5) model.\n",
    "\n",
    "An obvious extension of the AR model is to use the set of lagged predictors as the input vector to an ordinary feedforward neural network (10.1), and hence add more flexibility. This achieved a test $R^2 = 0.42$, slightly better than the linear AR, and the same as the RNN.\n",
    "\n",
    "All the models can be improved by including the variable day_of_week corresponding to the day $t$ of the target $v_t$ (which can be learned from the calendar dates supplied with the data); trading volume is often higher on Mondays and Fridays. Since there are five trading days, this one-hot encodes to five binary variables. The performance of the AR model improved to $R^2 = 0.46$ as did the RNN, and the nonlinear AR model improved to $R^2 = 0.47$.\n",
    "\n",
    "We used the most simple version of the RNN in our examples here. Additional experiments with the LSTM extension of the RNN yielded small improvements, typically of up to 1% in $R^2$ in these examples.\n",
    "\n",
    "We give details of how we fit all three models in Section 10.9.6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ee0e2",
   "metadata": {},
   "source": [
    "\n",
    "##### Summary of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a955e27",
   "metadata": {},
   "source": [
    "We have illustrated RNNs through two simple use cases, and have only scratched the surface.\n",
    "\n",
    "There are many variations and enhancements of the simple RNN we used for sequence modeling. One approach we did not discuss uses a one-dimensional convolutional neural network, treating the sequence of vectors (say words, as represented in the embedding space) as an image. The convolution filter slides along the sequence in a one-dimensional fashion, with the potential to learn particular phrases or short subsequences relevant to the learning task.\n",
    "\n",
    "One can also have additional hidden layers in an RNN. For example, with two hidden layers, the sequence $A$ is treated as an input sequence to the next hidden layer in an obvious fashion.\n",
    "\n",
    "The RNN we used scanned the document from beginning to end; alternative bidirectional RNNs scan the sequences in both directions.\n",
    "\n",
    "In language translation the target is also a sequence of words, in a language different from that of the input sequence. Both the input sequence and the target sequence are represented by a structure similar to Figure 10.12, and they share the hidden units. In this so-called Seq2Seq learning, the hidden units are thought to capture the semantic meaning of the sentences. Some of the big breakthroughs in language modeling and translation resulted from the relatively recent improvements in such RNNs.\n",
    "\n",
    "Algorithms used to fit RNNs can be complex and computationally costly. Fortunately, good software protects users somewhat from these complexities, and makes specifying and fitting these models relatively painless. Many of the models that we enjoy in daily life (like Google Translate) use state-of-the-art architectures developed by teams of highly skilled engineers, and have been trained using massive computational and data resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558b8dc",
   "metadata": {},
   "source": [
    "#### When to Use Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81404d4",
   "metadata": {},
   "source": [
    "The performance of deep learning in this chapter has been rather impressive. It nailed the digit classification problem, and deep CNNs have really revolutionized image classification. We see daily reports of new success stories for deep learning. Many of these are related to image classification tasks, such as machine diagnosis of mammograms or digital X-ray images, ophthalmology eye scans, annotations of MRI scans, and so on. Likewise there are numerous successes of RNNs in speech and language translation, forecasting, and document modeling. The question that then begs an answer is: should we discard all our older tools, and use deep learning on every problem with data? To address this question, we revisit our Hitters dataset from Chapter 6.\n",
    "\n",
    "This is a regression problem, where the goal is to predict the Salary of a baseball player in 1987 using his performance statistics from 1986. After removing players with missing responses, we are left with 263 players and 19 variables. We randomly split the data into a training set of 176 players (two thirds), and a test set of 87 players (one third). We used three methods for fitting a regression model to these data.\n",
    "\n",
    "- A linear model was used to fit the training data, and make predictions on the test data. The model has 20 parameters.\n",
    "\n",
    "- The same linear model was fit with lasso regularization. The tuning parameter was selected by 10-fold cross-validation on the training data. It selected a model with 12 variables having nonzero coefficients.\n",
    "\n",
    "- A neural network with one hidden layer consisting of 64 ReLU units was fit to the data. This model has 1,345 parameters.\n",
    "\n",
    "\n",
    "\n",
    "| Model           | # Parameters | Mean Abs. Error | Test Set R^2 |\n",
    "|-----------------|--------------:|----------------:|-------------:|\n",
    "| Linear Regression | 20          | 254.7           | 0.56         |\n",
    "| Lasso           | 12           | 252.3           | 0.51         |\n",
    "| Neural Network  | 1345         | 257.4           | 0.54         |\n",
    "\n",
    "TABLE 10.2. Prediction results on the Hitters test data for linear models fit by ordinary least squares and lasso, compared to a neural network fit by stochastic gradient descent with dropout regularization.\n",
    "\n",
    "\n",
    "| Coefficient | Std. error | t-statistic | p-value   |\n",
    "|-------------|-----------:|------------:|----------:|\n",
    "| Intercept   | -226.67    | 86.26       | -2.63     | 0.0103 |\n",
    "| Hits        | 3.06       | 1.02        | 3.00      | 0.0036 |\n",
    "| Walks       | 0.181      | 0.12        | 2.04      | 0.0294 |\n",
    "| CRuns       | 0.859      | 0.13        | 7.09      | <0.0001 |\n",
    "| PutOuts     | 0.465      | 0.09        | 3.60      | 0.0005 |\n",
    "\n",
    "TABLE 10.3. Least squares coefficient estimates associated with the regression of Salary on four variables chosen by lasso on the Hitters data set. This model achieved the best performance on the test data, with a mean absolute error of 224.8. The results reported here were obtained from a regression on the test data, which was not used in fitting the lasso model.\n",
    "\n",
    "\n",
    "Table 10.2 compares the results. We see similar performance for all three models. We report the mean absolute error on the test data, as well as the test $R^2$ for each method, which are all respectable (see Exercise 5). We spent a fair bit of time fiddling with the configuration parameters of the neural network to achieve these results. It is possible that if we were to spend more time, and got the form and amount of regularization just right, that we might be able to match or even outperform linear regression and the lasso. But with great ease we obtained linear models that work well. Linear models are much easier to present and understand than the neural network, which is essentially a black box. The lasso selected 12 of the 19 variables in making its prediction. So in cases like this we are much better off following the Occam’s razor principle: when faced with several methods that give roughly equivalent performance, pick the simplest.\n",
    "\n",
    "After a bit more exploration with the lasso model, we identified an even simpler model with four variables. We then refit the linear model with these four variables to the training data (the so-called relaxed lasso), and achieved a test mean absolute error of 224.8, the overall winner! It is tempting to present the summary table from this fit, so we can see coefficients and p-values; however, since the model was selected on the training data, there would be selection bias. Instead, we refit the model on the test data, which was not used in the selection. Table 10.3 shows the results.\n",
    "\n",
    "We have a number of very powerful tools at our disposal, including neural networks, random forests and boosting, support vector machines and generalized additive models, to name a few. And then we have linear models, and simple variants of these. When faced with new data modeling and prediction problems, it’s tempting to always go for the trendy new methods. Often they give extremely impressive results, especially when the datasets are very large and can support the fitting of high-dimensional nonlinear models. However, if we can produce models with the simpler tools that perform as well, they are likely to be easier to fit and understand, and potentially less fragile than the more complex approaches. Wherever possible, it makes sense to try the simpler models as well, and then make a choice based on the performance/complexity tradeoff.\n",
    "\n",
    "Typically we expect deep learning to be an attractive choice when the sample size of the training set is extremely large, and when interpretability of the model is not a high priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89681ff8",
   "metadata": {},
   "source": [
    "#### Fitting a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4960e3b",
   "metadata": {},
   "source": [
    "Fitting neural networks is somewhat complex, and we give a brief overview here. The ideas generalize to much more complex networks. Readers who find this material challenging can safely skip it. Fortunately, as we see in the lab at the end of this chapter, good software is available to fit neural network models in a relatively automated way, without worrying about the technical details of the model-fitting procedure.\n",
    "\n",
    "We start with the simple network depicted in Figure 10.1 in Section 10.1. In model (10.1) the parameters are $ \\theta = (\\beta_0, \\beta_1, \\dots, \\beta_K) $, as well as each of the $ w_k = (w_{k0}, w_{k1}, \\dots, w_{kp}) $, $ k = 1, \\dots, K $. Given observations $ (x_i, y_i) $, $ i = 1, \\dots, n $, we could fit the model by solving a nonlinear least squares problem\n",
    "\n",
    "$$\n",
    "\\min_{\\{\\!w_k\\!\\}_{k=1}^K,\\ \\beta}\\ \\frac{1}{2}\\sum_{i=1}^n \\bigl(y_i - f(x_i)\\bigr)^2,\n",
    "\\tag{10.23}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f(x_i) \\;=\\; \\beta_0 + \\sum_{k=1}^K \\alpha_k\\, g\\!\\left(w_{k0} + \\sum_{j=1}^p w_{kj}\\, x_{ij}\\right).\n",
    "\\tag{10.24}\n",
    "$$\n",
    "\n",
    "The objective in (10.23) looks simple enough, but because of the nested arrangement of the parameters and the symmetry of the hidden units, it is not straightforward to minimize. The problem is nonconvex in the parameters, and hence there are multiple solutions. As an example, Figure 10.17 shows a simple nonconvex function of a single variable $ \\theta $; there are two solutions: one is a local minimum and the other is a global minimum.\n",
    "\n",
    "Furthermore, (10.1) is the very simplest of neural networks; in this chapter we have presented much more complex ones where these problems are compounded. To overcome some of these issues and to protect from overfitting, two general strategies are employed when fitting neural networks.\n",
    "\n",
    "- Slow Learning: the model is fit in a somewhat slow iterative fashion, using gradient descent. The fitting process is then stopped when overfitting is detected.\n",
    "\n",
    "- Regularization: penalties are imposed on the parameters, usually lasso or ridge as discussed in Section 6.2.\n",
    "\n",
    "Suppose we represent all the parameters in one long vector $ \\theta $. Then we can rewrite the objective in (10.23) as\n",
    "\n",
    "$$\n",
    "R(\\theta) \\;=\\; \\frac{1}{2}\\sum_{i=1}^n \\bigl(y_i - f_{\\theta}(x_i)\\bigr)^2.\n",
    "\\tag{10.25}\n",
    "$$\n",
    "\n",
    "where we make explicit the dependence of f on the parameters. The idea of gradient descent is very simple.\n",
    "\n",
    "1. Start with a guess $\\theta^0$ for all the parameters in $\\theta$, and set $t = 0$.\n",
    "2. Iterate until the objective (10.25) fails to decrease:\n",
    "   (a) Find a vector $\\delta$ that reflects a small change in $\\theta$, such that $\\theta^{t+1} = \\theta^t + \\delta$ reduces the objective; i.e. such that $R(\\theta^{t+1}) < R(\\theta^t)$.\n",
    "   (b) Set $t \\leftarrow t + 1$.\n",
    "\n",
    "One can visualize (Figure 10.17) standing in a mountainous terrain, and the goal is to get to the bottom through a series of steps. As long as each step goes downhill, we must eventually get to the bottom. In this case we were lucky, because with our starting guess $\\theta^0$ we end up at the global minimum. In general we can hope to end up at a (good) local minimum.\n",
    "\n",
    "10.7.1 Backpropagation\n",
    "\n",
    "How do we find the directions to move so as to decrease the objective $R(\\theta)$ in (10.25)? The gradient of $R(\\theta)$, evaluated at some current value $\\theta = \\theta^m$, is the vector of partial derivatives at that point:\n",
    "\n",
    "$$\n",
    "\\nabla R(\\theta^m) \\;=\\; \\left.\\frac{\\partial R(\\theta)}{\\partial \\theta}\\right|_{\\theta=\\theta^m}.\n",
    "\\tag{10.26}\n",
    "$$\n",
    "\n",
    "The subscript $\\theta=\\theta^m$ means that after computing the vector of derivatives, we evaluate it at the current guess, $\\theta^m$. This gives the direction in $\\theta$-space in which $R(\\theta)$ increases most rapidly. The idea of gradient descent is to move a little in the opposite direction (since we wish to go downhill):\n",
    "\n",
    "$$\n",
    "\\theta^{m+1} \\leftarrow \\theta^{m} - \\rho \\,\\nabla R(\\theta^{m}).\n",
    "\\tag{10.27}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c30f52",
   "metadata": {},
   "source": [
    "where we make explicit the dependence of f on the parameters. The idea of gradient descent is very simple.\n",
    "\n",
    "1. Start with a guess $\\theta^0$ for all the parameters in $\\theta$, and set $t = 0$.\n",
    "2. Iterate until the objective (10.25) fails to decrease:\n",
    "   (a) Find a vector $\\delta$ that reflects a small change in $\\theta$, such that $\\theta^{t+1} = \\theta^t + \\delta$ reduces the objective; i.e. such that $R(\\theta^{t+1}) < R(\\theta^t)$.\n",
    "   (b) Set $t \\leftarrow t + 1$.\n",
    "\n",
    "One can visualize (Figure 10.17) standing in a mountainous terrain, and the goal is to get to the bottom through a series of steps. As long as each step goes downhill, we must eventually get to the bottom. In this case we were lucky, because with our starting guess $\\theta^0$ we end up at the global minimum. In general we can hope to end up at a (good) local minimum.\n",
    "\n",
    "10.7.1 Backpropagation\n",
    "\n",
    "How do we find the directions to move so as to decrease the objective $R(\\theta)$ in (10.25)? The gradient of $R(\\theta)$, evaluated at some current value $\\theta = \\theta^m$, is the vector of partial derivatives at that point:\n",
    "\n",
    "$$\n",
    "\\nabla R(\\theta^m) \\;=\\; \\left.\\frac{\\partial R(\\theta)}{\\partial \\theta}\\right|_{\\theta=\\theta^m}.\n",
    "\\tag{10.26}\n",
    "$$\n",
    "\n",
    "The subscript $\\theta=\\theta^m$ means that after computing the vector of derivatives, we evaluate it at the current guess, $\\theta^m$. This gives the direction in $\\theta$-space in which $R(\\theta)$ increases most rapidly. The idea of gradient descent is to move a little in the opposite direction (since we wish to go downhill):\n",
    "\n",
    "$$\n",
    "\\theta^{m+1} \\leftarrow \\theta^{m} - \\rho \\,\\nabla R(\\theta^{m}).\n",
    "\\tag{10.27}\n",
    "$$\n",
    "\n",
    "For a small enough value of the learning rate $\\rho$, this step will decrease the objective $R(\\theta)$; i.e. $R(\\theta^{m+1}) < R(\\theta^m)$. If the gradient vector is zero, then we may have arrived at a minimum of the objective.\n",
    "\n",
    "How complicated is the calculation (10.26)? It turns out that it is quite simple here, and remains simple even for much more complex networks, because of the chain rule of differentiation.\n",
    "\n",
    "Since $R(\\theta)=\\sum_{i=1}^n R_i(\\theta)=\\tfrac{1}{2}\\sum_{i=1}^n (y_i - f_\\theta(x_i))^2$ is a sum, its gradient is also a sum over the $n$ observations, so we will just examine one of these terms,\n",
    "\n",
    "$$\n",
    "R_i(\\theta) \\;=\\; \\tfrac{1}{2}\\bigl(y_i - f_\\theta(x_i)\\bigr)^2\n",
    "\\tag{10.28}\n",
    "$$\n",
    "\n",
    "To simplify the expressions to follow, we write $z_{ik} = w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}$.\n",
    "\n",
    "First we take the derivative with respect to $\\alpha_k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial R_i(\\theta)}{\\partial \\alpha_k}\n",
    "= \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)}\\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\alpha_k}\n",
    "= -(y_i - f_\\theta(x_i))\\cdot g(z_{ik}).\n",
    "\\tag{10.29}\n",
    "$$\n",
    "\n",
    "And now we take the derivative with respect to $w_{kj}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial R_i(\\theta)}{\\partial w_{kj}}\n",
    "= \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)}\\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial g(z_{ik})}\\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}}\\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}}\n",
    "= -(y_i - f_\\theta(x_i))\\cdot \\alpha_k\\cdot g'(z_{ik})\\cdot x_{ij}.\n",
    "\\tag{10.30}\n",
    "$$\n",
    "\n",
    "Notice that both these expressions contain the residual $y_i - f_\\theta(x_i)$. In (10.29) we see that a fraction of that residual gets attributed to each of the hidden units according to the value of $g(z_{ik})$. Then in (10.30) we see a similar attribution to input $j$ via hidden unit $k$. So the act of differentiation assigns a fraction of the residual to each of the parameters via the chain rule — a process known as backpropagation in the neural network literature. Although these calculations are straightforward, it takes careful bookkeeping to keep track of all the pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc35b3",
   "metadata": {},
   "source": [
    "##### Regularization and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3a668",
   "metadata": {},
   "source": [
    "Gradient descent usually takes many steps to reach a local minimum. In practice, there are a number of approaches for accelerating the process. Also, when $n$ is large, instead of summing (10.29)–(10.30) over all $n$ observations, we can sample a small fraction or minibatch of them each time we compute a gradient step. This process is known as stochastic gradient descent (SGD) and is the state of the art for learning deep neural networks. Fortunately, there is very good software for setting up deep learning models, and for fitting them to data, so most of the technicalities are hidden from the user.\n",
    "\n",
    "We now turn to the multilayer network (Figure 10.4) used in the digit recognition problem. The network has over 235,000 weights, which is around four times the number of training examples. Regularization is essential here to avoid overfitting. The first row in Table 10.1 uses ridge regularization on the weights. This is achieved by augmenting the objective function (10.14) with a penalty term:\n",
    "\n",
    "$$\n",
    "R(\\theta; \\Lambda) \\;=\\; -\\sum_{i=1}^n \\sum_{m=0}^9 y_{im}\\,\\log\\bigl(f_m(x_i)\\bigr) \\;+\\; \\Lambda \\sum_j \\theta_j^2.\n",
    "\\tag{10.31}\n",
    "$$\n",
    "\n",
    "The parameter $\\Lambda$ is often preset at a small value, or else it is found using the validation-set approach of Section 5.3.1. We can also use different values of $\\Lambda$ for the groups of weights from different layers; in this case $W_1$ and $W_2$ were penalized, while the relatively few weights $B$ of the output layer were not penalized at all. Lasso regularization is also popular as an additional form of regularization, or as an alternative to ridge.\n",
    "\n",
    "Figure 10.18 shows some metrics that evolve during the training of the network on the MNIST data. It turns out that SGD naturally enforces its own form of approximately quadratic regularization. Here the minibatch size was 128 observations per gradient update. The term epochs labeling the horizontal axis in Figure 10.18 counts the number of times an equivalent of the full training set has been processed. For this network, 20% of the 60,000 training observations were used as a validation set in order to determine when training should stop. So in fact 48,000 observations were used for training, and hence there are $48{,}000/128 \\approx 375$ minibatch gradient updates per epoch. We see that the value of the validation objective actually starts to increase by 30 epochs, so early stopping can also be used as an additional form of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3f1a7",
   "metadata": {},
   "source": [
    "##### Dropout Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269d54c",
   "metadata": {},
   "source": [
    "The second row in Table 10.1 is labeled dropout. This is a relatively new and efficient form of regularization, similar in some respects to ridge regularization. Inspired by random forests (Section 8.2), the idea is to randomly remove a fraction of the units in a layer when fitting the model. Figure 10.19 illustrates this. This is done separately each time a training observation is processed. The surviving units stand in for those missing, and their weights are scaled up by a factor of $1/(1 − \\phi)$ to compensate. This prevents nodes from becoming over-specialized, and can be seen as a form of regularization. In practice dropout is achieved by randomly setting the activations for the “dropped out” units to zero, while keeping the architecture intact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f127bc",
   "metadata": {},
   "source": [
    "##### Network Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6d65c",
   "metadata": {},
   "source": [
    "The network in Figure 10.4 is considered to be relatively straightforward; it nevertheless requires a number of choices that all have an effect on the performance:\n",
    "\n",
    "- The number of hidden layers, and the number of units per layer. Modern thinking is that the number of units per hidden layer can be large, and overfitting can be controlled via the various forms of regularization.\n",
    "- Regularization tuning parameters. These include the dropout rate p and the strength of lasso and ridge regularization, and are typically set separately at each layer.\n",
    "- Details of stochastic gradient descent. These include the batch size, the number of epochs, and, if used, details of data augmentation (Section 10.3.4).\n",
    "\n",
    "Choices such as these can make a difference. In preparing this MNIST example, we achieved a respectable 1.8% misclassification error after some trial and error. Finer tuning and training of a similar network can get under 1% error on these data, but the tinkering process can be tedious, and can result in overfitting if done carelessly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310da7e",
   "metadata": {},
   "source": [
    "#### Interpolation and Double Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c348ccd",
   "metadata": {},
   "source": [
    "Throughout this book, we have repeatedly discussed the bias-variance trade off, first presented in Section 2.2.2. This trade-off indicates that statistical learning methods tend to perform the best, in terms of test-set error, for an intermediate level of model complexity. In particular, if we plot “flexibility” on the x-axis and error on the y-axis, then we generally expect to see that test error has a U-shape, whereas training error decreases monotonically. Two “typical” examples of this behavior can be seen in the right-hand panel of Figure 2.9 on page 29, and in Figure 2.17 on page 39. One implication of the bias-variance trade-off is that it is generally not a good idea to interpolate the training data — that is, to get zero training error — since that will often result in very high test error.\n",
    "\n",
    "However, it turns out that in certain specific settings it can be possible for a statistical learning method that interpolates the training data to perform well — or at least, better than a slightly less complex model that does not quite interpolate the data. This phenomenon is known as double descent, and is displayed in Figure 10.20. “Double descent” gets its name from the fact that the test error has a U-shape before the interpolation threshold is reached, and then it descends again (for a while, at least) as an increasingly flexible model is fit.\n",
    "\n",
    "We now describe the set-up that resulted in Figure 10.20. We simulated n = 20 observations from the model\n",
    "\n",
    "$$\n",
    "Y = \\sin(X) + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $X \\sim U[-5,5]$ (uniform distribution), and $\\epsilon \\sim N(0,\\sigma^2)$ with $\\sigma = 0.3$.\n",
    "\n",
    "We then fit a natural spline to the data, as described in Section 7.4, with d degrees of freedom.22 Recall from Section 7.4 that fitting a natural spline with d degrees of freedom amounts to fitting a least-squares regression of the response onto a set of d basis functions. The upper-left panel of Figure 10.21 shows the data, the true function $f(X)$, and $\\hat f_8(X)$, the fitted natural spline with $d = 8$ degrees of freedom.\n",
    "\n",
    "Next, we fit a natural spline with $d = 20$ degrees of freedom. Since $n = 20$, this means that $n = d$, and we have zero training error; in other words, we have interpolated the training data! We can see from the top-right panel of Figure 10.21 that $\\hat f_{20}(X)$ makes wild excursions, and hence the test error will be large.\n",
    "\n",
    "We now continue to fit natural splines to the data, with increasing values of $d$. For $d > 20$, the least squares regression of $Y$ onto $d$ basis functions is not unique: there are an infinite number of least squares coefficient estimates that achieve zero error. To select among them, we choose the one with the smallest sum of squared coefficients, $\\sum_{j=1}^d \\hat\\beta_j^2$. This is known as the minimum-norm solution.\n",
    "\n",
    "The two lower panels of Figure 10.21 show the minimum-norm natural spline fits with $d = 42$ and $d = 80$ degrees of freedom. Incredibly, $\\hat f_{42}(X)$ is quite a bit less wild than $\\hat f_{20}(X)$, even though it makes use of more degrees of freedom. And $\\hat f_{80}(X)$ is not much different. How can this be? Essentially, $\\hat f_{20}(X)$ is very wild because there is just a single way to interpolate $n = 20$ observations using $d = 20$ basis functions, and that single way results in a somewhat extreme fitted function. By contrast, there are an infinite number of ways to interpolate n = 20 observations using d = 42 or d = 80 basis functions, and the smoothest of them — that is, the minimum-norm solution — is much less wild than $\\hat f_{20}(X)$!\n",
    "\n",
    "In Figure 10.20, we display the training error and test error associated with $\\hat f_d(X)$, for a range of values of the degrees of freedom $d$. We see that the training error drops to zero once $d = 20$ and beyond; i.e. once the interpolation threshold is reached. By contrast, the test error shows a U shape for $d \\le 20$, grows extremely large around $d = 20$, and then shows a second region of descent for $d>20$. For this example the signal-to-noise ratio — $\\mathrm{Var}(f(X))/\\sigma^2$ — is 5.9, which is quite high (the data points are close to the true curve). So an estimate that interpolates the data and does not wander too far in between the observed data points will likely do well.\n",
    "\n",
    "In Figures 10.20 and 10.21, we have illustrated the double descent phenomenon in a simple one-dimensional setting using natural splines. However, it turns out that the same phenomenon can arise for deep learning. Basically, when we fit neural networks with a huge number of parameters, we are sometimes able to get good results with zero training error. This is particularly true in problems with high signal-to-noise ratio, such as natural image recognition and language translation, for example. This is because the techniques used to fit neural networks, including stochastic gradient descent, naturally lend themselves to selecting a “smooth” interpolating model that has good test-set performance on these kinds of problems.\n",
    "\n",
    "Some points are worth emphasizing:\n",
    "- The double-descent phenomenon does not contradict the bias-variance trade-off, as presented in Section 2.2.2. Rather, the double-descent curve seen in the right-hand side of Figure 10.20 is a consequence of the fact that the x-axis displays the number of spline basis functions used, which does not properly capture the true “flexibility” of models that interpolate the training data. Stated another way, in this example, the minimum-norm natural spline with $d = 42$ has lower variance than the natural spline with $d = 20$.\n",
    "- Most of the statistical learning methods seen in this book do not exhibit double descent. For instance, regularization approaches typically do not interpolate the training data, and thus double descent does not occur. This is not a drawback of regularized methods: they can give great results without interpolating the data!\n",
    "\n",
    "In particular, in the examples here, if we had fit the natural splines using ridge regression with an appropriately-chosen penalty rather than least squares, then we would not have seen double descent, and in fact would have obtained better test error results.\n",
    "\n",
    "- In Chapter 9, we saw that maximal margin classifiers and SVMs that have zero training error nonetheless often achieve very good test error. This is in part because those methods seek smooth minimum norm solutions. This is similar to the fact that the minimum-norm natural spline can give good results with zero training error.\n",
    "- The double-descent phenomenon has been used by the machine learning community to explain the successful practice of using an overparametrized neural network (many layers, and many hidden units), and then fitting all the way to zero training error. However, fitting to zero error is not always optimal, and whether it is advisable depends on the signal-to-noise ratio. For instance, we may use ridge regularization to avoid overfitting a neural network, as in (10.31). In this case, provided that we use an appropriate choice for the tuning parameter \\(\\Lambda\\), we will never interpolate the training data, and thus will not see the double descent phenomenon. Nonetheless we can get very good test-set performance, likely much better than we would have achieved had we interpolated the training data. Early stopping during stochastic gradient descent can also serve as a form of regularization that prevents us from interpolating the training data, while still getting very good results on test data.\n",
    "\n",
    "To summarize: though double descent can sometimes occur in neural networks, we typically do not want to rely on this behavior. Moreover, it is important to remember that the bias-variance trade-off always holds (though it is possible that test error as a function of flexibility may not exhibit a U-shape, depending on how we have parametrized the notion of “flexibility” on the x-axis).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e42e5",
   "metadata": {},
   "source": [
    "#### Lab: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4135042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "from sklearn.linear_model import \\\n",
    "    (LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Lasso)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "from sklearn.model_selection import \\\n",
    "    (train_test_split,\n",
    "    GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cf67e",
   "metadata": {},
   "source": [
    "Torch-Specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2912997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import RMSprop\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0cff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19862cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (0.17.2+cpu)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torchvision) (2.2.2+cpu)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torch==2.2.2->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torch==2.2.2->torchvision) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torch==2.2.2->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torch==2.2.2->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torch==2.2.2->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from torch==2.2.2->torchvision) (2025.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from jinja2->torch==2.2.2->torchvision) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zoe oladokun\\appdata\\local\\anaconda3\\envs\\islp_clean\\lib\\site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d74daf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cpu\n",
      "0.17.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da03c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import (MeanAbsoluteError,\n",
    "                          R2Score)\n",
    "from torchinfo import summary\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d480e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea970a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(0, workers=True)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64fd590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR100\n",
    "from torchvision.models import (resnet50,\n",
    "                                ResNet50_Weights)\n",
    "from torchvision.transforms import (Resize,\n",
    "                                    Normalize,\n",
    "                                    CenterCrop,\n",
    "                                    ToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c1f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.torch import (SimpleDataModule,\n",
    "                        SimpleModule,\n",
    "                        ErrorTracker,\n",
    "                        rec_num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebe7af",
   "metadata": {},
   "source": [
    "from ISLP.torch.imdb import (load_lookup,\n",
    "                             load_tensor,\n",
    "                             load_sparse,\n",
    "                             load_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a309c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaae9c2",
   "metadata": {},
   "source": [
    "##### Single Layer Network on Hitters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1849ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = load_data('Hitters').dropna()\n",
    "n = Hitters.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc66d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MS(Hitters.columns.drop('Salary'), intercept=False)\n",
    "X = model.fit_transform(Hitters).to_numpy()\n",
    "Y = Hitters['Salary'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d6a5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,\n",
    "X_test,\n",
    "Y_train,\n",
    "Y_test) = train_test_split(X,\n",
    "                           Y,\n",
    "                           test_size=1/3,\n",
    "                           random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccb4e8",
   "metadata": {},
   "source": [
    "Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "190f0727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259.71528833146294"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_lm = LinearRegression().fit(X_train, Y_train)\n",
    "Yhat_test = hit_lm.predict(X_test)\n",
    "np.abs(Yhat_test- Y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c054df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "lasso = Lasso(warm_start=True, max_iter=30000)\n",
    "standard_lasso = Pipeline(steps=[('scaler', scaler),\n",
    "                                 ('lasso', lasso)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ef57521",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = scaler.fit_transform(X_train)\n",
    "n = X_s.shape[0]\n",
    "lam_max = np.fabs(X_s.T.dot(Y_train- Y_train.mean())).max() / n\n",
    "param_grid = {'alpha': np.exp(np.linspace(0, np.log(0.01), 100))\n",
    "             * lam_max}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "565d2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(10,\n",
    "           shuffle=True,\n",
    "           random_state=1)\n",
    "grid = GridSearchCV(lasso,\n",
    "                    param_grid,\n",
    "                    cv=cv,\n",
    "                    scoring='neg_mean_absolute_error')\n",
    "grid.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3387b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257.23820107995"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lasso = grid.best_estimator_\n",
    "Yhat_test = trained_lasso.predict(X_test)\n",
    "np.fabs(Yhat_test- Y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133a978",
   "metadata": {},
   "source": [
    "Specifying a Network: Classes and Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e618089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HittersModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HittersModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(50, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return torch.flatten(self.sequential(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "736b2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_model = HittersModel(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6e4ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "HittersModel                             [175, 19]                 [175]                     --\n",
       "├─Flatten: 1-1                           [175, 19]                 [175, 19]                 --\n",
       "├─Sequential: 1-2                        [175, 19]                 [175, 1]                  --\n",
       "│    └─Linear: 2-1                       [175, 19]                 [175, 50]                 1,000\n",
       "│    └─ReLU: 2-2                         [175, 50]                 [175, 50]                 --\n",
       "│    └─Dropout: 2-3                      [175, 50]                 [175, 50]                 --\n",
       "│    └─Linear: 2-4                       [175, 50]                 [175, 1]                  51\n",
       "===================================================================================================================\n",
       "Total params: 1,051\n",
       "Trainable params: 1,051\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.18\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.07\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(hit_model,\n",
    "        input_size=X_train.shape,\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cddeeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "islp_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
