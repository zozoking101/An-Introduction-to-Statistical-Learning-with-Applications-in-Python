{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73742591",
   "metadata": {},
   "source": [
    "### Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8e0c4",
   "metadata": {},
   "source": [
    "In this chapter, we describe tree-based methods for regression and classifi\n",
    "cation. These involve stratifying or segmenting the predictor space into a\n",
    "number of simple regions. In order to make a prediction for a given ob\n",
    "servation, we typically use the mean or the mode response value for the\n",
    "training observations in the region to which it belongs. Since the set of\n",
    "splitting rules used to segment the predictor space can be summarized in\n",
    "a tree, these types of approaches are known as decision tree methods.\n",
    "Tree-based methods are simple and useful for interpretation. However,\n",
    "they typically are not competitive with the best supervised learning ap\n",
    "proaches, such as those seen in Chapters 6 and 7, in terms of prediction\n",
    "accuracy. Hence in this chapter we also introduce bagging, random forests,\n",
    "boosting, and Bayesian additive regression trees. Each of these approaches\n",
    "involves producing multiple trees which are then combined to yield a single\n",
    "consensus prediction. We will see that combining a large number of trees\n",
    "can often result in dramatic improvements in prediction accuracy, at the\n",
    "expense of some loss in interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbacea35",
   "metadata": {},
   "source": [
    "#### The Basics of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237d152",
   "metadata": {},
   "source": [
    "Decision trees can be applied to both regression and classification problems.\n",
    "We first consider regression problems, and then move on to classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9cfe48",
   "metadata": {},
   "source": [
    "##### Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461bb99",
   "metadata": {},
   "source": [
    "In order to motivate regression trees, we begin with a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a0cc8",
   "metadata": {},
   "source": [
    "Predicting Baseball Players’ Salaries Using Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08822309",
   "metadata": {},
   "source": [
    "We use the Hitters data set to predict a baseball player’s Salary based on\n",
    "Years (the number of years that he has played in the major leagues) and\n",
    "Hits (the number of hits that he made in the previous year). We first remove\n",
    "observations that are missing Salary values, and log-transform Salary so\n",
    "that its distribution has more of a typical bell-shape. (Recall that Salary\n",
    "is measured in thousands of dollars.)\n",
    "Figure 8.1 shows a regression tree fit to this data. It consists of a series\n",
    "of splitting rules, starting at the top of the tree. The top split assigns\n",
    "observations having Years<4.5 to the left branch.1 The predicted salary\n",
    "for these players is given by the mean response value for the players in\n",
    "the data set with Years<4.5. For such players, the mean log salary is 5.107,\n",
    "and so we make a prediction of e5.107 thousands of dollars, i.e. $165,174, for\n",
    "these players. Players with Years>=4.5 are assigned to the right branch, and\n",
    "then that group is further subdivided by Hits. Overall, the tree stratifies\n",
    "or segments the players into three regions of predictor space: players who\n",
    "have played for four or fewer years, players who have played for five or more\n",
    "years and who made fewer than 118 hits last year, and players who have\n",
    "played for five or more years and who made at least 118 hits last year. These\n",
    "three regions can be written as R1 ={X | Years<4.5}, R2 ={X | Years>=4.5,\n",
    "Hits<117.5}, and R3 ={X | Years>=4.5, Hits>=117.5}. Figure 8.2 illustrates the regions as a function of Years and Hits. The predicted salaries for these\n",
    "three groups are $1,000 e5.107 =$165,174, $1,000 e5.999 =$402,834, and\n",
    "$1,000 e6.740 =$845,346 respectively.\n",
    "In keeping with the tree analogy, the regions R1, R2, and R3 are known as\n",
    "terminal nodes or leaves of the tree. As is the case for Figure 8.1, decision terminal\n",
    "trees are typically drawn upside down, in the sense that the leaves are at\n",
    "the bottom of the tree. The points along the tree where the predictor space\n",
    "is split are referred to as internal nodes. In Figure 8.1, the two internal internal\n",
    "nodes are indicated by the text Years<4.5 and Hits<117.5. We refer to the\n",
    "segments of the trees that connect the nodes as branches.\n",
    "We might interpret the regression tree displayed in Figure 8.1 as follows:\n",
    "Years is the most important factor in determining Salary, and players with\n",
    "less experience earn lower salaries than more experienced players. Given\n",
    "that a player is less experienced, the number of hits that he made in the\n",
    "previous year seems to play little role in his salary. But among players who\n",
    "have been in the major leagues for five or more years, the number of hits\n",
    "made in the previous year does affect salary, and players who made more\n",
    "hits last year tend to have higher salaries. The regression tree shown in\n",
    "Figure 8.1 is likely an over-simplification of the true relationship between\n",
    "Hits, Years, and Salary. However, it has advantages over other types of\n",
    "regression models (such as those seen in Chapters 3 and 6): it is easier to\n",
    "interpret, and has a nice graphical representation.\n",
    "Prediction via Stratification of the Feature Space\n",
    "Wenowdiscuss the process of building a regression tree. Roughly speaking,\n",
    "there are two steps.\n",
    "1. We divide the predictor space — that is, the set of possible values\n",
    "for X1,X2,...,Xp — into J distinct and non-overlapping regions,\n",
    "R1,R2,...,RJ.\n",
    "2. For every observation that falls into the region Rj, we make the same\n",
    "prediction, which is simply the mean of the response values for the\n",
    "training observations in Rj.\n",
    "For instance, suppose that in Step 1 we obtain two regions, R1 and R2,\n",
    "and that the response mean of the training observations in the first region\n",
    "is 10, while the response mean of the training observations in the second\n",
    "region is 20. Then for a given observation X = x, if x R1 we will predict\n",
    "a value of 10, and if x R2 we will predict a value of 20.\n",
    "We now elaborate on Step 1 above. How do we construct the regions\n",
    "R1,...,RJ? In theory, the regions could have any shape. However, we\n",
    "choose to divide the predictor space into high-dimensional rectangles, or\n",
    "boxes, for simplicity and for ease of interpretation of the resulting predic\n",
    "tive model. The goal is to find boxes R1,...,RJ that minimize the RSS,\n",
    "given by For instance, suppose that in Step 1 we obtain two regions, $ R_1 $ and $ R_2 $, and that the response mean of the training observations in the first region is 10, while the response mean of the training observations in the second region is 20. Then for a given observation $ X = x $, if $ x \\in R_1 $ we will predict a value of 10, and if $ x \\in R_2 $ we will predict a value of 20.\n",
    "\n",
    "We now elaborate on Step 1 above. How do we construct the regions $ R_1, \\ldots, R_J $? In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predictive model. The goal is to find boxes $ R_1, \\ldots, R_J $ that minimize the RSS, given by\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2,\n",
    "$$\n",
    "\n",
    "where $ \\hat{y}_{R_j} $ is the mean response for the training observations within the $ j $th box. Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into $ J $ boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "\n",
    "In order to perform recursive binary splitting, we first select the predictor $ X_j $ and the cutpoint $ s $ such that splitting the predictor space into the regions $ \\{X | X_j < s\\} $ and $ \\{X | X_j \\geq s\\} $ leads to the greatest possible reduction in RSS. (The notation $ \\{X | X_j < s\\} $ means the region of predictor space in which $ X_j $ takes on a value less than $ s $.) That is, we consider all predictors $ X_1, \\ldots, X_p $, and all possible values of the cutpoint $ s $ for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS. In greater detail, for any $ j $ and $ s $, we define the pair of half-planes\n",
    "\n",
    "$$\n",
    "R_1(j,s) = \\{X | X_j < s\\} \\quad \\text{and} \\quad R_2(j,s) = \\{X | X_j \\geq s\\},\n",
    "$$\n",
    "\n",
    "and we seek the value of $ j $ and $ s $ that minimize the equation\n",
    "\n",
    "$$\n",
    "\\sum_{i: x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2,\n",
    "$$\n",
    "\n",
    "where $ \\hat{y}_{R_1} $ is the mean response for the training observations in $ R_1(j,s) $, and $ \\hat{y}_{R_2} $ is the mean response for the training observations in $ R_2(j,s) $. Finding the values of $ j $ and $ s $ that minimize the above expression can be done quite quickly, especially when the number of features $ p $ is not too large.\n",
    "\n",
    "Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the\n",
    "entire predictor space, we split one of the two previously identified regions.\n",
    "Wenowhavethree regions. Again, we look to split one of these three regions\n",
    "further, so as to minimize the RSS. The process continues until a stopping\n",
    "criterion is reached; for instance, we may continue until no region contains\n",
    "more than five observations.\n",
    "Once the regions R1,...,RJ have been created, we predict the response\n",
    "for a given test observation using the mean of the training observations in\n",
    "the region to which that test observation belongs.\n",
    "A five-region example of this approach is shown in Figure 8.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0149a",
   "metadata": {},
   "source": [
    "Tree Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0df84",
   "metadata": {},
   "source": [
    "\n",
    "The process described above may produce good predictions on the training\n",
    "set, but is likely to overfit the data, leading to poor test set performance.\n",
    "This is because the resulting tree might be too complex. A smaller tree\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
